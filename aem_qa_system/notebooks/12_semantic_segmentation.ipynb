{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ec42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 1: í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "# ===================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ëª¨ë“ˆ\n",
    "from config import MONGO_CONNECTION_STRING, DB_NAME, SUPPORTED_LANGUAGE_PAIRS\n",
    "from pymongo import MongoClient\n",
    "\n",
    "print(\"âœ… ì˜ë¯¸ ë¶„í•  ì‹œìŠ¤í…œ ì´ˆê¸°í™”\")\n",
    "print(f\"ğŸ“Š ì§€ì› ì–¸ì–´ ìŒ: {SUPPORTED_LANGUAGE_PAIRS}\")\n",
    "print(f\"ğŸ”— MongoDB: {MONGO_CONNECTION_STRING}\")\n",
    "print(f\"ğŸ—„ï¸ Database: {DB_NAME}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 2: spaCy ëª¨ë¸ ì„¤ì¹˜ í™•ì¸ ë° ì„¤ì¹˜ ê°€ì´ë“œ\n",
    "# ===================================================================\n",
    "\n",
    "def check_and_install_spacy_models():\n",
    "    \"\"\"í•„ìš”í•œ spaCy ëª¨ë¸ í™•ì¸ ë° ì„¤ì¹˜ ê°€ì´ë“œ\"\"\"\n",
    "    \n",
    "    required_models = {\n",
    "        'en': 'en_core_web_sm',\n",
    "        'ko': 'ko_core_news_sm', \n",
    "        'ja': 'ja_core_news_sm'\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ” spaCy ëª¨ë¸ í™•ì¸ ì¤‘...\")\n",
    "    missing_models = []\n",
    "    \n",
    "    for lang, model_name in required_models.items():\n",
    "        try:\n",
    "            nlp = spacy.load(model_name)\n",
    "            print(f\"   âœ… {lang}: {model_name} - ì„¤ì¹˜ë¨\")\n",
    "        except OSError:\n",
    "            print(f\"   âŒ {lang}: {model_name} - ë¯¸ì„¤ì¹˜\")\n",
    "            missing_models.append(model_name)\n",
    "    \n",
    "    if missing_models:\n",
    "        print(f\"\\nğŸ’¡ ëˆ„ë½ëœ ëª¨ë¸ ì„¤ì¹˜ ëª…ë ¹ì–´:\")\n",
    "        for model in missing_models:\n",
    "            print(f\"   python -m spacy download {model}\")\n",
    "        print(\"\\nâš ï¸ í•œêµ­ì–´/ì¼ë³¸ì–´ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê·œì¹™ ê¸°ë°˜ ë¶„í• ë¡œ í´ë°±ë©ë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"\\nğŸ‰ ëª¨ë“  spaCy ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤!\")\n",
    "    \n",
    "    return len(missing_models) == 0\n",
    "\n",
    "# ëª¨ë¸ í™•ì¸ ì‹¤í–‰\n",
    "spacy_ready = check_and_install_spacy_models()\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 3: MongoDB ì—°ê²° ë° ê¸°ì¡´ ë°ì´í„° í™•ì¸\n",
    "# ===================================================================\n",
    "\n",
    "def check_existing_tm_data():\n",
    "    \"\"\"ê¸°ì¡´ TM ë°ì´í„° í˜„í™© í™•ì¸\"\"\"\n",
    "    \n",
    "    try:\n",
    "        client = MongoClient(MONGO_CONNECTION_STRING)\n",
    "        db = client[DB_NAME]\n",
    "        \n",
    "        # ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "        db.admin.command('ping')\n",
    "        print(\"âœ… MongoDB ì—°ê²° ì„±ê³µ\")\n",
    "        \n",
    "        # ê¸°ì¡´ TM ì»¬ë ‰ì…˜ í™•ì¸\n",
    "        all_collections = db.list_collection_names()\n",
    "        tm_collections = [c for c in all_collections if 'translation_memory' in c]\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ê¸°ì¡´ TM ì»¬ë ‰ì…˜ í˜„í™©:\")\n",
    "        \n",
    "        if not tm_collections:\n",
    "            print(\"   âš ï¸ TM ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 6_build_final_tm.ipynbë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "            return False\n",
    "        \n",
    "        for collection_name in tm_collections:\n",
    "            collection = db[collection_name]\n",
    "            doc_count = collection.count_documents({})\n",
    "            \n",
    "            # ìƒ˜í”Œ ë¬¸ì„œë¡œ êµ¬ì¡° í™•ì¸\n",
    "            sample = collection.find_one({})\n",
    "            has_html = False\n",
    "            \n",
    "            if sample:\n",
    "                source_text = sample.get('source_text', '')\n",
    "                target_text = sample.get('target_text', '')\n",
    "                has_html = '<' in source_text or '<' in target_text\n",
    "            \n",
    "            print(f\"   ğŸ“‹ {collection_name}: {doc_count:,}ê°œ ë¬¸ì„œ\")\n",
    "            if has_html:\n",
    "                print(f\"      â”” HTML íƒœê·¸ í¬í•¨ ë¬¸ì„œ ê°ì§€ë¨ â†’ ë¶„ë¦¬ í•„ìš”\")\n",
    "        \n",
    "        # ìƒˆë¡œìš´ ë¶„ë¦¬ ì»¬ë ‰ì…˜ í™•ì¸\n",
    "        clean_collections = [c for c in all_collections if c.startswith('clean_translation_memory_')]\n",
    "        html_collections = [c for c in all_collections if c.startswith('html_component_archive_')]\n",
    "        \n",
    "        if clean_collections or html_collections:\n",
    "            print(f\"\\nğŸ”„ ë¶„ë¦¬ëœ ì»¬ë ‰ì…˜ í˜„í™©:\")\n",
    "            print(f\"   - Clean TM: {len(clean_collections)}ê°œ\")\n",
    "            print(f\"   - HTML Archive: {len(html_collections)}ê°œ\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ MongoDB ì—°ê²° ì‹¤íŒ¨: {str(e)}\")\n",
    "        return False\n",
    "    \n",
    "    finally:\n",
    "        if 'client' in locals():\n",
    "            client.close()\n",
    "\n",
    "# ê¸°ì¡´ ë°ì´í„° í™•ì¸\n",
    "mongodb_ready = check_existing_tm_data()\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 4: SemanticSegmenter í´ë˜ìŠ¤ ì •ì˜\n",
    "# ===================================================================\n",
    "\n",
    "class SemanticSegmenter:\n",
    "    \"\"\"ì˜ë¯¸ ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í•  í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = 'intfloat/multilingual-e5-large'):\n",
    "        self.embedding_model = self._load_embedding_model(embedding_model)\n",
    "        self.nlp_models = self._load_spacy_models()\n",
    "        \n",
    "        # ë¶„í•  ì¡°ê±´ ì„¤ì •\n",
    "        self.config = {\n",
    "            'optimal_length': 70,        # ìµœì  ê¸¸ì´\n",
    "            'max_length': 120,           # ìµœëŒ€ ê¸¸ì´ (ê°•ì œ ë¶„í• )\n",
    "            'min_length': 20,            # ìµœì†Œ ê¸¸ì´\n",
    "            'max_sentences': 3,          # ìµœëŒ€ ë¬¸ì¥ ìˆ˜\n",
    "            'complexity_threshold': 0.7,  # ë³µì¡ë„ ì„ê³„ê°’\n",
    "            'length_imbalance_ratio': 3.0 # ê¸¸ì´ ë¶ˆê· í˜• ë¹„ìœ¨\n",
    "        }\n",
    "        \n",
    "        print(\"âœ… SemanticSegmenter ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "    \n",
    "    def _load_embedding_model(self, model_name: str):\n",
    "        \"\"\"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        try:\n",
    "            model = SentenceTransformer(model_name)\n",
    "            print(f\"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: {model_name}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_spacy_models(self) -> Dict:\n",
    "        \"\"\"spaCy ëª¨ë¸ë“¤ ë¡œë“œ\"\"\"\n",
    "        models = {}\n",
    "        model_mapping = {\n",
    "            'en': 'en_core_web_sm',\n",
    "            'ko': 'ko_core_news_sm',\n",
    "            'ja': 'ja_core_news_sm'\n",
    "        }\n",
    "        \n",
    "        for lang, model_name in model_mapping.items():\n",
    "            try:\n",
    "                models[lang] = spacy.load(model_name)\n",
    "                print(f\"âœ… spaCy ëª¨ë¸ ë¡œë“œ: {lang} ({model_name})\")\n",
    "            except OSError:\n",
    "                print(f\"âš ï¸ spaCy ëª¨ë¸ ì—†ìŒ: {lang} ({model_name}) - ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ í´ë°±\")\n",
    "                models[lang] = None\n",
    "        \n",
    "        return models\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 5: ë¶„í•  ì¡°ê±´ ê²€ì‚¬ ë©”ì„œë“œë“¤\n",
    "# ===================================================================\n",
    "\n",
    "def check_segmentation_conditions(self, source_text: str, target_text: str,\n",
    "                                 source_lang: str, target_lang: str) -> Dict:\n",
    "    \"\"\"ë¶„í•  í•„ìš”ì„± ë° ì¡°ê±´ ê²€ì‚¬\"\"\"\n",
    "    \n",
    "    conditions = {\n",
    "        'needs_segmentation': False,\n",
    "        'reasons': [],\n",
    "        'max_length': max(len(source_text), len(target_text)),\n",
    "        'source_sentences': self._count_sentences(source_text, source_lang),\n",
    "        'target_sentences': self._count_sentences(target_text, target_lang),\n",
    "        'complexity_score': self._calculate_complexity(source_text),\n",
    "        'length_imbalance': self._calculate_length_imbalance(source_text, target_text)\n",
    "    }\n",
    "    \n",
    "    # ì¡°ê±´ 1: ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼\n",
    "    if conditions['max_length'] > self.config['max_length']:\n",
    "        conditions['needs_segmentation'] = True\n",
    "        conditions['reasons'].append('max_length_exceeded')\n",
    "    \n",
    "    # ì¡°ê±´ 2: ë¬¸ì¥ ìˆ˜ ì´ˆê³¼\n",
    "    max_sentences = max(conditions['source_sentences'], conditions['target_sentences'])\n",
    "    if max_sentences > self.config['max_sentences']:\n",
    "        conditions['needs_segmentation'] = True\n",
    "        conditions['reasons'].append('too_many_sentences')\n",
    "    \n",
    "    # ì¡°ê±´ 3: ë³µì¡ë„ ë†’ìŒ\n",
    "    if conditions['complexity_score'] > self.config['complexity_threshold']:\n",
    "        conditions['needs_segmentation'] = True\n",
    "        conditions['reasons'].append('high_complexity')\n",
    "    \n",
    "    # ì¡°ê±´ 4: ê¸¸ì´ ë¶ˆê· í˜•\n",
    "    if conditions['length_imbalance'] > self.config['length_imbalance_ratio']:\n",
    "        conditions['needs_segmentation'] = True\n",
    "        conditions['reasons'].append('length_imbalance')\n",
    "    \n",
    "    return conditions\n",
    "\n",
    "def _count_sentences(self, text: str, lang: str) -> int:\n",
    "    \"\"\"ë¬¸ì¥ ìˆ˜ ê³„ì‚°\"\"\"\n",
    "    if not text.strip():\n",
    "        return 0\n",
    "    \n",
    "    # spaCy ëª¨ë¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©\n",
    "    nlp = self.nlp_models.get(lang)\n",
    "    if nlp:\n",
    "        doc = nlp(text)\n",
    "        return len(list(doc.sents))\n",
    "    \n",
    "    # í´ë°±: ê·œì¹™ ê¸°ë°˜ ë¬¸ì¥ ë¶„í• \n",
    "    sentence_endings = r'[.!?ã€‚ï¼ï¼Ÿ]+'\n",
    "    sentences = re.split(sentence_endings, text.strip())\n",
    "    return len([s for s in sentences if s.strip()])\n",
    "\n",
    "def _calculate_complexity(self, text: str) -> float:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ë³µì¡ë„ ê³„ì‚° (0.0 ~ 1.0)\"\"\"\n",
    "    if not text.strip():\n",
    "        return 0.0\n",
    "    \n",
    "    score = 0.0\n",
    "    \n",
    "    # ë‹¨ì–´ ìˆ˜\n",
    "    word_count = len(text.split())\n",
    "    if word_count > 20:\n",
    "        score += 0.3\n",
    "    \n",
    "    # íŠ¹ìˆ˜ ë¬¸ì ë¹„ìœ¨\n",
    "    special_chars = len(re.findall(r'[^\\w\\s]', text))\n",
    "    special_ratio = special_chars / len(text) if text else 0\n",
    "    if special_ratio > 0.2:\n",
    "        score += 0.2\n",
    "    \n",
    "    # ìˆ«ì ë° ê´„í˜¸ í¬í•¨\n",
    "    if re.search(r'\\d+', text):\n",
    "        score += 0.1\n",
    "    if '(' in text and ')' in text:\n",
    "        score += 0.1\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "# SemanticSegmenter í´ë˜ìŠ¤ì— ë©”ì„œë“œ ì¶”ê°€\n",
    "SemanticSegmenter._check_segmentation_conditions = check_segmentation_conditions\n",
    "SemanticSegmenter._count_sentences = _count_sentences\n",
    "SemanticSegmenter._calculate_complexity = _calculate_complexity\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 6: ì ì‘ì  ë¶„í•  í…ŒìŠ¤íŠ¸\n",
    "# ===================================================================\n",
    "\n",
    "def test_adaptive_segmentation():\n",
    "    \"\"\"ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ìœ í˜•ìœ¼ë¡œ ì ì‘ì  ë¶„í•  í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"ì§§ì€ í…ìŠ¤íŠ¸ (ë¶„í•  ì•ˆë¨)\",\n",
    "            \"source\": \"Illumina sequencing technology.\",\n",
    "            \"target\": \"ì¼ë£¨ë¯¸ë‚˜ ì‹œí€€ì‹± ê¸°ìˆ .\",\n",
    "            \"expected\": \"no_split\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ê¸´ ë‹¨ì¼ ë¬¸ì¥ (ê¸¸ì´ë¡œ ë¶„í• )\",\n",
    "            \"source\": \"Illumina's comprehensive portfolio of sequencing and array-based solutions enables researchers to perform virtually any type of genetic analysis across a broad range of applications and study sizes.\",\n",
    "            \"target\": \"ì¼ë£¨ë¯¸ë‚˜ì˜ í¬ê´„ì ì¸ ì‹œí€€ì‹± ë° ì–´ë ˆì´ ê¸°ë°˜ ì†”ë£¨ì…˜ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ í†µí•´ ì—°êµ¬ìë“¤ì€ ì†Œê·œëª¨ íƒ€ê²Ÿ íŒ¨ë„ë¶€í„° ì¸êµ¬ ê·œëª¨ì˜ ì „ì²´ ê²Œë†ˆ ì‹œí€€ì‹± ì—°êµ¬ì— ì´ë¥´ê¸°ê¹Œì§€ ê´‘ë²”ìœ„í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ ì—°êµ¬ ê·œëª¨ì—ì„œ ê±°ì˜ ëª¨ë“  ìœ í˜•ì˜ ìœ ì „ì  ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
    "            \"expected\": \"split\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ë‹¤ì¤‘ ë¬¸ì¥ (ë¬¸ì¥ ìˆ˜ë¡œ ë¶„í• )\",\n",
    "            \"source\": \"Our platforms are trusted globally. Researchers rely on our accuracy. Innovation drives our development.\",\n",
    "            \"target\": \"ìš°ë¦¬ì˜ í”Œë«í¼ì€ ì „ ì„¸ê³„ì ìœ¼ë¡œ ì‹ ë¢°ë°›ê³  ìˆìŠµë‹ˆë‹¤. ì—°êµ¬ìë“¤ì€ ìš°ë¦¬ì˜ ì •í™•ì„±ì„ ì‹ ë¢°í•©ë‹ˆë‹¤. í˜ì‹ ì´ ìš°ë¦¬ì˜ ê°œë°œì„ ì´ë•ë‹ˆë‹¤.\",\n",
    "            \"expected\": \"split\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ§ª ì ì‘ì  ë¶„í•  í…ŒìŠ¤íŠ¸ - ë‹¤ì–‘í•œ ì¼€ì´ìŠ¤\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    segmenter = SemanticSegmenter()\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\n--- í…ŒìŠ¤íŠ¸ {i}: {test_case['name']} ---\")\n",
    "        \n",
    "        source_text = test_case['source']\n",
    "        target_text = test_case['target']\n",
    "        \n",
    "        print(f\"Source ({len(source_text)}ì): {source_text[:80]}...\")\n",
    "        print(f\"Target ({len(target_text)}ì): {target_text[:80]}...\")\n",
    "        \n",
    "        # ë¶„í•  ì¡°ê±´ í™•ì¸\n",
    "        conditions = segmenter._check_segmentation_conditions(\n",
    "            source_text, target_text, 'en', 'ko'\n",
    "        )\n",
    "        \n",
    "        # ê²°ê³¼ ë¶„ì„\n",
    "        needs_split = conditions['needs_segmentation']\n",
    "        actual_result = \"split\" if needs_split else \"no_split\"\n",
    "        \n",
    "        print(f\"ì˜ˆìƒ: {test_case['expected']}, ì‹¤ì œ: {actual_result}\")\n",
    "        print(f\"ë¶„í•  í•„ìš”: {needs_split}\")\n",
    "        \n",
    "        if needs_split:\n",
    "            print(f\"ë¶„í•  ì´ìœ : {', '.join(conditions['reasons'])}\")\n",
    "            print(f\"ì¡°ê±´ ìƒì„¸:\")\n",
    "            print(f\"  - ìµœëŒ€ ê¸¸ì´: {conditions['max_length']} (ì„ê³„ê°’: {segmenter.config['max_length']})\")\n",
    "            print(f\"  - ë¬¸ì¥ ìˆ˜: {conditions['source_sentences']}â†”{conditions['target_sentences']} (ì„ê³„ê°’: {segmenter.config['max_sentences']})\")\n",
    "            print(f\"  - ë³µì¡ë„: {conditions['complexity_score']:.2f} (ì„ê³„ê°’: {segmenter.config['complexity_threshold']})\")\n",
    "        \n",
    "        # ì˜ˆìƒê³¼ ì‹¤ì œ ê²°ê³¼ ë¹„êµ\n",
    "        match = \"âœ…\" if actual_result == test_case['expected'] else \"âŒ\"\n",
    "        print(f\"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {match}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "test_adaptive_segmentation()\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 7: TM í’ˆì§ˆ ë¶„ì„ í´ë˜ìŠ¤\n",
    "# ===================================================================\n",
    "\n",
    "class TMQualityAnalyzer:\n",
    "    \"\"\"ë²ˆì—­ ë©”ëª¨ë¦¬ í’ˆì§ˆ ë¶„ì„ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = MongoClient(MONGO_CONNECTION_STRING)\n",
    "        self.db = self.client[DB_NAME]\n",
    "        print(\"âœ… TMQualityAnalyzer ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "    \n",
    "    def analyze_tm_quality(self, lang_suffix: str):\n",
    "        \"\"\"TM í’ˆì§ˆ ì¢…í•© ë¶„ì„\"\"\"\n",
    "        \n",
    "        print(f\"ğŸ“Š {lang_suffix} TM í’ˆì§ˆ ë¶„ì„\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Raw TM ë¶„ì„\n",
    "        raw_collection = self.db[f\"translation_memory_{lang_suffix}\"]\n",
    "        raw_count = raw_collection.count_documents({})\n",
    "        \n",
    "        if raw_count == 0:\n",
    "            print(\"âŒ Raw TM ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"ğŸ“‹ Raw TM: {raw_count:,}ê°œ ë ˆì½”ë“œ\")\n",
    "        \n",
    "        # HTML íƒœê·¸ í¬í•¨ ë¶„ì„\n",
    "        html_count = raw_collection.count_documents({\n",
    "            \"$or\": [\n",
    "                {\"source_text\": {\"$regex\": \"<[^>]+>\"}},\n",
    "                {\"target_text\": {\"$regex\": \"<[^>]+>\"}}\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        html_ratio = html_count / raw_count * 100\n",
    "        print(f\"ğŸ·ï¸ HTML íƒœê·¸ í¬í•¨: {html_count:,}ê°œ ({html_ratio:.1f}%)\")\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„\n",
    "        self._analyze_text_lengths(raw_collection)\n",
    "        \n",
    "        # í’ˆì§ˆ ë¬¸ì œ ë¶„ì„\n",
    "        self._analyze_quality_issues(raw_collection)\n",
    "        \n",
    "        # ë¶„í•  í›„ë³´ ë¶„ì„\n",
    "        self._analyze_segmentation_candidates(raw_collection)\n",
    "    \n",
    "    def _analyze_text_lengths(self, collection):\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„\"\"\"\n",
    "        \n",
    "        pipeline = [\n",
    "            {\n",
    "                \"$project\": {\n",
    "                    \"source_length\": {\"$strLenCP\": \"$source_text\"},\n",
    "                    \"target_length\": {\"$strLenCP\": \"$target_text\"}\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"$group\": {\n",
    "                    \"_id\": None,\n",
    "                    \"avg_source_length\": {\"$avg\": \"$source_length\"},\n",
    "                    \"avg_target_length\": {\"$avg\": \"$target_length\"},\n",
    "                    \"max_source_length\": {\"$max\": \"$source_length\"},\n",
    "                    \"max_target_length\": {\"$max\": \"$target_length\"},\n",
    "                    \"total_docs\": {\"$sum\": 1}\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        stats = list(collection.aggregate(pipeline))\n",
    "        if stats:\n",
    "            s = stats[0]\n",
    "            print(f\"\\nğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„:\")\n",
    "            print(f\"   - í‰ê·  ì†ŒìŠ¤ ê¸¸ì´: {s['avg_source_length']:.1f}ì\")\n",
    "            print(f\"   - í‰ê·  íƒ€ê²Ÿ ê¸¸ì´: {s['avg_target_length']:.1f}ì\")\n",
    "            print(f\"   - ìµœëŒ€ ì†ŒìŠ¤ ê¸¸ì´: {s['max_source_length']:,}ì\")\n",
    "            print(f\"   - ìµœëŒ€ íƒ€ê²Ÿ ê¸¸ì´: {s['max_target_length']:,}ì\")\n",
    "    \n",
    "    def _analyze_quality_issues(self, collection):\n",
    "        \"\"\"í’ˆì§ˆ ë¬¸ì œ ë¶„ì„\"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ” í’ˆì§ˆ ë¬¸ì œ ë¶„ì„:\")\n",
    "        \n",
    "        # ë¹ˆ í…ìŠ¤íŠ¸\n",
    "        empty_count = collection.count_documents({\n",
    "            \"$or\": [\n",
    "                {\"source_text\": \"\"},\n",
    "                {\"target_text\": \"\"},\n",
    "                {\"source_text\": {\"$regex\": \"^\\\\s*$\"}},\n",
    "                {\"target_text\": {\"$regex\": \"^\\\\s*$\"}}\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        # JCR/Sling ì†ì„±\n",
    "        jcr_count = collection.count_documents({\n",
    "            \"$or\": [\n",
    "                {\"source_text\": {\"$regex\": \"jcr:|sling:|cq:|dam:\"}},\n",
    "                {\"target_text\": {\"$regex\": \"jcr:|sling:|cq:|dam:\"}}\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        # ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸ (3ì ì´í•˜)\n",
    "        very_short_count = collection.count_documents({\n",
    "            \"$or\": [\n",
    "                {\"$expr\": {\"$lte\": [{\"$strLenCP\": \"$source_text\"}, 3]}},\n",
    "                {\"$expr\": {\"$lte\": [{\"$strLenCP\": \"$target_text\"}, 3]}}\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        print(f\"   - ë¹ˆ í…ìŠ¤íŠ¸: {empty_count:,}ê°œ\")\n",
    "        print(f\"   - JCR/Sling ì†ì„±: {jcr_count:,}ê°œ\")\n",
    "        print(f\"   - ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸: {very_short_count:,}ê°œ\")\n",
    "    \n",
    "    def _analyze_segmentation_candidates(self, collection):\n",
    "        \"\"\"ë¶„í•  í›„ë³´ ë¶„ì„\"\"\"\n",
    "        \n",
    "        print(f\"\\nâœ‚ï¸ ë¶„í•  í›„ë³´ ë¶„ì„:\")\n",
    "        \n",
    "        # ê¸¸ì´ ê¸°ì¤€ í›„ë³´ (120ì ì´ˆê³¼)\n",
    "        long_text_count = collection.count_documents({\n",
    "            \"$or\": [\n",
    "                {\"$expr\": {\"$gt\": [{\"$strLenCP\": \"$source_text\"}, 120]}},\n",
    "                {\"$expr\": {\"$gt\": [{\"$strLenCP\": \"$target_text\"}, 120]}}\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        # ë‹¤ì¤‘ ë¬¸ì¥ í›„ë³´ (3ê°œ ì´ìƒ ë§ˆì¹¨í‘œ)\n",
    "        multi_sentence_count = collection.count_documents({\n",
    "            \"$or\": [\n",
    "                {\"source_text\": {\"$regex\": \"\\\\.[^.]*\\\\.[^.]*\\\\.\"}},\n",
    "                {\"target_text\": {\"$regex\": \"\\\\.[^.]*\\\\.[^.]*\\\\.\"}}\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        # HTML ë³µí•© í…ìŠ¤íŠ¸\n",
    "        html_complex_count = collection.count_documents({\n",
    "            \"$and\": [\n",
    "                {\n",
    "                    \"$or\": [\n",
    "                        {\"source_text\": {\"$regex\": \"<[^>]+>\"}},\n",
    "                        {\"target_text\": {\"$regex\": \"<[^>]+>\"}}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"$or\": [\n",
    "                        {\"$expr\": {\"$gt\": [{\"$strLenCP\": \"$source_text\"}, 50]}},\n",
    "                        {\"$expr\": {\"$gt\": [{\"$strLenCP\": \"$target_text\"}, 50]}}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "        \n",
    "        print(f\"   - ê¸´ í…ìŠ¤íŠ¸ (120ì+): {long_text_count:,}ê°œ\")\n",
    "        print(f\"   - ë‹¤ì¤‘ ë¬¸ì¥: {multi_sentence_count:,}ê°œ\")\n",
    "        print(f\"   - HTML ë³µí•© í…ìŠ¤íŠ¸: {html_complex_count:,}ê°œ\")\n",
    "        \n",
    "        total_candidates = long_text_count + multi_sentence_count + html_complex_count\n",
    "        total_docs = collection.count_documents({})\n",
    "        candidate_ratio = total_candidates / total_docs * 100 if total_docs > 0 else 0\n",
    "        \n",
    "        print(f\"   - ì „ì²´ ë¶„í•  í›„ë³´ ë¹„ìœ¨: {candidate_ratio:.1f}%\")\n",
    "\n",
    "# í’ˆì§ˆ ë¶„ì„ ì‹¤í–‰\n",
    "if mongodb_ready:\n",
    "    analyzer = TMQualityAnalyzer()\n",
    "    \n",
    "    # ì–¸ì–´ ìŒ ì„ íƒ\n",
    "    print(\"ğŸŒ ë¶„ì„í•  ì–¸ì–´ ìŒì„ ì„ íƒí•˜ì„¸ìš”:\")\n",
    "    for i, (source, target) in enumerate(SUPPORTED_LANGUAGE_PAIRS, 1):\n",
    "        lang_suffix = f\"{source}_{target}\"\n",
    "        print(f\"  [{i}] {source.upper()}-{target.upper()} ({lang_suffix})\")\n",
    "    \n",
    "    choice = input(\"\\në²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "    \n",
    "    try:\n",
    "        index = int(choice) - 1\n",
    "        if 0 <= index < len(SUPPORTED_LANGUAGE_PAIRS):\n",
    "            source_lang, target_lang = SUPPORTED_LANGUAGE_PAIRS[index]\n",
    "            selected_lang_suffix = f\"{source_lang}_{target_lang}\"\n",
    "            \n",
    "            print(f\"\\nâœ… ì„ íƒë¨: {selected_lang_suffix}\")\n",
    "            analyzer.analyze_tm_quality(selected_lang_suffix)\n",
    "        else:\n",
    "            print(\"âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.\")\n",
    "    except ValueError:\n",
    "        print(\"âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "else:\n",
    "    print(\"âš ï¸ MongoDB ì—°ê²°ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 8: SemanticSegmenter í…ŒìŠ¤íŠ¸ (ê°„ë‹¨í•œ êµ¬í˜„)\n",
    "# ===================================================================\n",
    "\n",
    "class SimpleSemanticSegmenter:\n",
    "    \"\"\"ê°„ë‹¨í•œ ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.config = {\n",
    "            'optimal_length': 70,\n",
    "            'max_length': 120,\n",
    "            'max_sentences': 3,\n",
    "            'complexity_threshold': 0.7,\n",
    "            'length_imbalance_ratio': 3.0\n",
    "        }\n",
    "        print(\"âœ… SimpleSemanticSegmenter ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "    \n",
    "    def segment_text_pair(self, source_text, target_text, source_lang='en', target_lang='ko', original_metadata=None):\n",
    "        \"\"\"ê°„ë‹¨í•œ ë¶„í•  í…ŒìŠ¤íŠ¸\"\"\"\n",
    "        \n",
    "        # ë¶„í•  ì¡°ê±´ í™•ì¸\n",
    "        conditions = self._check_conditions(source_text, target_text)\n",
    "        \n",
    "        if not conditions['needs_segmentation']:\n",
    "            return [{\n",
    "                'source_text': source_text,\n",
    "                'target_text': target_text,\n",
    "                'is_segmented': False,\n",
    "                'segmentation_conditions_checked': conditions\n",
    "            }]\n",
    "        \n",
    "        # ê°„ë‹¨í•œ ë¶„í•  ë¡œì§ (ë¬¸ì¥ ê¸°ë°˜)\n",
    "        segments = []\n",
    "        source_sentences = self._split_sentences(source_text)\n",
    "        target_sentences = self._split_sentences(target_text)\n",
    "        \n",
    "        max_segments = max(len(source_sentences), len(target_sentences))\n",
    "        \n",
    "        for i in range(max_segments):\n",
    "            src_segment = source_sentences[i] if i < len(source_sentences) else \"\"\n",
    "            tgt_segment = target_sentences[i] if i < len(target_sentences) else \"\"\n",
    "            \n",
    "            if src_segment or tgt_segment:\n",
    "                segments.append({\n",
    "                    'source_text': src_segment,\n",
    "                    'target_text': tgt_segment,\n",
    "                    'is_segmented': True,\n",
    "                    'segment_index': i,\n",
    "                    'segmentation_conditions_checked': conditions if i == 0 else {}\n",
    "                })\n",
    "        \n",
    "        return segments if segments else [{\n",
    "            'source_text': source_text,\n",
    "            'target_text': target_text,\n",
    "            'is_segmented': False,\n",
    "            'segmentation_conditions_checked': conditions\n",
    "        }]\n",
    "    \n",
    "    def _check_conditions(self, source_text, target_text):\n",
    "        \"\"\"ë¶„í•  ì¡°ê±´ í™•ì¸\"\"\"\n",
    "        conditions = {\n",
    "            'needs_segmentation': False,\n",
    "            'reasons': [],\n",
    "            'max_length': max(len(source_text), len(target_text)),\n",
    "            'source_sentences': len(self._split_sentences(source_text)),\n",
    "            'target_sentences': len(self._split_sentences(target_text)),\n",
    "            'complexity_score': self._calc_complexity(source_text)\n",
    "        }\n",
    "        \n",
    "        # ê¸¸ì´ ì¡°ê±´\n",
    "        if conditions['max_length'] > self.config['max_length']:\n",
    "            conditions['needs_segmentation'] = True\n",
    "            conditions['reasons'].append('max_length_exceeded')\n",
    "        \n",
    "        # ë¬¸ì¥ ìˆ˜ ì¡°ê±´\n",
    "        max_sentences = max(conditions['source_sentences'], conditions['target_sentences'])\n",
    "        if max_sentences > self.config['max_sentences']:\n",
    "            conditions['needs_segmentation'] = True\n",
    "            conditions['reasons'].append('too_many_sentences')\n",
    "        \n",
    "        # ë³µì¡ë„ ì¡°ê±´\n",
    "        if conditions['complexity_score'] > self.config['complexity_threshold']:\n",
    "            conditions['needs_segmentation'] = True\n",
    "            conditions['reasons'].append('high_complexity')\n",
    "        \n",
    "        return conditions\n",
    "    \n",
    "    def _split_sentences(self, text):\n",
    "        \"\"\"ê°„ë‹¨í•œ ë¬¸ì¥ ë¶„í• \"\"\"\n",
    "        import re\n",
    "        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', text)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    def _calc_complexity(self, text):\n",
    "        \"\"\"ê°„ë‹¨í•œ ë³µì¡ë„ ê³„ì‚°\"\"\"\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        \n",
    "        score = 0.0\n",
    "        word_count = len(text.split())\n",
    "        if word_count > 20:\n",
    "            score += 0.3\n",
    "        \n",
    "        special_ratio = len(re.findall(r'[^\\w\\s]', text)) / len(text)\n",
    "        if special_ratio > 0.2:\n",
    "            score += 0.2\n",
    "        \n",
    "        if re.search(r'\\d+', text):\n",
    "            score += 0.1\n",
    "        if '(' in text and ')' in text:\n",
    "            score += 0.1\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "print(\"ğŸ§ª ê°„ë‹¨í•œ ì˜ë¯¸ ë¶„í•  í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "segmenter = SimpleSemanticSegmenter()\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"ì§§ì€ í…ìŠ¤íŠ¸\",\n",
    "        \"source\": \"Illumina sequencing technology.\",\n",
    "        \"target\": \"ì¼ë£¨ë¯¸ë‚˜ ì‹œí€€ì‹± ê¸°ìˆ .\",\n",
    "        \"expected\": \"no_split\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ê¸´ ë‹¤ì¤‘ ë¬¸ì¥\",\n",
    "        \"source\": \"Our platforms are trusted globally. Researchers rely on our accuracy. Innovation drives our development.\",\n",
    "        \"target\": \"ìš°ë¦¬ì˜ í”Œë«í¼ì€ ì „ ì„¸ê³„ì ìœ¼ë¡œ ì‹ ë¢°ë°›ê³  ìˆìŠµë‹ˆë‹¤. ì—°êµ¬ìë“¤ì€ ìš°ë¦¬ì˜ ì •í™•ì„±ì„ ì‹ ë¢°í•©ë‹ˆë‹¤. í˜ì‹ ì´ ìš°ë¦¬ì˜ ê°œë°œì„ ì´ë•ë‹ˆë‹¤.\",\n",
    "        \"expected\": \"split\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"\\n--- í…ŒìŠ¤íŠ¸ {i}: {test_case['name']} ---\")\n",
    "    \n",
    "    segments = segmenter.segment_text_pair(\n",
    "        test_case['source'], test_case['target']\n",
    "    )\n",
    "    \n",
    "    is_segmented = len(segments) > 1\n",
    "    result = \"split\" if is_segmented else \"no_split\"\n",
    "    \n",
    "    print(f\"ì˜ˆìƒ: {test_case['expected']}, ì‹¤ì œ: {result}\")\n",
    "    print(f\"ê²°ê³¼: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸\")\n",
    "    \n",
    "    if is_segmented and segments:\n",
    "        conditions = segments[0].get('segmentation_conditions_checked', {})\n",
    "        print(f\"ë¶„í•  ì´ìœ : {', '.join(conditions.get('reasons', []))}\")\n",
    "    \n",
    "    match = \"âœ…\" if result == test_case['expected'] else \"âŒ\"\n",
    "    print(f\"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {match}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 9: ChromaDB ë²¡í„° ì¸ë±ìŠ¤ ì¬ìƒì„±\n",
    "# ===================================================================\n",
    "\n",
    "def rebuild_vector_index_for_segmented_tm(lang_suffix: str):\n",
    "    \"\"\"ë¶„í• ëœ TMì— ëŒ€í•œ ë²¡í„° ì¸ë±ìŠ¤ ì¬ìƒì„±\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ§  {lang_suffix} ë²¡í„° ì¸ë±ìŠ¤ ì¬ìƒì„±\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # ChromaIndexer import ì‹œë„\n",
    "        sys.path.append(os.path.abspath('../src/indexing'))\n",
    "        from chroma_indexer import ChromaIndexer\n",
    "        \n",
    "        indexer = ChromaIndexer()\n",
    "        print(f\"ğŸ”„ ë¶„í• ëœ Clean TMì„ ê¸°ë°˜ìœ¼ë¡œ ë²¡í„° ì¸ë±ìŠ¤ ì¬ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # ì¸ë±ìŠ¤ ìƒì„±\n",
    "        indexer.create_index(lang_suffix)\n",
    "        \n",
    "        print(f\"âœ… ë²¡í„° ì¸ë±ìŠ¤ ì¬ìƒì„± ì™„ë£Œ!\")\n",
    "        print(f\"   - ì»¬ë ‰ì…˜: tm_{lang_suffix}\")\n",
    "        print(f\"   - ì„ë² ë”© ëª¨ë¸: intfloat/multilingual-e5-large\")\n",
    "        print(f\"   - ë¶„í• ëœ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì´ ê°œë³„ì ìœ¼ë¡œ ì¸ë±ì‹±ë˜ì–´ ê²€ìƒ‰ ì •í™•ë„ê°€ í–¥ìƒë©ë‹ˆë‹¤.\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(f\"âŒ ChromaIndexer í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"   ìˆ˜ë™ìœ¼ë¡œ 8_create_chroma_index.ipynbë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë²¡í„° ì¸ë±ìŠ¤ ì¬ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "# ì˜ˆì‹œ ì‹¤í–‰\n",
    "print(\"ğŸ’¡ ë²¡í„° ì¸ë±ìŠ¤ ì¬ìƒì„± í•¨ìˆ˜ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"ì–¸ì–´ ìŒì„ ì„ íƒí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”:\")\n",
    "for i, (source, target) in enumerate(SUPPORTED_LANGUAGE_PAIRS, 1):\n",
    "    lang_suffix = f\"{source}_{target}\"\n",
    "    print(f\"  rebuild_vector_index_for_segmented_tm('{lang_suffix}')\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 10: ì „ì²´ ì–¸ì–´ ìŒ ì¼ê´„ ì²˜ë¦¬ (ì˜µì…˜)\n",
    "# ===================================================================\n",
    "\n",
    "def process_all_language_pairs():\n",
    "    \"\"\"ëª¨ë“  ì§€ì› ì–¸ì–´ ìŒì— ëŒ€í•´ ì˜ë¯¸ ë¶„í•  ì¼ê´„ ì‹¤í–‰\"\"\"\n",
    "    \n",
    "    print(\"ğŸŒ ëª¨ë“  ì–¸ì–´ ìŒ ì˜ë¯¸ ë¶„í•  ì¼ê´„ ì²˜ë¦¬\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"âš ï¸ ì´ ê¸°ëŠ¥ì€ ì‹¤ì œ UltimateTMBuilder í´ë˜ìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "    print(\"í˜„ì¬ëŠ” í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for source_lang, target_lang in SUPPORTED_LANGUAGE_PAIRS:\n",
    "        lang_suffix = f\"{source_lang}_{target_lang}\"\n",
    "        \n",
    "        print(f\"\\nğŸ”„ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜: {lang_suffix}\")\n",
    "        try:\n",
    "            # ì‹¤ì œë¡œëŠ” execute_semantic_segmentation(lang_suffix) í˜¸ì¶œ\n",
    "            print(f\"   âœ… {lang_suffix} ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ì„±ê³µ\")\n",
    "            all_results[lang_suffix] = \"âœ… ì‹œë®¬ë ˆì´ì…˜ ì„±ê³µ\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {lang_suffix} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}\")\n",
    "            all_results[lang_suffix] = f\"âŒ ì‹¤íŒ¨: {str(e)}\"\n",
    "    \n",
    "    print(f\"\\nğŸ‰ ì „ì²´ ì²˜ë¦¬ ê²°ê³¼:\")\n",
    "    for lang_suffix, result in all_results.items():\n",
    "        print(f\"   - {lang_suffix}: {result}\")\n",
    "\n",
    "# ì˜µì…˜ ì œê³µ\n",
    "print(\"ğŸ’¡ ì˜µì…˜: ëª¨ë“  ì–¸ì–´ ìŒì„ í•œ ë²ˆì— ì²˜ë¦¬í•˜ë ¤ë©´:\")\n",
    "print(\"process_all_language_pairs()\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 11: ìµœì¢… ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´\n",
    "# ===================================================================\n",
    "\n",
    "def show_final_summary():\n",
    "    \"\"\"ìµœì¢… ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ë° ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´\"\"\"\n",
    "    \n",
    "    print(\"ğŸ‰ ì˜ë¯¸ ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í•  ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        client = MongoClient(MONGO_CONNECTION_STRING)\n",
    "        db = client[DB_NAME]\n",
    "        \n",
    "        print(\"ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½:\")\n",
    "        \n",
    "        total_clean = 0\n",
    "        total_html = 0\n",
    "        total_segmented = 0\n",
    "        \n",
    "        for source_lang, target_lang in SUPPORTED_LANGUAGE_PAIRS:\n",
    "            lang_suffix = f\"{source_lang}_{target_lang}\"\n",
    "            \n",
    "            try:\n",
    "                # Clean TM í†µê³„ (ìˆëŠ” ê²½ìš°ë§Œ)\n",
    "                clean_collection = db[f\"clean_translation_memory_{lang_suffix}\"]\n",
    "                clean_count = clean_collection.count_documents({})\n",
    "                segmented_count = clean_collection.count_documents({\"is_segmented\": True})\n",
    "                \n",
    "                # HTML Archive í†µê³„\n",
    "                html_collection = db[f\"html_component_archive_{lang_suffix}\"]\n",
    "                html_count = html_collection.count_documents({})\n",
    "                \n",
    "                total_clean += clean_count\n",
    "                total_html += html_count\n",
    "                total_segmented += segmented_count\n",
    "                \n",
    "                if clean_count > 0 or html_count > 0:\n",
    "                    print(f\"\\nğŸŒ {source_lang.upper()}-{target_lang.upper()}:\")\n",
    "                    print(f\"   - Clean TM: {clean_count:,}ê°œ ì„¸ê·¸ë¨¼íŠ¸\")\n",
    "                    print(f\"   - ë¶„í•  ì ìš©: {segmented_count:,}ê°œ\")\n",
    "                    print(f\"   - HTML Archive: {html_count:,}ê°œ ì»´í¬ë„ŒíŠ¸\")\n",
    "            except:\n",
    "                print(f\"\\nğŸŒ {source_lang.upper()}-{target_lang.upper()}: ë°ì´í„° ì—†ìŒ\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ ì „ì²´ ìš”ì•½:\")\n",
    "        print(f\"   - ì´ Clean ì„¸ê·¸ë¨¼íŠ¸: {total_clean:,}ê°œ\")\n",
    "        print(f\"   - ì´ ë¶„í•  ì ìš©: {total_segmented:,}ê°œ\")\n",
    "        print(f\"   - ì´ HTML ë³´ì¡´: {total_html:,}ê°œ\")\n",
    "        \n",
    "        if total_clean > 0:\n",
    "            segmentation_ratio = total_segmented / total_clean * 100\n",
    "            print(f\"   - ì „ì²´ ë¶„í•  ë¹„ìœ¨: {segmentation_ratio:.1f}%\")\n",
    "        \n",
    "        client.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ìš”ì•½ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n",
    "        print(\"   MongoDB ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ TM ë°ì´í„°ë¥¼ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.\")\n",
    "    \n",
    "    print(f\"\\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:\")\n",
    "    print(f\"   1. Streamlit QA Stationì—ì„œ í–¥ìƒëœ ê²€ìƒ‰ ì •í™•ë„ í™•ì¸\")\n",
    "    print(f\"   2. 9_test_chroma_search.ipynbë¡œ ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\")\n",
    "    print(f\"   3. ë¶„í• ëœ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì˜ ê²€ìƒ‰ í’ˆì§ˆ ê²€ì¦\")\n",
    "    print(f\"   4. HTML Archiveì—ì„œ ì¶”ê°€ ì •ì œ ì‘ì—… (ì„ íƒì‚¬í•­)\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ ì£¼ìš” ê°œì„  íš¨ê³¼:\")\n",
    "    print(f\"   - ğŸ¯ ê²€ìƒ‰ ì •í™•ë„: HTML ë…¸ì´ì¦ˆ ì œê±°ë¡œ 30-50% í–¥ìƒ ì˜ˆìƒ\")\n",
    "    print(f\"   - ğŸ“ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´: 70ì ë‚´ì™¸ë¡œ ìµœì í™”\")\n",
    "    print(f\"   - ğŸ§  ì˜ë¯¸ ì¼ê´€ì„±: ë™ì  í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ ìµœì  ë¶„í• \")\n",
    "    print(f\"   - ğŸ”„ ì¬ì‚¬ìš©ì„±: ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• ë˜ì–´ ì¬í™œìš© ìš©ì´\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ ì˜ë¯¸ ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "# ìµœì¢… ìš”ì•½ í‘œì‹œ\n",
    "show_final_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mywork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
