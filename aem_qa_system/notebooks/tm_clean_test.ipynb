{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28b6e528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ\n",
      "ğŸŒ ì§€ì› ì–¸ì–´ ìŒ: [('en', 'ko'), ('en', 'ja')]\n",
      "ğŸ—„ï¸ Database: aem_qa_system\n",
      "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ\n",
      "ğŸ“ CSV ì¶œë ¥ ê²½ë¡œ: /mnt/d/Cloud-Synced/Illumina/OneDrive - Illumina, Inc/aem_qa_system/data/3_processed/clean_tm_csv\n"
     ]
    }
   ],
   "source": [
    "# notebooks/10_clean_translation_memory.ipynb\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ import ë° í™˜ê²½ ì„¤ì •\n",
    "# ===================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# src í´ë”ë¥¼ íŒŒì´ì¬ ê²½ë¡œì— ì¶”ê°€\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ëª¨ë“ˆë“¤\n",
    "from config import SUPPORTED_LANGUAGE_PAIRS, MONGO_CONNECTION_STRING, DB_NAME, PROCESSED_DIR\n",
    "from processors.tm_cleaner import TMCleaner, clean_all_language_pairs, get_cleaning_stats\n",
    "from pymongo import MongoClient\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ\")\n",
    "print(f\"ğŸŒ ì§€ì› ì–¸ì–´ ìŒ: {SUPPORTED_LANGUAGE_PAIRS}\")\n",
    "print(f\"ğŸ—„ï¸ Database: {DB_NAME}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# CSV ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸/ìƒì„±\n",
    "csv_output_dir = os.path.join(PROCESSED_DIR, \"clean_tm_csv\")\n",
    "os.makedirs(csv_output_dir, exist_ok=True)\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ\")\n",
    "print(f\"ğŸ“ CSV ì¶œë ¥ ê²½ë¡œ: {csv_output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 2: Raw TM í˜„í™© í™•ì¸\n",
    "# ===================================================================\n",
    "\n",
    "def preview_raw_tm_samples(lang_suffix: str, limit: int = 5):\n",
    "    \"\"\"Raw TM ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°\"\"\"\n",
    "    client = MongoClient(MONGO_CONNECTION_STRING)\n",
    "    db = client[DB_NAME]\n",
    "    raw_tm_collection = db[f\"translation_memory_{lang_suffix}\"]\n",
    "    \n",
    "    total_count = raw_tm_collection.count_documents({})\n",
    "    print(f\"ğŸ” [{lang_suffix}] Raw TM í˜„í™©: ì´ {total_count:,}ê°œ\")\n",
    "    \n",
    "    if total_count == 0:\n",
    "        print(\"   âš ï¸ Raw TM ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"   ğŸ“‹ ìƒ˜í”Œ {limit}ê°œ:\")\n",
    "    samples = list(raw_tm_collection.find({}).limit(limit))\n",
    "    \n",
    "    for i, sample in enumerate(samples, 1):\n",
    "        source = sample.get('source_text', '')[:80]\n",
    "        target = sample.get('target_text', '')[:80]\n",
    "        print(f\"   {i}. Source: '{source}{'...' if len(sample.get('source_text', '')) > 80 else ''}'\")\n",
    "        print(f\"      Target: '{target}{'...' if len(sample.get('target_text', '')) > 80 else ''}'\")\n",
    "        print(f\"      Page: {sample.get('page_path', 'N/A')}\")\n",
    "        print()\n",
    "\n",
    "# í˜„ì¬ Raw TM í˜„í™© í™•ì¸\n",
    "print(\"ğŸ“Š Raw TM í˜„í™© í™•ì¸:\")\n",
    "for source_lang, target_lang in SUPPORTED_LANGUAGE_PAIRS:\n",
    "    lang_suffix = f\"{source_lang}_{target_lang}\"\n",
    "    print(f\"\\n--- {source_lang.upper()}-{target_lang.upper()} ---\")\n",
    "    preview_raw_tm_samples(lang_suffix, limit=3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 3: ë‹¨ì¼ ì–¸ì–´ ìŒ ì •ì œ (EN-KO)\n",
    "# ===================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë²ˆì—­ ë©”ëª¨ë¦¬ ì •ì œ ì‹œì‘\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TM Cleaner ì´ˆê¸°í™”\n",
    "cleaner = TMCleaner()\n",
    "\n",
    "# EN-KO TM ì •ì œ ì‹¤í–‰\n",
    "ko_result = cleaner.clean_translation_memory(\"en_ko\")\n",
    "\n",
    "print(f\"\\nâœ… í•œêµ­ì–´ TM ì •ì œ ì™„ë£Œ!\")\n",
    "if ko_result.get(\"status\") != \"no_data\":\n",
    "    print(f\"   ì…ë ¥: {ko_result.get('input_count', 0):,}ê°œ\")\n",
    "    print(f\"   ì¶œë ¥: {ko_result.get('output_count', 0):,}ê°œ\")\n",
    "    print(f\"   íš¨ìœ¨: {ko_result.get('cleaning_efficiency', 0):.1%}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28391071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 4: ëª¨ë“  ì–¸ì–´ ìŒ ì •ì œ\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸŒ ëª¨ë“  ì–¸ì–´ ìŒ ë²ˆì—­ ë©”ëª¨ë¦¬ ì •ì œ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ëª¨ë“  ì–¸ì–´ ìŒ ì¼ê´„ ì •ì œ\n",
    "all_results = clean_all_language_pairs()\n",
    "\n",
    "# ì „ì²´ ê²°ê³¼ ìš”ì•½\n",
    "print(\"\\nğŸ“Š ì „ì²´ ì •ì œ ê²°ê³¼ ìš”ì•½:\")\n",
    "print(\"-\" * 50)\n",
    "for lang_suffix, result in all_results.items():\n",
    "    if result.get(\"status\") == \"error\":\n",
    "        print(f\"âŒ {lang_suffix}: {result.get('error', 'Unknown error')}\")\n",
    "    elif result.get(\"status\") == \"no_data\":\n",
    "        print(f\"âš ï¸ {lang_suffix}: Raw TM ë°ì´í„° ì—†ìŒ\")\n",
    "    else:\n",
    "        input_count = result.get('input_count', 0)\n",
    "        output_count = result.get('output_count', 0)\n",
    "        efficiency = result.get('cleaning_efficiency', 0)\n",
    "        print(f\"âœ… {lang_suffix}: {input_count:,} â†’ {output_count:,} ({efficiency:.1%})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d46937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 5: ì •ì œ ê²°ê³¼ ë¹„êµ ë° í’ˆì§ˆ í™•ì¸\n",
    "# ===================================================================\n",
    "\n",
    "def compare_before_after(lang_suffix: str, limit: int = 3):\n",
    "    \"\"\"ì •ì œ ì „í›„ ë¹„êµ\"\"\"\n",
    "    client = MongoClient(MONGO_CONNECTION_STRING)\n",
    "    db = client[DB_NAME]\n",
    "    clean_tm_collection = db[f\"clean_translation_memory_{lang_suffix}\"]\n",
    "    \n",
    "    count = clean_tm_collection.count_documents({})\n",
    "    if count == 0:\n",
    "        print(f\"âš ï¸ [{lang_suffix}] ì •ì œëœ TMì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ğŸ” [{lang_suffix}] ì •ì œ ì „í›„ ë¹„êµ (ì´ {count:,}ê°œ ì¤‘ {limit}ê°œ ìƒ˜í”Œ):\")\n",
    "    samples = list(clean_tm_collection.find({}).limit(limit))\n",
    "    \n",
    "    for i, sample in enumerate(samples, 1):\n",
    "        print(f\"\\n--- ìƒ˜í”Œ {i} ---\")\n",
    "        print(f\"âœ¨ ì •ì œ í›„:\")\n",
    "        print(f\"   Source: '{sample.get('source_text', '')}'\")\n",
    "        print(f\"   Target: '{sample.get('target_text', '')}'\")\n",
    "        print(f\"ğŸ—‘ï¸ ì›ë³¸:\")\n",
    "        print(f\"   Source: '{sample.get('original_source_text', '')[:100]}...'\")\n",
    "        print(f\"   Target: '{sample.get('original_target_text', '')[:100]}...'\")\n",
    "        print(f\"ğŸ“Š í’ˆì§ˆ: {sample.get('quality_score', 'N/A'):.2f} ({sample.get('text_type', 'N/A')})\")\n",
    "\n",
    "print(\"ğŸ“‹ ì •ì œ ê²°ê³¼ í’ˆì§ˆ í™•ì¸:\")\n",
    "for source_lang, target_lang in SUPPORTED_LANGUAGE_PAIRS:\n",
    "    lang_suffix = f\"{source_lang}_{target_lang}\"\n",
    "    print(f\"\\n--- {source_lang.upper()}-{target_lang.upper()} ---\")\n",
    "    compare_before_after(lang_suffix, limit=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 6: ì •ì œ í†µê³„ ìƒì„¸ ë¶„ì„\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“ˆ ì •ì œ í†µê³„ ìƒì„¸ ë¶„ì„\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ê° ì–¸ì–´ ìŒì˜ ìƒì„¸ í†µê³„\n",
    "for source_lang, target_lang in SUPPORTED_LANGUAGE_PAIRS:\n",
    "    lang_suffix = f\"{source_lang}_{target_lang}\"\n",
    "    stats = get_cleaning_stats(lang_suffix)\n",
    "    \n",
    "    if not stats:\n",
    "        print(f\"âš ï¸ {source_lang.upper()}-{target_lang.upper()}: í†µê³„ ë°ì´í„° ì—†ìŒ\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {source_lang.upper()}-{target_lang.upper()} ìƒì„¸ í†µê³„:\")\n",
    "    print(f\"   ğŸ• ì •ì œ ì‹œê°„: {stats['cleaning_date']}\")\n",
    "    print(f\"   ğŸ“ˆ íš¨ìœ¨ì„±: {stats['cleaning_efficiency']:.1%}\")\n",
    "    print(f\"   ğŸ“¥ ì…ë ¥: {stats['input_count']:,}ê°œ\")\n",
    "    print(f\"   ğŸ“¤ ì¶œë ¥: {stats['output_count']:,}ê°œ\")\n",
    "    print(f\"   ğŸ—‘ï¸ ì œê±°: {stats['removed_count']:,}ê°œ\")\n",
    "    \n",
    "    # ì œê±° ì´ìœ  Top 5\n",
    "    print(f\"   ğŸ” ì£¼ìš” ì œê±° ì´ìœ :\")\n",
    "    removal_reasons = sorted(stats['removal_reasons'].items(), key=lambda x: x[1], reverse=True)\n",
    "    for reason, count in removal_reasons[:5]:\n",
    "        percentage = count / stats['input_count'] * 100\n",
    "        print(f\"     - {reason}: {count:,}ê°œ ({percentage:.1f}%)\")\n",
    "    \n",
    "    # í’ˆì§ˆ ë¶„í¬\n",
    "    print(f\"   ğŸ† í’ˆì§ˆ ë¶„í¬:\")\n",
    "    quality_dist = stats['quality_distribution']\n",
    "    for tier in ['high', 'medium', 'low', 'very_low']:\n",
    "        count = quality_dist.get(tier, 0)\n",
    "        if count > 0:\n",
    "            percentage = count / stats['output_count'] * 100\n",
    "            print(f\"     - {tier}: {count:,}ê°œ ({percentage:.1f}%)\")\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ìœ í˜• Top 3\n",
    "    print(f\"   ğŸ“ ì£¼ìš” í…ìŠ¤íŠ¸ ìœ í˜•:\")\n",
    "    text_types = sorted(stats['text_types'].items(), key=lambda x: x[1], reverse=True)\n",
    "    for text_type, count in text_types[:3]:\n",
    "        percentage = count / stats['output_count'] * 100\n",
    "        print(f\"     - {text_type}: {count:,}ê°œ ({percentage:.1f}%)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489de40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 7: MongoDB ì»¬ë ‰ì…˜ í˜„í™© í™•ì¸\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ—„ï¸ MongoDB ì»¬ë ‰ì…˜ í˜„í™©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# MongoDB ì—°ê²° ë° ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ\n",
    "client = MongoClient(MONGO_CONNECTION_STRING)\n",
    "db = client[DB_NAME]\n",
    "collections = db.list_collection_names()\n",
    "\n",
    "# TM ê´€ë ¨ ì»¬ë ‰ì…˜ ë¶„ë¥˜\n",
    "raw_tm_collections = [c for c in collections if c.startswith(\"translation_memory_\") and not c.startswith(\"clean_\")]\n",
    "clean_tm_collections = [c for c in collections if c.startswith(\"clean_translation_memory_\")]\n",
    "stats_collections = [c for c in collections if c.startswith(\"tm_cleaning_stats_\")]\n",
    "\n",
    "print(f\"ğŸ“‹ TM ê´€ë ¨ ì»¬ë ‰ì…˜ í˜„í™©:\")\n",
    "print(f\"   ğŸ“„ Raw TM: {len(raw_tm_collections)}ê°œ\")\n",
    "print(f\"   âœ¨ Clean TM: {len(clean_tm_collections)}ê°œ\")\n",
    "print(f\"   ğŸ“Š í†µê³„: {len(stats_collections)}ê°œ\")\n",
    "\n",
    "# ì»¬ë ‰ì…˜ë³„ ë¬¸ì„œ ìˆ˜ì™€ ì €ì¥ ê³µê°„ ì •ë³´\n",
    "print(f\"\\nğŸ“Š ì»¬ë ‰ì…˜ë³„ ìƒì„¸ ì •ë³´:\")\n",
    "print(f\"{'íƒ€ì…':<12} {'ì»¬ë ‰ì…˜ëª…':<35} {'ë¬¸ì„œìˆ˜':<12} {'ìƒíƒœ'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for collection_name in sorted(raw_tm_collections + clean_tm_collections + stats_collections):\n",
    "    count = db[collection_name].count_documents({})\n",
    "    \n",
    "    if collection_name.startswith(\"translation_memory_\"):\n",
    "        col_type = \"Raw TM\"\n",
    "    elif collection_name.startswith(\"clean_translation_memory_\"):\n",
    "        col_type = \"Clean TM\"\n",
    "    else:\n",
    "        col_type = \"í†µê³„\"\n",
    "    \n",
    "    status = \"âœ… ì •ìƒ\" if count > 0 else \"âš ï¸ ë¹„ì–´ìˆìŒ\"\n",
    "    print(f\"{col_type:<12} {collection_name:<35} {count:>8,}ê°œ {status}\")\n",
    "\n",
    "# ì •ì œ íš¨ìœ¨ì„± ìš”ì•½\n",
    "print(f\"\\nğŸ“ˆ ì „ì²´ ì •ì œ íš¨ìœ¨ì„± ìš”ì•½:\")\n",
    "total_raw = sum(db[c].count_documents({}) for c in raw_tm_collections)\n",
    "total_clean = sum(db[c].count_documents({}) for c in clean_tm_collections)\n",
    "\n",
    "if total_raw > 0:\n",
    "    overall_efficiency = total_clean / total_raw\n",
    "    print(f\"   ì „ì²´ Raw TM: {total_raw:,}ê°œ\")\n",
    "    print(f\"   ì „ì²´ Clean TM: {total_clean:,}ê°œ\")\n",
    "    print(f\"   ì „ì²´ íš¨ìœ¨ì„±: {overall_efficiency:.1%}\")\n",
    "    print(f\"   ì œê±°ëœ ë…¸ì´ì¦ˆ: {total_raw - total_clean:,}ê°œ\")\n",
    "else:\n",
    "    print(\"   âš ï¸ Raw TM ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\nâœ… TM ì •ì œ ì‘ì—… ë° ë¶„ì„ ì™„ë£Œ!\")\n",
    "print(\"ğŸ’¡ ì´ì œ clean_translation_memory_* ì»¬ë ‰ì…˜ì„ ì‚¬ìš©í•˜ì—¬ AI ê²€ìƒ‰ ì¸ë±ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7687e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 8: ì •ì œëœ TM ìƒ˜í”Œ í™•ì¸ (ìµœì¢… ê²€ì¦)\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ” ì •ì œëœ TM ìµœì¢… ê²€ì¦\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def verify_clean_tm_quality(lang_suffix: str, sample_size: int = 10):\n",
    "    \"\"\"ì •ì œëœ TM í’ˆì§ˆ ê²€ì¦\"\"\"\n",
    "    client = MongoClient(MONGO_CONNECTION_STRING)\n",
    "    db = client[DB_NAME]\n",
    "    clean_tm_collection = db[f\"clean_translation_memory_{lang_suffix}\"]\n",
    "    \n",
    "    print(f\"ğŸ” [{lang_suffix}] ì •ì œëœ TM í’ˆì§ˆ ê²€ì¦:\")\n",
    "    \n",
    "    # ëœë¤ ìƒ˜í”Œ ì¶”ì¶œ\n",
    "    pipeline = [{\"$sample\": {\"size\": sample_size}}]\n",
    "    samples = list(clean_tm_collection.aggregate(pipeline))\n",
    "    \n",
    "    if not samples:\n",
    "        print(\"   âš ï¸ ì •ì œëœ TMì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    # í’ˆì§ˆ ê²€ì¦\n",
    "    high_quality = 0\n",
    "    has_meaningful_text = 0\n",
    "    proper_length = 0\n",
    "    \n",
    "    print(f\"   ğŸ“‹ {len(samples)}ê°œ ìƒ˜í”Œ ê²€ì¦ ê²°ê³¼:\")\n",
    "    \n",
    "    for sample in samples:\n",
    "        source = sample.get('source_text', '')\n",
    "        target = sample.get('target_text', '')\n",
    "        quality = sample.get('quality_score', 0)\n",
    "        \n",
    "        # ê³ í’ˆì§ˆ ì²´í¬\n",
    "        if quality >= 0.5:\n",
    "            high_quality += 1\n",
    "        \n",
    "        # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ ì²´í¬\n",
    "        if len(source.strip()) > 2 and len(target.strip()) > 2:\n",
    "            has_meaningful_text += 1\n",
    "        \n",
    "        # ì ì ˆí•œ ê¸¸ì´ ì²´í¬\n",
    "        if 2 <= len(source.split()) <= 100:\n",
    "            proper_length += 1\n",
    "    \n",
    "    print(f\"     âœ… ê³ í’ˆì§ˆ (â‰¥0.5): {high_quality}/{len(samples)} ({high_quality/len(samples)*100:.1f}%)\")\n",
    "    print(f\"     âœ… ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸: {has_meaningful_text}/{len(samples)} ({has_meaningful_text/len(samples)*100:.1f}%)\")\n",
    "    print(f\"     âœ… ì ì ˆí•œ ê¸¸ì´: {proper_length}/{len(samples)} ({proper_length/len(samples)*100:.1f}%)\")\n",
    "    \n",
    "    # ìš°ìˆ˜ ìƒ˜í”Œ 3ê°œ í‘œì‹œ\n",
    "    high_quality_samples = [s for s in samples if s.get('quality_score', 0) >= 0.7]\n",
    "    if high_quality_samples:\n",
    "        print(f\"   ğŸ† ìš°ìˆ˜ í’ˆì§ˆ ìƒ˜í”Œ:\")\n",
    "        for i, sample in enumerate(high_quality_samples[:3], 1):\n",
    "            print(f\"     {i}. '{sample.get('source_text', '')}' â†’ '{sample.get('target_text', '')}'\")\n",
    "            print(f\"        (í’ˆì§ˆ: {sample.get('quality_score', 0):.2f}, ìœ í˜•: {sample.get('text_type', 'N/A')})\")\n",
    "\n",
    "# ê° ì–¸ì–´ ìŒ ê²€ì¦\n",
    "for source_lang, target_lang in SUPPORTED_LANGUAGE_PAIRS:\n",
    "    lang_suffix = f\"{source_lang}_{target_lang}\"\n",
    "    verify_clean_tm_quality(lang_suffix, sample_size=20)\n",
    "    print()\n",
    "\n",
    "print(\"ğŸ‰ TM ì •ì œ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!\")\n",
    "print(\"ğŸ“ ë‹¤ìŒ ë‹¨ê³„: ChromaDB ì¸ë±ìŠ¤ë¥¼ ì •ì œëœ TMìœ¼ë¡œ ì—…ë°ì´íŠ¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8afd7371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 2: Clean TMì„ CSVë¡œ ë‚´ë³´ë‚´ê¸° í•¨ìˆ˜\n",
    "# ===================================================================\n",
    "\n",
    "def export_clean_tm_to_csv(lang_suffix: str, include_metadata: bool = True) -> str:\n",
    "    \"\"\"ì •ì œëœ TMì„ CSV íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°\"\"\"\n",
    "    \n",
    "    client = MongoClient(MONGO_CONNECTION_STRING)\n",
    "    db = client[DB_NAME]\n",
    "    clean_tm_collection = db[f\"clean_translation_memory_{lang_suffix}\"]\n",
    "    \n",
    "    print(f\"ğŸ“¤ [{lang_suffix}] Clean TM CSV ë‚´ë³´ë‚´ê¸° ì‹œì‘...\")\n",
    "    \n",
    "    # ë°ì´í„° ì¡°íšŒ\n",
    "    cursor = clean_tm_collection.find({})\n",
    "    docs = list(cursor)\n",
    "    \n",
    "    if not docs:\n",
    "        print(f\"   âš ï¸ {lang_suffix} Clean TM ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"   ğŸ“Š ì´ {len(docs):,}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\")\n",
    "    \n",
    "    # DataFrame ìƒì„±ìš© ë°ì´í„° ì¤€ë¹„\n",
    "    csv_data = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        row = {\n",
    "            'source_text': doc.get('source_text', ''),\n",
    "            'target_text': doc.get('target_text', ''),\n",
    "            'quality_score': doc.get('quality_score', 0),\n",
    "            'text_type': doc.get('text_type', ''),\n",
    "            'word_count_source': doc.get('word_count_source', 0),\n",
    "            'word_count_target': doc.get('word_count_target', 0),\n",
    "        }\n",
    "        \n",
    "        if include_metadata:\n",
    "            row.update({\n",
    "                'page_path': doc.get('page_path', ''),\n",
    "                'component_path': doc.get('component_path', ''),\n",
    "                'component_type': doc.get('component_type', ''),\n",
    "                'version_name_source': doc.get('version_name', ''),  # ì†ŒìŠ¤ ë²„ì „\n",
    "                'version_number': doc.get('version_number', 1),\n",
    "                'original_source_text': doc.get('original_source_text', ''),\n",
    "                'original_target_text': doc.get('original_target_text', ''),\n",
    "                'cleaning_method': doc.get('cleaning_method', ''),\n",
    "                'cleaned_at': doc.get('cleaned_at', '')\n",
    "            })\n",
    "        \n",
    "        csv_data.append(row)\n",
    "    \n",
    "    # DataFrame ìƒì„±\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    \n",
    "    # í’ˆì§ˆ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "    df = df.sort_values(['quality_score', 'word_count_source'], ascending=[False, False])\n",
    "    \n",
    "    # CSV íŒŒì¼ëª… ìƒì„±\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    metadata_suffix = \"_with_metadata\" if include_metadata else \"_simple\"\n",
    "    filename = f\"clean_tm_{lang_suffix}{metadata_suffix}_{timestamp}.csv\"\n",
    "    filepath = os.path.join(csv_output_dir, filename)\n",
    "    \n",
    "    # CSV ì €ì¥\n",
    "    df.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"   âœ… CSV ì €ì¥ ì™„ë£Œ: {filename}\")\n",
    "    print(f\"   ğŸ“Š ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ\")\n",
    "    print(f\"   ğŸ“‹ í–‰ ìˆ˜: {len(df):,}ê°œ\")\n",
    "    print(f\"   ğŸ’¾ íŒŒì¼ í¬ê¸°: {os.path.getsize(filepath) / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def export_tm_statistics_to_csv() -> str:\n",
    "    \"\"\"TM ì •ì œ í†µê³„ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸°\"\"\"\n",
    "    \n",
    "    client = MongoClient(MONGO_CONNECTION_STRING)\n",
    "    db = client[DB_NAME]\n",
    "    \n",
    "    print(\"ğŸ“Š TM ì •ì œ í†µê³„ CSV ë‚´ë³´ë‚´ê¸°...\")\n",
    "    \n",
    "    stats_data = []\n",
    "    \n",
    "    for source_lang, target_lang in SUPPORTED_LANGUAGE_PAIRS:\n",
    "        lang_suffix = f\"{source_lang}_{target_lang}\"\n",
    "        stats_collection = db[f\"tm_cleaning_stats_{lang_suffix}\"]\n",
    "        \n",
    "        stats_doc = stats_collection.find_one({}, sort=[(\"cleaning_date\", -1)])\n",
    "        \n",
    "        if stats_doc:\n",
    "            # ì œê±° ì´ìœ ë¥¼ ê°œë³„ ì»¬ëŸ¼ìœ¼ë¡œ ë¶„ë¦¬\n",
    "            removal_reasons = stats_doc.get('removal_reasons', {})\n",
    "            quality_dist = stats_doc.get('quality_distribution', {})\n",
    "            text_types = stats_doc.get('text_types', {})\n",
    "            \n",
    "            row = {\n",
    "                'language_pair': lang_suffix,\n",
    "                'source_language': source_lang,\n",
    "                'target_language': target_lang,\n",
    "                'cleaning_date': stats_doc.get('cleaning_date', ''),\n",
    "                'input_count': stats_doc.get('input_count', 0),\n",
    "                'output_count': stats_doc.get('output_count', 0),\n",
    "                'removed_count': stats_doc.get('removed_count', 0),\n",
    "                'cleaning_efficiency': stats_doc.get('cleaning_efficiency', 0),\n",
    "                \n",
    "                # ì œê±° ì´ìœ ë³„ ì¹´ìš´íŠ¸\n",
    "                'removed_empty_source_text': removal_reasons.get('empty_source_text', 0),\n",
    "                'removed_empty_target_text': removal_reasons.get('empty_target_text', 0),\n",
    "                'removed_source_too_short': removal_reasons.get('source_too_short', 0),\n",
    "                'removed_target_too_short': removal_reasons.get('target_too_short', 0),\n",
    "                'removed_jcr_sling_properties': removal_reasons.get('jcr_sling_properties', 0),\n",
    "                'removed_no_meaningful_chars': removal_reasons.get('source_no_meaningful_chars', 0) + removal_reasons.get('target_no_meaningful_chars', 0),\n",
    "                'removed_only_numbers_symbols': removal_reasons.get('source_only_numbers_symbols', 0) + removal_reasons.get('target_only_numbers_symbols', 0),\n",
    "                \n",
    "                # í’ˆì§ˆ ë¶„í¬\n",
    "                'quality_high': quality_dist.get('high', 0),\n",
    "                'quality_medium': quality_dist.get('medium', 0),\n",
    "                'quality_low': quality_dist.get('low', 0),\n",
    "                'quality_very_low': quality_dist.get('very_low', 0),\n",
    "                \n",
    "                # í…ìŠ¤íŠ¸ ìœ í˜• (ì£¼ìš” 3ê°œ)\n",
    "                'text_type_title': text_types.get('title', 0),\n",
    "                'text_type_sentence': text_types.get('sentence', 0),\n",
    "                'text_type_plain_text': text_types.get('plain_text', 0),\n",
    "                'text_type_others': sum(text_types.values()) - text_types.get('title', 0) - text_types.get('sentence', 0) - text_types.get('plain_text', 0)\n",
    "            }\n",
    "            \n",
    "            stats_data.append(row)\n",
    "    \n",
    "    if not stats_data:\n",
    "        print(\"   âš ï¸ ì •ì œ í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    # DataFrame ìƒì„± ë° ì €ì¥\n",
    "    df = pd.DataFrame(stats_data)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f\"tm_cleaning_statistics_{timestamp}.csv\"\n",
    "    filepath = os.path.join(csv_output_dir, filename)\n",
    "    \n",
    "    df.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"   âœ… í†µê³„ CSV ì €ì¥ ì™„ë£Œ: {filename}\")\n",
    "    print(f\"   ğŸ“Š ì–¸ì–´ ìŒ ìˆ˜: {len(df)}ê°œ\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c2fa757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Cell 3: í’ˆì§ˆë³„ ìƒ˜í”Œ CSV ìƒì„±\n",
    "# ===================================================================\n",
    "\n",
    "def export_quality_samples_to_csv(lang_suffix: str, samples_per_tier: int = 50) -> str:\n",
    "    \"\"\"í’ˆì§ˆ ë“±ê¸‰ë³„ ìƒ˜í”Œì„ CSVë¡œ ë‚´ë³´ë‚´ê¸°\"\"\"\n",
    "    \n",
    "    client = MongoClient(MONGO_CONNECTION_STRING)\n",
    "    db = client[DB_NAME]\n",
    "    clean_tm_collection = db[f\"clean_translation_memory_{lang_suffix}\"]\n",
    "    \n",
    "    print(f\"ğŸ† [{lang_suffix}] í’ˆì§ˆë³„ ìƒ˜í”Œ CSV ìƒì„±...\")\n",
    "    \n",
    "    quality_tiers = ['high', 'medium', 'low', 'very_low']\n",
    "    all_samples = []\n",
    "    \n",
    "    for tier in quality_tiers:\n",
    "        print(f\"   ğŸ” {tier} í’ˆì§ˆ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...\")\n",
    "        \n",
    "        # í•´ë‹¹ í’ˆì§ˆ ë“±ê¸‰ì˜ ë¬¸ì„œë“¤ì„ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§\n",
    "        pipeline = [\n",
    "            {\"$match\": {\"quality_score\": {\"$exists\": True}}},\n",
    "            {\"$addFields\": {\n",
    "                \"quality_tier\": {\n",
    "                    \"$switch\": {\n",
    "                        \"branches\": [\n",
    "                            {\"case\": {\"$gte\": [\"$quality_score\", 0.8]}, \"then\": \"high\"},\n",
    "                            {\"case\": {\"$gte\": [\"$quality_score\", 0.5]}, \"then\": \"medium\"},\n",
    "                            {\"case\": {\"$gte\": [\"$quality_score\", 0.2]}, \"then\": \"low\"}\n",
    "                        ],\n",
    "                        \"default\": \"very_low\"\n",
    "                    }\n",
    "                }\n",
    "            }},\n",
    "            {\"$match\": {\"quality_tier\": tier}},\n",
    "            {\"$sample\": {\"size\": samples_per_tier}},\n",
    "            {\"$sort\": {\"quality_score\": -1}}\n",
    "        ]\n",
    "        \n",
    "        samples = list(clean_tm_collection.aggregate(pipeline))\n",
    "        \n",
    "        for sample in samples:\n",
    "            sample_data = {\n",
    "                'quality_tier': tier,\n",
    "                'quality_score': sample.get('quality_score', 0),\n",
    "                'source_text': sample.get('source_text', ''),\n",
    "                'target_text': sample.get('target_text', ''),\n",
    "                'text_type': sample.get('text_type', ''),\n",
    "                'word_count_source': sample.get('word_count_source', 0),\n",
    "                'word_count_target': sample.get('word_count_target', 0),\n",
    "                'page_path': sample.get('page_path', ''),\n",
    "                'component_type': sample.get('component_type', ''),\n",
    "                'original_source_text': sample.get('original_source_text', '')[:200],  # ì›ë³¸ì€ 200ìë§Œ\n",
    "                'original_target_text': sample.get('original_target_text', '')[:200]   # ì›ë³¸ì€ 200ìë§Œ\n",
    "            }\n",
    "            all_samples.append(sample_data)\n",
    "        \n",
    "        print(f\"     âœ… {len(samples)}ê°œ ìƒ˜í”Œ ì¶”ì¶œ\")\n",
    "    \n",
    "    if not all_samples:\n",
    "        print(f\"   âš ï¸ {lang_suffix} í’ˆì§ˆ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    # DataFrame ìƒì„± ë° ì €ì¥\n",
    "    df = pd.DataFrame(all_samples)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f\"clean_tm_quality_samples_{lang_suffix}_{timestamp}.csv\"\n",
    "    filepath = os.path.join(csv_output_dir, filename)\n",
    "    \n",
    "    df.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"   âœ… í’ˆì§ˆ ìƒ˜í”Œ CSV ì €ì¥ ì™„ë£Œ: {filename}\")\n",
    "    print(f\"   ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {len(df):,}ê°œ\")\n",
    "    \n",
    "    # í’ˆì§ˆë³„ ë¶„í¬ ì¶œë ¥\n",
    "    tier_counts = df['quality_tier'].value_counts()\n",
    "    for tier in quality_tiers:\n",
    "        count = tier_counts.get(tier, 0)\n",
    "        print(f\"     - {tier}: {count}ê°œ\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1183bef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì •ì œëœ TM CSV ë‚´ë³´ë‚´ê¸° ì‹œì‘\n",
      "==================================================\n",
      "\n",
      "ğŸ“¤ EN-KO TM ë‚´ë³´ë‚´ê¸°...\n",
      "ğŸ“¤ [en_ko] Clean TM CSV ë‚´ë³´ë‚´ê¸° ì‹œì‘...\n",
      "   ğŸ“Š ì´ 13,743ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\n",
      "   âœ… CSV ì €ì¥ ì™„ë£Œ: clean_tm_en_ko_with_metadata_20250812_111944.csv\n",
      "   ğŸ“Š ì»¬ëŸ¼ ìˆ˜: 15ê°œ\n",
      "   ğŸ“‹ í–‰ ìˆ˜: 13,743ê°œ\n",
      "   ğŸ’¾ íŒŒì¼ í¬ê¸°: 12.0 MB\n",
      "ğŸ“¤ [en_ko] Clean TM CSV ë‚´ë³´ë‚´ê¸° ì‹œì‘...\n",
      "   ğŸ“Š ì´ 13,743ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\n",
      "   âœ… CSV ì €ì¥ ì™„ë£Œ: clean_tm_en_ko_simple_20250812_111944.csv\n",
      "   ğŸ“Š ì»¬ëŸ¼ ìˆ˜: 6ê°œ\n",
      "   ğŸ“‹ í–‰ ìˆ˜: 13,743ê°œ\n",
      "   ğŸ’¾ íŒŒì¼ í¬ê¸°: 4.3 MB\n",
      "\n",
      "ğŸ“¤ EN-JA TM ë‚´ë³´ë‚´ê¸°...\n",
      "ğŸ“¤ [en_ja] Clean TM CSV ë‚´ë³´ë‚´ê¸° ì‹œì‘...\n",
      "   ğŸ“Š ì´ 14,492ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\n",
      "   âœ… CSV ì €ì¥ ì™„ë£Œ: clean_tm_en_ja_with_metadata_20250812_111945.csv\n",
      "   ğŸ“Š ì»¬ëŸ¼ ìˆ˜: 15ê°œ\n",
      "   ğŸ“‹ í–‰ ìˆ˜: 14,492ê°œ\n",
      "   ğŸ’¾ íŒŒì¼ í¬ê¸°: 14.0 MB\n",
      "ğŸ“¤ [en_ja] Clean TM CSV ë‚´ë³´ë‚´ê¸° ì‹œì‘...\n",
      "   ğŸ“Š ì´ 14,492ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ\n",
      "   âœ… CSV ì €ì¥ ì™„ë£Œ: clean_tm_en_ja_simple_20250812_111945.csv\n",
      "   ğŸ“Š ì»¬ëŸ¼ ìˆ˜: 6ê°œ\n",
      "   ğŸ“‹ í–‰ ìˆ˜: 14,492ê°œ\n",
      "   ğŸ’¾ íŒŒì¼ í¬ê¸°: 5.1 MB\n",
      "\n",
      "ğŸ“Š ì •ì œ í†µê³„ ë‚´ë³´ë‚´ê¸°...\n",
      "ğŸ“Š TM ì •ì œ í†µê³„ CSV ë‚´ë³´ë‚´ê¸°...\n",
      "   âœ… í†µê³„ CSV ì €ì¥ ì™„ë£Œ: tm_cleaning_statistics_20250812_111945.csv\n",
      "   ğŸ“Š ì–¸ì–´ ìŒ ìˆ˜: 2ê°œ\n",
      "\n",
      "âœ… CSV ë‚´ë³´ë‚´ê¸° ì™„ë£Œ!\n",
      "ğŸ“ ì¶œë ¥ ê²½ë¡œ: /mnt/d/Cloud-Synced/Illumina/OneDrive - Illumina, Inc/aem_qa_system/data/3_processed/clean_tm_csv\n",
      "ğŸ“„ ìƒì„±ëœ íŒŒì¼: 5ê°œ\n",
      "   - clean_tm_en_ko_with_metadata_20250812_111944.csv (12.0 MB)\n",
      "   - clean_tm_en_ko_simple_20250812_111944.csv (4.3 MB)\n",
      "   - clean_tm_en_ja_with_metadata_20250812_111945.csv (14.0 MB)\n",
      "   - clean_tm_en_ja_simple_20250812_111945.csv (5.1 MB)\n",
      "   - tm_cleaning_statistics_20250812_111945.csv (0.0 MB)\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Cell 4: ì „ì²´ Clean TM CSV ë‚´ë³´ë‚´ê¸° ì‹¤í–‰\n",
    "# ===================================================================\n",
    "\n",
    "print(\"ğŸš€ ì •ì œëœ TM CSV ë‚´ë³´ë‚´ê¸° ì‹œì‘\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "exported_files = []\n",
    "\n",
    "# 1. ê° ì–¸ì–´ ìŒì˜ Clean TM ë‚´ë³´ë‚´ê¸°\n",
    "for source_lang, target_lang in SUPPORTED_LANGUAGE_PAIRS:\n",
    "    lang_suffix = f\"{source_lang}_{target_lang}\"\n",
    "    \n",
    "    print(f\"\\nğŸ“¤ {source_lang.upper()}-{target_lang.upper()} TM ë‚´ë³´ë‚´ê¸°...\")\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„° í¬í•¨ ë²„ì „\n",
    "    filepath_full = export_clean_tm_to_csv(lang_suffix, include_metadata=True)\n",
    "    if filepath_full:\n",
    "        exported_files.append(filepath_full)\n",
    "    \n",
    "    # ê°„ë‹¨ ë²„ì „ (ë²ˆì—­ìŒ + í’ˆì§ˆ ì •ë³´ë§Œ)\n",
    "    filepath_simple = export_clean_tm_to_csv(lang_suffix, include_metadata=False)\n",
    "    if filepath_simple:\n",
    "        exported_files.append(filepath_simple)\n",
    "\n",
    "# 2. ì •ì œ í†µê³„ ë‚´ë³´ë‚´ê¸°\n",
    "print(f\"\\nğŸ“Š ì •ì œ í†µê³„ ë‚´ë³´ë‚´ê¸°...\")\n",
    "stats_filepath = export_tm_statistics_to_csv()\n",
    "if stats_filepath:\n",
    "    exported_files.append(stats_filepath)\n",
    "\n",
    "print(f\"\\nâœ… CSV ë‚´ë³´ë‚´ê¸° ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“ ì¶œë ¥ ê²½ë¡œ: {csv_output_dir}\")\n",
    "print(f\"ğŸ“„ ìƒì„±ëœ íŒŒì¼: {len(exported_files)}ê°œ\")\n",
    "\n",
    "for filepath in exported_files:\n",
    "    filename = os.path.basename(filepath)\n",
    "    size_mb = os.path.getsize(filepath) / 1024 / 1024\n",
    "    print(f\"   - {filename} ({size_mb:.1f} MB)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4028f8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ† í’ˆì§ˆë³„ ìƒ˜í”Œ CSV ìƒì„±\n",
      "==================================================\n",
      "\n",
      "ğŸ” EN-KO í’ˆì§ˆ ìƒ˜í”Œ...\n",
      "ğŸ† [en_ko] í’ˆì§ˆë³„ ìƒ˜í”Œ CSV ìƒì„±...\n",
      "   ğŸ” high í’ˆì§ˆ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...\n",
      "     âœ… 30ê°œ ìƒ˜í”Œ ì¶”ì¶œ\n",
      "   ğŸ” medium í’ˆì§ˆ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...\n",
      "     âœ… 30ê°œ ìƒ˜í”Œ ì¶”ì¶œ\n",
      "   ğŸ” low í’ˆì§ˆ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...\n",
      "     âœ… 30ê°œ ìƒ˜í”Œ ì¶”ì¶œ\n",
      "   ğŸ” very_low í’ˆì§ˆ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...\n",
      "     âœ… 0ê°œ ìƒ˜í”Œ ì¶”ì¶œ\n",
      "   âœ… í’ˆì§ˆ ìƒ˜í”Œ CSV ì €ì¥ ì™„ë£Œ: clean_tm_quality_samples_en_ko_20250812_112206.csv\n",
      "   ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: 90ê°œ\n",
      "     - high: 30ê°œ\n",
      "     - medium: 30ê°œ\n",
      "     - low: 30ê°œ\n",
      "     - very_low: 0ê°œ\n",
      "\n",
      "ğŸ” EN-JA í’ˆì§ˆ ìƒ˜í”Œ...\n",
      "ğŸ† [en_ja] í’ˆì§ˆë³„ ìƒ˜í”Œ CSV ìƒì„±...\n",
      "   ğŸ” high í’ˆì§ˆ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...\n",
      "     âœ… 30ê°œ ìƒ˜í”Œ ì¶”ì¶œ\n",
      "   ğŸ” medium í’ˆì§ˆ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...\n",
      "     âœ… 30ê°œ ìƒ˜í”Œ ì¶”ì¶œ\n",
      "   ğŸ” low í’ˆì§ˆ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...\n",
      "     âœ… 30ê°œ ìƒ˜í”Œ ì¶”ì¶œ\n",
      "   ğŸ” very_low í’ˆì§ˆ ìƒ˜í”Œ ì¶”ì¶œ ì¤‘...\n",
      "     âœ… 0ê°œ ìƒ˜í”Œ ì¶”ì¶œ\n",
      "   âœ… í’ˆì§ˆ ìƒ˜í”Œ CSV ì €ì¥ ì™„ë£Œ: clean_tm_quality_samples_en_ja_20250812_112206.csv\n",
      "   ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: 90ê°œ\n",
      "     - high: 30ê°œ\n",
      "     - medium: 30ê°œ\n",
      "     - low: 30ê°œ\n",
      "     - very_low: 0ê°œ\n",
      "\n",
      "âœ… í’ˆì§ˆ ìƒ˜í”Œ CSV ìƒì„± ì™„ë£Œ!\n",
      "   - clean_tm_quality_samples_en_ko_20250812_112206.csv (0.0 MB)\n",
      "   - clean_tm_quality_samples_en_ja_20250812_112206.csv (0.2 MB)\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Cell 5: í’ˆì§ˆë³„ ìƒ˜í”Œ CSV ìƒì„±\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ† í’ˆì§ˆë³„ ìƒ˜í”Œ CSV ìƒì„±\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sample_files = []\n",
    "\n",
    "for source_lang, target_lang in SUPPORTED_LANGUAGE_PAIRS:\n",
    "    lang_suffix = f\"{source_lang}_{target_lang}\"\n",
    "    \n",
    "    print(f\"\\nğŸ” {source_lang.upper()}-{target_lang.upper()} í’ˆì§ˆ ìƒ˜í”Œ...\")\n",
    "    \n",
    "    sample_filepath = export_quality_samples_to_csv(lang_suffix, samples_per_tier=30)\n",
    "    if sample_filepath:\n",
    "        sample_files.append(sample_filepath)\n",
    "\n",
    "print(f\"\\nâœ… í’ˆì§ˆ ìƒ˜í”Œ CSV ìƒì„± ì™„ë£Œ!\")\n",
    "for filepath in sample_files:\n",
    "    filename = os.path.basename(filepath)\n",
    "    size_mb = os.path.getsize(filepath) / 1024 / 1024\n",
    "    print(f\"   - {filename} ({size_mb:.1f} MB)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63e1ec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ‘€ ìƒì„±ëœ CSV íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°\n",
      "==================================================\n",
      "\n",
      "ğŸ“„ clean_tm_en_ko_with_metadata_20250812_111944.csv\n",
      "   ğŸ“Š í¬ê¸°: 13,743í–‰ Ã— 15ì—´\n",
      "   ğŸ“‹ ì»¬ëŸ¼: source_text, target_text, quality_score, text_type, word_count_source...\n",
      "   ğŸ” ì²˜ìŒ 5í–‰:\n",
      "     1. [1.0] 'Circulating tumor DNA (ctDNA) may represent a very...' â†’ 'ìˆœí™˜ ì¢…ì–‘ DNA(circulating tumor DNA, ctDNA)ëŠ” ì°¨ì„¸ëŒ€ ì—¼ê¸°ì„œì—´ë¶„...'\n",
      "     2. [1.0] 'Putting people first and doing the right thing are...' â†’ 'ì˜³ì€ ì¼ì„ í•˜ëŠ” ê²ƒì€ Illuminaì˜ ì •ì²´ì„±ê³¼ Illumina ì‚¬ì—…ì˜ í•µì‹¬ì…ë‹ˆë‹¤. Ill...'\n",
      "     3. [1.0] 'Compute pricing rates for Illumina Connected Analy...' â†’ 'Illumina Connected Analyticsì˜ ì»´í“¨íŒ… ê°€ê²© ì±…ì • ë¹„ìœ¨ì€ ICA ë„ì›€...'\n",
      "     4. [1.0] 'If you currently own a HiSeq 2500, HiSeq 3000/4000...' â†’ 'í˜„ì¬ HiSeq 2500, HiSeq 3000/4000 ë˜ëŠ” HiSeq Xë¥¼ ì†Œìœ í•˜ê³  ìˆë‹¤...'\n",
      "     5. [1.0] 'Run the RNA-Seq workflow (FASTQ only) on the MiSeq...' â†’ 'MiSeq Systemì—ì„œ RNA-Seq ì›Œí¬í”Œë¡œìš°(FASTQë§Œ í•´ë‹¹)ë¥¼ ì‹¤í–‰í•˜ê³  Base...'\n",
      "\n",
      "ğŸ“„ clean_tm_en_ja_with_metadata_20250812_111945.csv\n",
      "   ğŸ“Š í¬ê¸°: 14,492í–‰ Ã— 15ì—´\n",
      "   ğŸ“‹ ì»¬ëŸ¼: source_text, target_text, quality_score, text_type, word_count_source...\n",
      "   ğŸ” ì²˜ìŒ 5í–‰:\n",
      "     1. [1.0] 'To prepare targeted NGS libraries, Illumina DNA Pr...' â†’ 'Illumina DNA Prep with Enrichment Dxã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆNGSãƒ©ã‚¤ãƒ–ãƒ©ãƒª...'\n",
      "     2. [1.0] 'TruSeq DNA PCR-Free with Single Indexes supports 2...' â†’ 'TruSeq DNA PCR-Free with Single Indexesã¯ã€ãƒ­ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆ...'\n",
      "     3. [1.0] 'Illumina RNA Prep with Enrichment, (L) Tagmentatio...' â†’ 'Illumina RNA Prep with Enrichment, (L) Tagmentatio...'\n",
      "     4. [1.0] 'The oncoReveal Multi-Cancer with CNV and RNA Fusio...' â†’ 'oncoReveal Multi-Cancer with CNV and RNA Fusion Pa...'\n",
      "     5. [1.0] 'Buffer cartridges and library tubes are sold separ...' â†’ 'ãƒãƒƒãƒ•ã‚¡ãƒ¼ã‚«ãƒ¼ãƒˆãƒªãƒƒã‚¸ãŠã‚ˆã³ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ¼ãƒãƒ¥ãƒ¼ãƒ–ã¯ã€è©¦è–¬ã‚­ãƒƒãƒˆã¨ã¯åˆ¥å£²ã‚Šã§ã™ã€‚  NovaSeq 6...'\n",
      "\n",
      "ğŸ“„ clean_tm_quality_samples_en_ko_20250812_112206.csv\n",
      "   ğŸ“Š í¬ê¸°: 90í–‰ Ã— 11ì—´\n",
      "   ğŸ“‹ ì»¬ëŸ¼: quality_tier, quality_score, source_text, target_text, text_type...\n",
      "   ğŸ” ì²˜ìŒ 5í–‰:\n",
      "     1. [1.0] 'The BaseSpace Sequence Hub workflow tracks samples...' â†’ 'BaseSpace Sequence Hub ì›Œí¬í”Œë¡œìš°ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤€ë¹„ì—ì„œ ë°ì´í„° ë¶„ì„ê¹Œì§€ ...'\n",
      "     2. [1.0] 'Scale up or down depending on your storage needsâ€”w...' â†’ 'í•˜ë“œì›¨ì–´ ìœ ì§€ ê´€ë¦¬ ë˜ëŠ” IT ì§€ì› ì—†ì´ ìŠ¤í† ë¦¬ì§€ ìš”êµ¬ ì‚¬í•­ì— ë”°ë¼ ê·œëª¨ë¥¼ í™•ì¥ ë˜ëŠ” ì¶•ì†Œ...'\n",
      "     3. [1.0] 'Intuitive, streamlined user interface Plan and sta...' â†’ 'ì§ê´€ì ì´ê³  ê°„ì†Œí™”ëœ ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ Illumina Run Managerë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œí€€...'\n",
      "     4. [1.0] 'Illumina has established an integration process to...' â†’ 'IlluminaëŠ” í†µí•© í”„ë¡œì„¸ìŠ¤ë¥¼ êµ¬ì¶•í•˜ì˜€ìœ¼ë©° í†µí•©ì´ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì² ì €í•˜ê²Œ ì‹œí—˜ë  ìˆ˜ ...'\n",
      "     5. [1.0] 'Transparent logic. Every AI hypothesis is backed b...' â†’ 'íˆ¬ëª…í•œ ë…¼ë¦¬. ëª¨ë“  AI ê°€ì„¤ì€ ë¬¸í—Œê³¼ ë°ì´í„°ë² ì´ìŠ¤ ì¶œì²˜ë¡œ ë’·ë°›ì¹¨ë©ë‹ˆë‹¤. ...'\n",
      "\n",
      "ğŸ“„ clean_tm_quality_samples_en_ja_20250812_112206.csv\n",
      "   ğŸ“Š í¬ê¸°: 90í–‰ Ã— 11ì—´\n",
      "   ğŸ“‹ ì»¬ëŸ¼: quality_tier, quality_score, source_text, target_text, text_type...\n",
      "   ğŸ” ì²˜ìŒ 5í–‰:\n",
      "     1. [1.0] 'Test results, supported by an expertly curated Kno...' â†’ 'Test results, supported by an expertly curated Kno...'\n",
      "     2. [0.8] 'Supports a low input DNA sample requirement of 100...' â†’ '100ï½200 ngç¨‹åº¦ã®å°‘é‡ã®DNAã‚µãƒ³ãƒ—ãƒ«ã§ã€æ•°ç™¾ä¸‡ç®‡æ‰€ã®éºä¼å­åº§ã‚’è§£æå¯èƒ½ã€‚...'\n",
      "     3. [0.8] 'Targeted NGS Empowers Genetic Testing...' â†’ 'Targeted NGS Empowers Genetic Testing...'\n",
      "     4. [0.8] 'Illumina's iRefer program...' â†’ 'Illumina's iRefer program...'\n",
      "     5. [0.8] 'MiSeq Operational Qualification...' â†’ 'MiSeq Operational Qualification...'\n",
      "\n",
      "ğŸ‰ ëª¨ë“  CSV ë‚´ë³´ë‚´ê¸° ì‘ì—… ì™„ë£Œ!\n",
      "ğŸ“‚ íŒŒì¼ ìœ„ì¹˜: /mnt/d/Cloud-Synced/Illumina/OneDrive - Illumina, Inc/aem_qa_system/data/3_processed/clean_tm_csv\n",
      "ğŸ’¡ Excelì´ë‚˜ í…ìŠ¤íŠ¸ ì—ë””í„°ë¡œ íŒŒì¼ì„ ì—´ì–´ì„œ ì •ì œ í’ˆì§ˆì„ í™•ì¸í•´ë³´ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Cell 6: CSV íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ‘€ ìƒì„±ëœ CSV íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def preview_csv_file(filepath: str, rows: int = 5):\n",
    "    \"\"\"CSV íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°\"\"\"\n",
    "    filename = os.path.basename(filepath)\n",
    "    print(f\"\\nğŸ“„ {filename}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"   ğŸ“Š í¬ê¸°: {len(df):,}í–‰ Ã— {len(df.columns)}ì—´\")\n",
    "        print(f\"   ğŸ“‹ ì»¬ëŸ¼: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")\n",
    "        \n",
    "        print(f\"   ğŸ” ì²˜ìŒ {rows}í–‰:\")\n",
    "        for i, (_, row) in enumerate(df.head(rows).iterrows()):\n",
    "            source = str(row.get('source_text', ''))[:50]\n",
    "            target = str(row.get('target_text', ''))[:50]\n",
    "            quality = row.get('quality_score', 'N/A')\n",
    "            print(f\"     {i+1}. [{quality}] '{source}...' â†’ '{target}...'\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ë¯¸ë¦¬ë³´ê¸° ì˜¤ë¥˜: {str(e)}\")\n",
    "\n",
    "# ì£¼ìš” íŒŒì¼ë“¤ ë¯¸ë¦¬ë³´ê¸°\n",
    "preview_files = [f for f in exported_files if 'with_metadata' in f]  # ë©”íƒ€ë°ì´í„° í¬í•¨ íŒŒì¼ë§Œ\n",
    "preview_files.extend(sample_files)  # ìƒ˜í”Œ íŒŒì¼ë“¤\n",
    "\n",
    "for filepath in preview_files[:4]:  # ìµœëŒ€ 4ê°œ íŒŒì¼ë§Œ ë¯¸ë¦¬ë³´ê¸°\n",
    "    preview_csv_file(filepath)\n",
    "\n",
    "print(f\"\\nğŸ‰ ëª¨ë“  CSV ë‚´ë³´ë‚´ê¸° ì‘ì—… ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“‚ íŒŒì¼ ìœ„ì¹˜: {csv_output_dir}\")\n",
    "print(f\"ğŸ’¡ Excelì´ë‚˜ í…ìŠ¤íŠ¸ ì—ë””í„°ë¡œ íŒŒì¼ì„ ì—´ì–´ì„œ ì •ì œ í’ˆì§ˆì„ í™•ì¸í•´ë³´ì„¸ìš”.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mywork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
