{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b80a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/10_clean_translation_memory.ipynb\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ import ë° í™˜ê²½ ì„¤ì •\n",
    "# ===================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import Counter\n",
    "\n",
    "# src í´ë”ë¥¼ íŒŒì´ì¬ ê²½ë¡œì— ì¶”ê°€\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# MongoDB ê´€ë ¨\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ì„¤ì •\n",
    "from config import (\n",
    "    MONGO_CONNECTION_STRING, DB_NAME, SUPPORTED_LANGUAGE_PAIRS\n",
    ")\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ\")\n",
    "print(f\"ğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {os.path.abspath('../')}\")\n",
    "print(f\"ğŸ”— MongoDB: {MONGO_CONNECTION_STRING}\")\n",
    "print(f\"ğŸ—„ï¸ Database: {DB_NAME}\")\n",
    "print(f\"ğŸŒ ì§€ì› ì–¸ì–´ ìŒ: {SUPPORTED_LANGUAGE_PAIRS}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 2: TM Cleaner í´ë˜ìŠ¤ ì •ì˜\n",
    "# ===================================================================\n",
    "\n",
    "class TMCleaner:\n",
    "    \"\"\"ë²ˆì—­ ë©”ëª¨ë¦¬ ì •ì œ í´ë˜ìŠ¤ - ë©”íƒ€ë°ì´í„° ì™„ì „ ë³´ì¡´\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = MongoClient(MONGO_CONNECTION_STRING)\n",
    "        self.db = self.client[DB_NAME]\n",
    "        \n",
    "        # ì •ì œ ê·œì¹™ ì •ì˜\n",
    "        self.noise_patterns = [\n",
    "            r'<[^>]+>',                    # HTML íƒœê·¸\n",
    "            r'jcr:[a-zA-Z_]+',             # JCR ì†ì„±\n",
    "            r'sling:[a-zA-Z_]+',           # Sling ì†ì„±\n",
    "            r'cq:[a-zA-Z_]+',              # CQ ì†ì„±\n",
    "            r'dam:[a-zA-Z_]+',             # DAM ì†ì„±\n",
    "            r'^\\s*$',                      # ë¹ˆ ë¬¸ìì—´\n",
    "            r'^[\\s\\n\\t\\r]+$',              # ê³µë°±ë§Œ\n",
    "            r'^[^\\w\\sê°€-í£ã-ã‚Ÿã‚¡-ãƒ¾ä¸€-é¾¯]+$',  # íŠ¹ìˆ˜ë¬¸ìë§Œ (ë‹¤êµ­ì–´ ì§€ì›)\n",
    "        ]\n",
    "        \n",
    "        # ì¶”ì¶œí•  í…ìŠ¤íŠ¸ í‚¤ë“¤ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)\n",
    "        self.valuable_keys = [\n",
    "            'text', 'jcr:title', 'title', 'alt', 'linkText', \n",
    "            'label', 'placeholder', 'value', 'description'\n",
    "        ]\n",
    "        \n",
    "        print(\"âœ… TM Cleaner ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "    \n",
    "    def clean_translation_memory(self, lang_suffix: str) -> Dict:\n",
    "        \"\"\"ë²ˆì—­ ë©”ëª¨ë¦¬ ì •ì œ ë©”ì¸ í•¨ìˆ˜\"\"\"\n",
    "        print(f\"ğŸ§¹ [{lang_suffix}] ë²ˆì—­ ë©”ëª¨ë¦¬ ì •ì œ ì‹œì‘...\")\n",
    "        \n",
    "        # ì»¬ë ‰ì…˜ ì´ë¦„ ì„¤ì •\n",
    "        raw_tm_collection_name = f\"translation_memory_{lang_suffix}\"\n",
    "        clean_tm_collection_name = f\"clean_translation_memory_{lang_suffix}\"\n",
    "        stats_collection_name = f\"tm_cleaning_stats_{lang_suffix}\"\n",
    "        \n",
    "        # ì»¬ë ‰ì…˜ ì°¸ì¡°\n",
    "        raw_tm_collection = self.db[raw_tm_collection_name]\n",
    "        clean_tm_collection = self.db[clean_tm_collection_name]\n",
    "        stats_collection = self.db[stats_collection_name]\n",
    "        \n",
    "        # ê¸°ì¡´ ì •ì œëœ TM ì‚­ì œ (ìƒˆë¡œ ì‹œì‘)\n",
    "        clean_tm_collection.delete_many({})\n",
    "        print(f\"   - ê¸°ì¡´ ì •ì œ TM ì‚­ì œ ì™„ë£Œ\")\n",
    "        \n",
    "        # Raw TM ë°ì´í„° ë¡œë“œ\n",
    "        print(f\"   - Raw TM ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "        raw_tm_docs = list(raw_tm_collection.find({}))\n",
    "        print(f\"   - ì´ {len(raw_tm_docs)}ê°œ Raw TM ë¡œë“œ ì™„ë£Œ\")\n",
    "        \n",
    "        if not raw_tm_docs:\n",
    "            print(\"âš ï¸ Raw TM ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return {\"status\": \"no_data\"}\n",
    "        \n",
    "        # ì •ì œ ì²˜ë¦¬\n",
    "        cleaned_docs = []\n",
    "        stats = {\n",
    "            \"input_count\": len(raw_tm_docs),\n",
    "            \"output_count\": 0,\n",
    "            \"removed_count\": 0,\n",
    "            \"removal_reasons\": Counter(),\n",
    "            \"quality_distribution\": Counter(),\n",
    "            \"text_types\": Counter()\n",
    "        }\n",
    "        \n",
    "        print(f\"   - í…ìŠ¤íŠ¸ ì •ì œ ì¤‘...\")\n",
    "        for i, raw_doc in enumerate(raw_tm_docs):\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"     ì§„í–‰ë¥ : {i}/{len(raw_tm_docs)} ({i/len(raw_tm_docs)*100:.1f}%)\")\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ ì •ì œ ì‹œë„\n",
    "            clean_result = self._clean_text_pair(raw_doc)\n",
    "            \n",
    "            if clean_result[\"is_valid\"]:\n",
    "                # ë©”íƒ€ë°ì´í„° ë³´ì¡´í•˜ë©´ì„œ ì •ì œëœ ë¬¸ì„œ ìƒì„±\n",
    "                clean_doc = self._preserve_metadata(raw_doc, clean_result)\n",
    "                cleaned_docs.append(clean_doc)\n",
    "                \n",
    "                stats[\"output_count\"] += 1\n",
    "                stats[\"quality_distribution\"][clean_result[\"quality_tier\"]] += 1\n",
    "                stats[\"text_types\"][clean_result[\"text_type\"]] += 1\n",
    "            else:\n",
    "                stats[\"removed_count\"] += 1\n",
    "                for reason in clean_result[\"removal_reasons\"]:\n",
    "                    stats[\"removal_reasons\"][reason] += 1\n",
    "        \n",
    "        print(f\"   - ì •ì œ ì™„ë£Œ: {stats['output_count']}/{stats['input_count']} ë³´ì¡´\")\n",
    "        \n",
    "        # ì •ì œëœ TM ì €ì¥\n",
    "        if cleaned_docs:\n",
    "            print(f\"   - MongoDBì— ì •ì œëœ TM ì €ì¥ ì¤‘...\")\n",
    "            clean_tm_collection.insert_many(cleaned_docs)\n",
    "            print(f\"   - âœ… {len(cleaned_docs)}ê°œ ì •ì œ TM ì €ì¥ ì™„ë£Œ\")\n",
    "        \n",
    "        # í†µê³„ ì €ì¥\n",
    "        stats_doc = {\n",
    "            \"language_pair\": lang_suffix,\n",
    "            \"cleaning_date\": datetime.utcnow(),\n",
    "            \"input_count\": stats[\"input_count\"],\n",
    "            \"output_count\": stats[\"output_count\"], \n",
    "            \"removed_count\": stats[\"removed_count\"],\n",
    "            \"removal_reasons\": dict(stats[\"removal_reasons\"]),\n",
    "            \"quality_distribution\": dict(stats[\"quality_distribution\"]),\n",
    "            \"text_types\": dict(stats[\"text_types\"]),\n",
    "            \"cleaning_efficiency\": stats[\"output_count\"] / stats[\"input_count\"] if stats[\"input_count\"] > 0 else 0\n",
    "        }\n",
    "        \n",
    "        stats_collection.delete_many({})  # ê¸°ì¡´ í†µê³„ ì‚­ì œ\n",
    "        stats_collection.insert_one(stats_doc)\n",
    "        print(f\"   - âœ… ì •ì œ í†µê³„ ì €ì¥ ì™„ë£Œ\")\n",
    "        \n",
    "        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥\n",
    "        self._print_cleaning_summary(stats_doc)\n",
    "        \n",
    "        return stats_doc\n",
    "    \n",
    "    def _clean_text_pair(self, raw_doc: Dict) -> Dict:\n",
    "        \"\"\"ë‹¨ì¼ TM ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ ìŒì„ ì •ì œ\"\"\"\n",
    "        source_text = raw_doc.get(\"source_text\", \"\")\n",
    "        target_text = raw_doc.get(\"target_text\", \"\")\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ì •ì œ\n",
    "        clean_source = self._clean_single_text(source_text)\n",
    "        clean_target = self._clean_single_text(target_text)\n",
    "        \n",
    "        # ìœ íš¨ì„± ê²€ì‚¬\n",
    "        validation_result = self._validate_cleaned_text_pair(\n",
    "            clean_source, clean_target, source_text, target_text\n",
    "        )\n",
    "        \n",
    "        if not validation_result[\"is_valid\"]:\n",
    "            return validation_result\n",
    "        \n",
    "        # í’ˆì§ˆ í‰ê°€\n",
    "        quality_score = self._calculate_quality_score(clean_source, clean_target)\n",
    "        quality_tier = self._get_quality_tier(quality_score)\n",
    "        text_type = self._classify_text_type(clean_source)\n",
    "        \n",
    "        return {\n",
    "            \"is_valid\": True,\n",
    "            \"clean_source\": clean_source,\n",
    "            \"clean_target\": clean_target,\n",
    "            \"quality_score\": quality_score,\n",
    "            \"quality_tier\": quality_tier,\n",
    "            \"text_type\": text_type,\n",
    "            \"metadata\": {\n",
    "                \"quality_score\": quality_score,\n",
    "                \"text_type\": text_type,\n",
    "                \"word_count_source\": len(clean_source.split()),\n",
    "                \"word_count_target\": len(clean_target.split()),\n",
    "                \"cleaning_method\": \"html_and_noise_removal\",\n",
    "                \"cleaned_at\": datetime.utcnow()\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _clean_single_text(self, text: str) -> str:\n",
    "        \"\"\"ë‹¨ì¼ í…ìŠ¤íŠ¸ ì •ì œ\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        cleaned = text\n",
    "        \n",
    "        # HTML íƒœê·¸ ì œê±°\n",
    "        cleaned = re.sub(r'<[^>]+>', '', cleaned)\n",
    "        \n",
    "        # ë…¸ì´ì¦ˆ íŒ¨í„´ ì œê±°\n",
    "        for pattern in self.noise_patterns[1:]:  # HTML íƒœê·¸ëŠ” ì´ë¯¸ ì œê±°í–ˆìœ¼ë¯€ë¡œ ì œì™¸\n",
    "            cleaned = re.sub(pattern, '', cleaned)\n",
    "        \n",
    "        # ì—°ì†ëœ ê³µë°± ì •ë¦¬\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "        \n",
    "        # ì•ë’¤ ê³µë°± ì œê±°\n",
    "        cleaned = cleaned.strip()\n",
    "        \n",
    "        # HTML ì—”í‹°í‹° ë””ì½”ë”© (ê¸°ë³¸ì ì¸ ê²ƒë“¤)\n",
    "        html_entities = {\n",
    "            '&amp;': '&', '&lt;': '<', '&gt;': '>', \n",
    "            '&quot;': '\"', '&#39;': \"'\", '&nbsp;': ' '\n",
    "        }\n",
    "        for entity, char in html_entities.items():\n",
    "            cleaned = cleaned.replace(entity, char)\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def _validate_cleaned_text_pair(self, clean_source: str, clean_target: str, \n",
    "                                   original_source: str, original_target: str) -> Dict:\n",
    "        \"\"\"ì •ì œëœ í…ìŠ¤íŠ¸ ìŒì˜ ìœ íš¨ì„± ê²€ì‚¬\"\"\"\n",
    "        removal_reasons = []\n",
    "        \n",
    "        # ë¹ˆ í…ìŠ¤íŠ¸ ì²´í¬\n",
    "        if not clean_source.strip():\n",
    "            removal_reasons.append(\"empty_source_text\")\n",
    "        \n",
    "        if not clean_target.strip():\n",
    "            removal_reasons.append(\"empty_target_text\")\n",
    "        \n",
    "        # ìµœì†Œ ê¸¸ì´ ì²´í¬\n",
    "        if len(clean_source.strip()) < 2:\n",
    "            removal_reasons.append(\"source_too_short\")\n",
    "        \n",
    "        if len(clean_target.strip()) < 2:\n",
    "            removal_reasons.append(\"target_too_short\")\n",
    "        \n",
    "        # ì˜ë¯¸ìˆëŠ” ë¬¸ì ì²´í¬ (ì•ŒíŒŒë²³, í•œê¸€, ì¼ë³¸ì–´, ì¤‘êµ­ì–´)\n",
    "        meaningful_pattern = r'[a-zA-Zê°€-í£ã-ã‚Ÿã‚¡-ãƒ¾ä¸€-é¾¯]'\n",
    "        if not re.search(meaningful_pattern, clean_source):\n",
    "            removal_reasons.append(\"source_no_meaningful_chars\")\n",
    "        \n",
    "        if not re.search(meaningful_pattern, clean_target):\n",
    "            removal_reasons.append(\"target_no_meaningful_chars\")\n",
    "        \n",
    "        # ìˆ«ìë‚˜ íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ê²½ìš°\n",
    "        if re.match(r'^[\\d\\s\\-_.,;:!?()[\\]{}]+$', clean_source.strip()):\n",
    "            removal_reasons.append(\"source_only_numbers_symbols\")\n",
    "        \n",
    "        if re.match(r'^[\\d\\s\\-_.,;:!?()[\\]{}]+$', clean_target.strip()):\n",
    "            removal_reasons.append(\"target_only_numbers_symbols\")\n",
    "        \n",
    "        # JCR/Sling ì†ì„± ì²´í¬\n",
    "        if any(keyword in original_source.lower() for keyword in ['jcr:', 'sling:', 'cq:', 'dam:']):\n",
    "            removal_reasons.append(\"jcr_sling_properties\")\n",
    "        \n",
    "        return {\n",
    "            \"is_valid\": len(removal_reasons) == 0,\n",
    "            \"removal_reasons\": removal_reasons\n",
    "        }\n",
    "    \n",
    "    def _calculate_quality_score(self, source: str, target: str) -> float:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)\"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # ê¸¸ì´ ì ìˆ˜ (ì ì ˆí•œ ê¸¸ì´ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)\n",
    "        source_len = len(source.split())\n",
    "        target_len = len(target.split())\n",
    "        avg_len = (source_len + target_len) / 2\n",
    "        \n",
    "        if 2 <= avg_len <= 50:\n",
    "            score += 0.3\n",
    "        elif avg_len > 50:\n",
    "            score += 0.1\n",
    "        \n",
    "        # ë¬¸ì¥ ì™„ì„±ë„ (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë“±)\n",
    "        if any(source.strip().endswith(punct) for punct in '.!?ã€‚ï¼ï¼Ÿ'):\n",
    "            score += 0.2\n",
    "        \n",
    "        # ëŒ€ì†Œë¬¸ì ì ì ˆì„± (ì²« ê¸€ì ëŒ€ë¬¸ì ë“±)\n",
    "        if source and source[0].isupper():\n",
    "            score += 0.1\n",
    "        \n",
    "        # íŠ¹ìˆ˜ë¬¸ì ë¹„ìœ¨ (ë„ˆë¬´ ë§ìœ¼ë©´ ê°ì )\n",
    "        special_char_ratio = len(re.findall(r'[^\\w\\sê°€-í£ã-ã‚Ÿã‚¡-ãƒ¾ä¸€-é¾¯]', source)) / len(source) if source else 0\n",
    "        if special_char_ratio < 0.3:\n",
    "            score += 0.2\n",
    "        \n",
    "        # ë²ˆì—­ ìŒ ê¸¸ì´ ë¹„ìœ¨ (ë„ˆë¬´ ì°¨ì´ë‚˜ë©´ ê°ì )\n",
    "        if source_len > 0 and target_len > 0:\n",
    "            ratio = min(source_len, target_len) / max(source_len, target_len)\n",
    "            if ratio > 0.3:\n",
    "                score += 0.2\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "    \n",
    "    def _get_quality_tier(self, score: float) -> str:\n",
    "        \"\"\"í’ˆì§ˆ ì ìˆ˜ë¥¼ í‹°ì–´ë¡œ ë³€í™˜\"\"\"\n",
    "        if score >= 0.8:\n",
    "            return \"high\"\n",
    "        elif score >= 0.5:\n",
    "            return \"medium\"\n",
    "        elif score >= 0.2:\n",
    "            return \"low\"\n",
    "        else:\n",
    "            return \"very_low\"\n",
    "    \n",
    "    def _classify_text_type(self, text: str) -> str:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ìœ í˜• ë¶„ë¥˜\"\"\"\n",
    "        if not text:\n",
    "            return \"empty\"\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # ì œëª© í˜•íƒœ (ì§§ê³  ì²« ê¸€ì ëŒ€ë¬¸ì)\n",
    "        if len(text.split()) <= 5 and text[0].isupper() and not text.endswith('.'):\n",
    "            return \"title\"\n",
    "        \n",
    "        # ë¬¸ì¥ í˜•íƒœ (ë§ˆì¹¨í‘œë¡œ ëë‚¨)\n",
    "        if text.endswith('.') or text.endswith('ã€‚'):\n",
    "            return \"sentence\"\n",
    "        \n",
    "        # ë§í¬ í…ìŠ¤íŠ¸ (íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨)\n",
    "        if any(keyword in text_lower for keyword in ['learn more', 'read more', 'click here', 'ìì„¸íˆ', 'ë”ë³´ê¸°']):\n",
    "            return \"link_text\"\n",
    "        \n",
    "        # ë¼ë²¨ í˜•íƒœ (ì½œë¡ ìœ¼ë¡œ ëë‚¨)\n",
    "        if text.endswith(':') or text.endswith('ï¼š'):\n",
    "            return \"label\"\n",
    "        \n",
    "        # ì¼ë°˜ í…ìŠ¤íŠ¸\n",
    "        return \"plain_text\"\n",
    "    \n",
    "    def _preserve_metadata(self, raw_doc: Dict, clean_result: Dict) -> Dict:\n",
    "        \"\"\"ì›ë³¸ ë©”íƒ€ë°ì´í„°ë¥¼ 100% ë³´ì¡´í•˜ë©´ì„œ ì •ì œëœ ë°ì´í„° ì¶”ê°€\"\"\"\n",
    "        # ì›ë³¸ ë¬¸ì„œ ì „ì²´ ë³µì‚¬\n",
    "        clean_doc = raw_doc.copy()\n",
    "        \n",
    "        # ì›ë³¸ í…ìŠ¤íŠ¸ ë°±ì—…\n",
    "        clean_doc[\"original_source_text\"] = raw_doc.get(\"source_text\", \"\")\n",
    "        clean_doc[\"original_target_text\"] = raw_doc.get(\"target_text\", \"\")\n",
    "        \n",
    "        # ì •ì œëœ í…ìŠ¤íŠ¸ë¡œ êµì²´\n",
    "        clean_doc[\"source_text\"] = clean_result[\"clean_source\"]\n",
    "        clean_doc[\"target_text\"] = clean_result[\"clean_target\"]\n",
    "        \n",
    "        # ì •ì œ ë©”íƒ€ë°ì´í„° ì¶”ê°€\n",
    "        clean_doc.update(clean_result[\"metadata\"])\n",
    "        \n",
    "        return clean_doc\n",
    "    \n",
    "    def _print_cleaning_summary(self, stats: Dict):\n",
    "        \"\"\"ì •ì œ ê²°ê³¼ ìš”ì•½ ì¶œë ¥\"\"\"\n",
    "        print(f\"\\nğŸ“Š [{stats['language_pair']}] TM ì •ì œ ê²°ê³¼ ìš”ì•½:\")\n",
    "        print(f\"   ğŸ”¢ ì…ë ¥: {stats['input_count']:,}ê°œ\")\n",
    "        print(f\"   âœ… ë³´ì¡´: {stats['output_count']:,}ê°œ ({stats['cleaning_efficiency']:.1%})\")\n",
    "        print(f\"   ğŸ—‘ï¸ ì œê±°: {stats['removed_count']:,}ê°œ\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ ì œê±° ì´ìœ :\")\n",
    "        for reason, count in sorted(stats['removal_reasons'].items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"   - {reason}: {count:,}ê°œ\")\n",
    "        \n",
    "        print(f\"\\nğŸ† í’ˆì§ˆ ë¶„í¬:\")\n",
    "        for tier, count in sorted(stats['quality_distribution'].items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"   - {tier}: {count:,}ê°œ\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ í…ìŠ¤íŠ¸ ìœ í˜•:\")\n",
    "        for text_type, count in sorted(stats['text_types'].items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"   - {text_type}: {count:,}ê°œ\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 3: í¸ì˜ í•¨ìˆ˜ë“¤\n",
    "# ===================================================================\n",
    "\n",
    "def clean_all_language_pairs():\n",
    "    \"\"\"ëª¨ë“  ì–¸ì–´ ìŒì˜ TM ì •ì œ\"\"\"\n",
    "    cleaner = TMCleaner()\n",
    "    results = {}\n",
    "    \n",
    "    for source_lang, target_lang in SUPPORTED_LANGUAGE_PAIRS:\n",
    "        lang_suffix = f\"{source_lang}_{target_lang}\"\n",
    "        print(f\"\\nğŸŒ {source_lang.upper()}-{target_lang.upper()} TM ì •ì œ ì‹œì‘...\")\n",
    "        \n",
    "        try:\n",
    "            result = cleaner.clean_translation_memory(lang_suffix)\n",
    "            results[lang_suffix] = result\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {lang_suffix} ì •ì œ ì‹¤íŒ¨: {str(e)}\")\n",
    "            results[lang_suffix] = {\"status\": \"error\", \"error\": str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_cleaning_stats(lang_suffix: str) -> Optional[Dict]:\n",
    "    \"\"\"íŠ¹ì • ì–¸ì–´ ìŒì˜ ì •ì œ í†µê³„ ì¡°íšŒ\"\"\"\n",
    "    client = MongoClient(MONGO_CONNECTION_STRING)\n",
    "    db = client[DB_NAME]\n",
    "    stats_collection = db[f\"tm_cleaning_stats_{lang_suffix}\"]\n",
    "    \n",
    "    return stats_collection.find_one({}, sort=[(\"cleaning_date\", -1)])\n",
    "\n",
    "def preview_raw_tm_samples(lang_suffix: str, limit: int = 5):\n",
    "    \"\"\"Raw TM ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°\"\"\"\n",
    "    client = MongoClient(MONGO_CONNECTION_STRING)\n",
    "    db = client[DB_NAME]\n",
    "    raw_tm_collection = db[f\"translation_memory_{lang_suffix}\"]\n",
    "    \n",
    "    print(f\"ğŸ” [{lang_suffix}] Raw TM ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    samples = list(raw_tm_collection.find({}).limit(limit))\n",
    "    \n",
    "    for i, sample in enumerate(samples, 1):\n",
    "        print(f\"\\n--- ìƒ˜í”Œ {i} ---\")\n",
    "        print(f\"Source: '{sample.get('source_text', '')[:100]}...'\")\n",
    "        print(f\"Target: '{sample.get('target_text', '')[:100]}...'\")\n",
    "        print(f\"Page: {sample.get('page_path', 'N/A')}\")\n",
    "        print(f\"Component: {sample.get('component_type', 'N/A')}\")\n",
    "\n",
    "def compare_before_after(lang_suffix: str, limit: int = 3):\n",
    "    \"\"\"ì •ì œ ì „í›„ ë¹„êµ\"\"\"\n",
    "    client = MongoClient(MONGO_CONNECTION_STRING)\n",
    "    db = client[DB_NAME]\n",
    "    clean_tm_collection = db[f\"clean_translation_memory_{lang_suffix}\"]\n",
    "    \n",
    "    print(f\"ğŸ” [{lang_suffix}] ì •ì œ ì „í›„ ë¹„êµ:\")\n",
    "    samples = list(clean_tm_collection.find({}).limit(limit))\n",
    "    \n",
    "    for i, sample in enumerate(samples, 1):\n",
    "        print(f\"\\n--- ë¹„êµ {i} ---\")\n",
    "        print(f\"âœ¨ ì •ì œ í›„ Source: '{sample.get('source_text', '')}'\")\n",
    "        print(f\"âœ¨ ì •ì œ í›„ Target: '{sample.get('target_text', '')}'\")\n",
    "        print(f\"ğŸ—‘ï¸ ì›ë³¸ Source: '{sample.get('original_source_text', '')[:100]}...'\")\n",
    "        print(f\"ğŸ—‘ï¸ ì›ë³¸ Target: '{sample.get('original_target_text', '')[:100]}...'\")\n",
    "        print(f\"ğŸ† í’ˆì§ˆ ì ìˆ˜: {sample.get('quality_score', 'N/A')}\")\n",
    "        print(f\"ğŸ“ í…ìŠ¤íŠ¸ ìœ í˜•: {sample.get('text_type', 'N/A')}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 4: ì‹¤í–‰ ì˜ˆì‹œ - ë‹¨ì¼ ì–¸ì–´ ìŒ ì •ì œ\n",
    "# ===================================================================\n",
    "\n",
    "# í•œêµ­ì–´ TM ì •ì œ ì‹¤í–‰\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë²ˆì—­ ë©”ëª¨ë¦¬ ì •ì œ ì‹œì‘\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Raw TM ë¯¸ë¦¬ë³´ê¸°\n",
    "preview_raw_tm_samples(\"en_ko\", limit=3)\n",
    "\n",
    "# TM ì •ì œ ì‹¤í–‰\n",
    "cleaner = TMCleaner()\n",
    "ko_result = cleaner.clean_translation_memory(\"en_ko\")\n",
    "\n",
    "# ì •ì œ í›„ ë¹„êµ\n",
    "compare_before_after(\"en_ko\", limit=3)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 5: ì‹¤í–‰ ì˜ˆì‹œ - ëª¨ë“  ì–¸ì–´ ìŒ ì •ì œ\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸŒ ëª¨ë“  ì–¸ì–´ ìŒ ë²ˆì—­ ë©”ëª¨ë¦¬ ì •ì œ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ëª¨ë“  ì–¸ì–´ ìŒ ì •ì œ\n",
    "all_results = clean_all_language_pairs()\n",
    "\n",
    "# ì „ì²´ ê²°ê³¼ ìš”ì•½\n",
    "print(\"\\nğŸ“Š ì „ì²´ ì •ì œ ê²°ê³¼ ìš”ì•½:\")\n",
    "for lang_suffix, result in all_results.items():\n",
    "    if result.get(\"status\") == \"error\":\n",
    "        print(f\"âŒ {lang_suffix}: {result.get('error', 'Unknown error')}\")\n",
    "    elif result.get(\"status\") == \"no_data\":\n",
    "        print(f\"âš ï¸ {lang_suffix}: ë°ì´í„° ì—†ìŒ\")\n",
    "    else:\n",
    "        efficiency = result.get('cleaning_efficiency', 0)\n",
    "        print(f\"âœ… {lang_suffix}: {result.get('output_count', 0):,}ê°œ ì •ì œ ({efficiency:.1%} íš¨ìœ¨)\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 6: ì •ì œ í†µê³„ ì¡°íšŒ ë° ë¶„ì„\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“ˆ ì •ì œ í†µê³„ ë¶„ì„\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ê° ì–¸ì–´ ìŒì˜ í†µê³„ ì¡°íšŒ\n",
    "for source_lang, target_lang in SUPPORTED_LANGUAGE_PAIRS:\n",
    "    lang_suffix = f\"{source_lang}_{target_lang}\"\n",
    "    stats = get_cleaning_stats(lang_suffix)\n",
    "    \n",
    "    if stats:\n",
    "        print(f\"\\nğŸ“Š {source_lang.upper()}-{target_lang.upper()} í†µê³„:\")\n",
    "        print(f\"   ì •ì œ ë‚ ì§œ: {stats['cleaning_date']}\")\n",
    "        print(f\"   íš¨ìœ¨ì„±: {stats['cleaning_efficiency']:.1%}\")\n",
    "        print(f\"   ì…ë ¥: {stats['input_count']:,}ê°œ\")\n",
    "        print(f\"   ì¶œë ¥: {stats['output_count']:,}ê°œ\")\n",
    "        print(f\"   ì œê±°: {stats['removed_count']:,}ê°œ\")\n",
    "        \n",
    "        # ì£¼ìš” ì œê±° ì´ìœ  Top 3\n",
    "        top_reasons = sorted(stats['removal_reasons'].items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        print(f\"   ì£¼ìš” ì œê±° ì´ìœ :\")\n",
    "        for reason, count in top_reasons:\n",
    "            print(f\"     - {reason}: {count:,}ê°œ\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ {source_lang.upper()}-{target_lang.upper()}: í†µê³„ ì—†ìŒ\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# Cell 7: MongoDB ì»¬ë ‰ì…˜ í™•ì¸\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ—„ï¸ MongoDB ì»¬ë ‰ì…˜ í˜„í™©\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# MongoDB ì—°ê²° ë° ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ\n",
    "client = MongoClient(MONGO_CONNECTION_STRING)\n",
    "db = client[DB_NAME]\n",
    "collections = db.list_collection_names()\n",
    "\n",
    "# TM ê´€ë ¨ ì»¬ë ‰ì…˜ ë¶„ë¥˜\n",
    "raw_tm_collections = [c for c in collections if c.startswith(\"translation_memory_\")]\n",
    "clean_tm_collections = [c for c in collections if c.startswith(\"clean_translation_memory_\")]\n",
    "stats_collections = [c for c in collections if c.startswith(\"tm_cleaning_stats_\")]\n",
    "\n",
    "print(f\"ğŸ“‹ ì „ì²´ ì»¬ë ‰ì…˜: {len(collections)}ê°œ\")\n",
    "print(f\"ğŸ“„ Raw TM ì»¬ë ‰ì…˜: {len(raw_tm_collections)}ê°œ\")\n",
    "print(f\"âœ¨ Clean TM ì»¬ë ‰ì…˜: {len(clean_tm_collections)}ê°œ\")\n",
    "print(f\"ğŸ“Š í†µê³„ ì»¬ë ‰ì…˜: {len(stats_collections)}ê°œ\")\n",
    "\n",
    "# ê° ì»¬ë ‰ì…˜ì˜ ë¬¸ì„œ ìˆ˜ í™•ì¸\n",
    "print(f\"\\nğŸ“Š ì»¬ë ‰ì…˜ë³„ ë¬¸ì„œ ìˆ˜:\")\n",
    "for collection_name in sorted(raw_tm_collections + clean_tm_collections + stats_collections):\n",
    "    count = db[collection_name].count_documents({})\n",
    "    collection_type = \"Raw TM\" if collection_name.startswith(\"translation_memory_\") else \\\n",
    "                     \"Clean TM\" if collection_name.startswith(\"clean_translation_memory_\") else \"í†µê³„\"\n",
    "    print(f\"   {collection_type:>8} | {collection_name:<35} | {count:>8,}ê°œ\")\n",
    "\n",
    "print(\"\\nâœ… TM ì •ì œ ì‘ì—… ì™„ë£Œ!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
