{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc5e947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– LLM ê¸°ë°˜ PDF ë²ˆì—­ìŒ ì¶”ì¶œ ì‹œì‘\n",
      "ğŸŒ ì²˜ë¦¬í•  ì–¸ì–´ ìŒ: [('en', 'ko'), ('en', 'ja')]\n",
      "ğŸ“ PDF í´ë”: /mnt/d/Cloud-Synced/Illumina/OneDrive - Illumina, Inc/aem_qa_system/data/2_downloaded/pdfs\n",
      "ğŸ“ ê²°ê³¼ ì €ì¥ í´ë”: /mnt/d/Cloud-Synced/Illumina/OneDrive - Illumina, Inc/aem_qa_system/data/3_processed\n",
      "   - EN: 101ê°œ PDF\n",
      "   - KO: âŒ í´ë” ì—†ìŒ (/mnt/d/Cloud-Synced/Illumina/OneDrive - Illumina, Inc/aem_qa_system/data/2_downloaded/pdfs/ko)\n",
      "   - EN: 101ê°œ PDF\n",
      "   - JA: 95ê°œ PDF\n"
     ]
    }
   ],
   "source": [
    "# notebooks/3c_extract_pdf_pairs_with_llm.ipynb\n",
    "\n",
    "# ========================================\n",
    "# Cell 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ import ë° ì„¤ì •\n",
    "# ========================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "from processors.pdf_text_extractor import PDFTextExtractor\n",
    "from processors.pdf_pair_matcher import PDFPairMatcher\n",
    "from processors.llm_pdf_tm_generator import LLMPDFTMGenerator  # ğŸ†• ìƒˆë¡œìš´ LLM ìƒì„±ê¸°\n",
    "from config import PROCESSED_DIR, PDF_DOWNLOAD_DIR\n",
    "\n",
    "# ì²˜ë¦¬í•  ì–¸ì–´ ìŒë“¤ ì •ì˜\n",
    "LANGUAGE_PAIRS = [\n",
    "    ('en', 'ko'),  # ì˜ì–´-í•œêµ­ì–´\n",
    "    ('en', 'ja'),  # ì˜ì–´-ì¼ë³¸ì–´\n",
    "]\n",
    "\n",
    "# í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ¤– LLM ê¸°ë°˜ PDF ë²ˆì—­ìŒ ì¶”ì¶œ ì‹œì‘\")\n",
    "print(f\"ğŸŒ ì²˜ë¦¬í•  ì–¸ì–´ ìŒ: {LANGUAGE_PAIRS}\")\n",
    "print(f\"ğŸ“ PDF í´ë”: {PDF_DOWNLOAD_DIR}\")\n",
    "print(f\"ğŸ“ ê²°ê³¼ ì €ì¥ í´ë”: {PROCESSED_DIR}\")\n",
    "\n",
    "# PDF í´ë” ì¡´ì¬ í™•ì¸\n",
    "for lang_pair in LANGUAGE_PAIRS:\n",
    "    for lang in lang_pair:\n",
    "        lang_dir = os.path.join(PDF_DOWNLOAD_DIR, lang)\n",
    "        if os.path.exists(lang_dir):\n",
    "            pdf_count = len([f for f in os.listdir(lang_dir) if f.endswith('.pdf')])\n",
    "            print(f\"   - {lang.upper()}: {pdf_count}ê°œ PDF\")\n",
    "        else:\n",
    "            print(f\"   - {lang.upper()}: âŒ í´ë” ì—†ìŒ ({lang_dir})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c667095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ”„ EN-KO ì–¸ì–´ ìŒ ì²˜ë¦¬ ì‹œì‘ (LLM ê¸°ë°˜)\n",
      "================================================================================\n",
      "âŒ ë§¤ì¹­ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: /mnt/d/Cloud-Synced/Illumina/OneDrive - Illumina, Inc/aem_qa_system/data/1_input/pdf_list_master_en_ko.csv\n",
      "âŒ EN-KO PDF í˜ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "\n",
      "================================================================================\n",
      "ğŸ”„ EN-JA ì–¸ì–´ ìŒ ì²˜ë¦¬ ì‹œì‘ (LLM ê¸°ë°˜)\n",
      "================================================================================\n",
      "ğŸ“‹ EN-JA ë§¤ì¹­ í…Œì´ë¸”ì—ì„œ 101ê°œ í•­ëª©ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "âœ… ì‹¤ì œ ì¡´ì¬í•˜ëŠ” 100ê°œì˜ EN-JA PDF í˜ì–´ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     25\u001b[39m pairs_for_this_language = []\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# ğŸ”§ ì¼ë¶€ë§Œ ì²˜ë¦¬í•˜ì—¬ í…ŒìŠ¤íŠ¸ (ì „ì²´ ì²˜ë¦¬ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# test_pairs = pdf_pairs[:]  # ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (source_path, target_path) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtest_pairs\u001b[49m):\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# print(f\"\\n[{i+1}/{len(test_pairs)}] ğŸ“„ ì²˜ë¦¬ ì¤‘: {os.path.basename(target_path)}\")\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     34\u001b[39m         \u001b[38;5;66;03m# ğŸ†• ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìˆ˜ì •ëœ ë°©ì‹)\u001b[39;00m\n\u001b[32m     35\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfitz\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'test_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Cell 2: LLM ê¸°ë°˜ ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)\n",
    "# ========================================\n",
    "\n",
    "# ğŸ†• LLM ê¸°ë°˜ ìƒì„±ê¸° ì´ˆê¸°í™”\n",
    "llm_generator = LLMPDFTMGenerator(fast_mode=True)\n",
    "basic_extractor = PDFTextExtractor(min_text_length=25)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for source_lang, target_lang in LANGUAGE_PAIRS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ”„ {source_lang.upper()}-{target_lang.upper()} ì–¸ì–´ ìŒ ì²˜ë¦¬ ì‹œì‘ (LLM ê¸°ë°˜)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 1. PDF í˜ì–´ ì°¾ê¸°\n",
    "    matcher = PDFPairMatcher(source_lang, target_lang)\n",
    "    pdf_pairs = matcher.find_pdf_pairs()\n",
    "    \n",
    "    if not pdf_pairs:\n",
    "        print(f\"âŒ {source_lang.upper()}-{target_lang.upper()} PDF í˜ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        continue\n",
    "    \n",
    "    # 2. LLM ê¸°ë°˜ ë²ˆì—­ ìŒ ìƒì„±\n",
    "    pairs_for_this_language = []\n",
    "    \n",
    "    # ğŸ”§ ì¼ë¶€ë§Œ ì²˜ë¦¬í•˜ì—¬ í…ŒìŠ¤íŠ¸ (ì „ì²´ ì²˜ë¦¬ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)\n",
    "    test_pairs = pdf_pairs[:]  # ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸\n",
    "    \n",
    "    for i, (source_path, target_path) in enumerate(test_pairs):\n",
    "        print(f\"\\n[{i+1}/{len(test_pairs)}] ğŸ“„ ì²˜ë¦¬ ì¤‘: {os.path.basename(target_path)}\")\n",
    "        \n",
    "        try:\n",
    "            # ğŸ†• ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìˆ˜ì •ëœ ë°©ì‹)\n",
    "            import fitz\n",
    "            \n",
    "            # Source PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "            source_raw_text = \"\"\n",
    "            with fitz.open(source_path) as doc:\n",
    "                for page in doc:\n",
    "                    source_raw_text += page.get_text() + \"\\n\\n\"\n",
    "            \n",
    "            # Target PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ  \n",
    "            target_raw_text = \"\"\n",
    "            with fitz.open(target_path) as doc:\n",
    "                for page in doc:\n",
    "                    target_raw_text += page.get_text() + \"\\n\\n\"\n",
    "            \n",
    "            print(f\"   - ì›ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(source_raw_text)} / {len(target_raw_text)} ë¬¸ì)\")\n",
    "            \n",
    "            # ğŸ†• LLMìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±\n",
    "            print(f\"   - ğŸ¤– {source_lang.upper()} LLM ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± ì¤‘...\")\n",
    "            source_segments = llm_generator.generate_meaningful_segments(source_raw_text, source_lang, target_lang)\n",
    "            \n",
    "            print(f\"   - ğŸ¤– {target_lang.upper()} LLM ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± ì¤‘...\")\n",
    "            target_segments = llm_generator.generate_meaningful_segments(target_raw_text, target_lang, source_lang)\n",
    "            \n",
    "            print(f\"   - LLM ì„¸ê·¸ë¨¼íŠ¸: {source_lang.upper()} {len(source_segments)}ê°œ, {target_lang.upper()} {len(target_segments)}ê°œ\")\n",
    "\n",
    "            # í•¨ìˆ˜ í˜¸ì¶œ ì „ì— ë””ë²„ê¹… ì¶œë ¥ ì¶”ê°€\n",
    "            print(f\"   - í•¨ìˆ˜ í˜¸ì¶œ ì¸ì: source_segments({len(source_segments)}), target_segments({len(target_segments)})\")\n",
    "            print(f\"   - ê²½ë¡œ: {source_path}, {target_path}\")\n",
    "            print(f\"   - ì–¸ì–´: {source_lang}, {target_lang}\")\n",
    "\n",
    "            \n",
    "            # ğŸ†• ë¬¸ë§¥ ì •ë³´ë¥¼ í¬í•¨í•œ ë²ˆì—­ ìŒ ìƒì„±\n",
    "            pairs = llm_generator.create_translation_pairs_with_context(\n",
    "                source_segments, target_segments, \n",
    "                source_path, target_path,  # ğŸ†• source_path ì¶”ê°€\n",
    "                source_lang, target_lang\n",
    "            )\n",
    "            pairs_for_this_language.extend(pairs)\n",
    "            \n",
    "            print(f\"   - âœ… {len(pairs)}ê°œ ê³ í’ˆì§ˆ ë²ˆì—­ ìŒ ìƒì„±\")\n",
    "            \n",
    "            # ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥\n",
    "            if pairs:\n",
    "                sample = pairs[0]\n",
    "                print(f\"   - ğŸ” ìƒ˜í”Œ (ìœ ì‚¬ë„: {sample['similarity_score']:.3f}):\")\n",
    "                print(f\"     {source_lang.upper()}: {sample['source_text'][:60]}...\")\n",
    "                print(f\"     {target_lang.upper()}: {sample['target_text'][:60]}...\")\n",
    "                print(f\"     ì¹´í…Œê³ ë¦¬: {sample.get('source_category', 'N/A')} â†’ {sample.get('target_category', 'N/A')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   - âŒ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    all_results[f\"{source_lang}_{target_lang}\"] = pairs_for_this_language\n",
    "    print(f\"\\nğŸ‰ {source_lang.upper()}-{target_lang.upper()}: ì´ {len(pairs_for_this_language)}ê°œ ê³ í’ˆì§ˆ ë²ˆì—­ ìŒ ì™„ë£Œ!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Cell 3: ê²°ê³¼ ë¹„êµ ë° ì €ì¥\n",
    "# ========================================\n",
    "\n",
    "total_pairs = 0\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ğŸ“Š LLM ê¸°ë°˜ ë²ˆì—­ìŒ ì¶”ì¶œ ê²°ê³¼\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for lang_pair, pairs in all_results.items():\n",
    "    if pairs:\n",
    "        # ğŸ†• í–¥ìƒëœ ê²°ê³¼ íŒŒì¼ ì €ì¥\n",
    "        csv_path = os.path.join(PROCESSED_DIR, f\"pdf_translation_pairs_{lang_pair}_llm_enhanced.csv\")\n",
    "        df = pd.DataFrame(pairs)\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        total_pairs += len(pairs)\n",
    "        source_lang, target_lang = lang_pair.split('_')\n",
    "        \n",
    "        print(f\"\\nğŸŒ {source_lang.upper()}-{target_lang.upper()}:\")\n",
    "        print(f\"   ğŸ’¾ ì €ì¥: {os.path.basename(csv_path)}\")\n",
    "        print(f\"   ğŸ“„ ë²ˆì—­ ìŒ: {len(pairs)}ê°œ\")\n",
    "        print(f\"   ğŸ“Š í‰ê·  ìœ ì‚¬ë„: {df['similarity_score'].mean():.3f}\")\n",
    "        print(f\"   ğŸ¯ í‰ê·  ì‹ ë¢°ë„: {df['source_confidence'].mean():.3f}\")\n",
    "        \n",
    "        # ğŸ†• ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬\n",
    "        if 'source_category' in df.columns:\n",
    "            category_dist = df['source_category'].value_counts()\n",
    "            print(f\"   ğŸ“‹ ì¹´í…Œê³ ë¦¬ ë¶„í¬: {dict(category_dist)}\")\n",
    "\n",
    "# ğŸ†• í’ˆì§ˆ í–¥ìƒ ë¶„ì„\n",
    "if total_pairs > 0:\n",
    "    print(f\"\\nğŸ‰ LLM ê¸°ë°˜ ê³ í’ˆì§ˆ ë²ˆì—­ìŒ ì¶”ì¶œ ì™„ë£Œ!\")\n",
    "    print(f\"   ğŸ“ˆ ì´ ë²ˆì—­ ìŒ: {total_pairs}ê°œ\")\n",
    "    print(f\"   ğŸš€ ê¸°ì¡´ ëŒ€ë¹„ ì˜ˆìƒ í’ˆì§ˆ í–¥ìƒ: ë†’ì€ ë¬¸ë§¥ ë³´ì¡´, ì˜ë¯¸ ë‹¨ìœ„ ì™„ì„±ë„\")\n",
    "    print(f\"   ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: ì´ ë°ì´í„°ë¥¼ base_tm.csvì™€ í†µí•©í•˜ì—¬ ìµœì¢… TM êµ¬ì¶•\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ ìƒì„±ëœ ë²ˆì—­ ìŒì´ ì—†ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ec689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Cell 4: ê²°ê³¼ ì €ì¥ ë° ë¶„ì„\n",
    "# ========================================\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "total_pairs = 0\n",
    "all_pairs_combined = []\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ğŸ“Š LLM ê¸°ë°˜ ë²ˆì—­ìŒ ì¶”ì¶œ ê²°ê³¼ ì €ì¥\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for lang_pair, pairs in all_results.items():\n",
    "    if pairs:\n",
    "        source_lang, target_lang = lang_pair.split('_')\n",
    "        \n",
    "        # CSV ì €ì¥\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        csv_filename = f\"pdf_tm_{lang_pair}_llm_enhanced_{timestamp}.csv\"\n",
    "        csv_path = os.path.join(PROCESSED_DIR, csv_filename)\n",
    "        \n",
    "        df = pd.DataFrame(pairs)\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        total_pairs += len(pairs)\n",
    "        all_pairs_combined.extend(pairs)\n",
    "        \n",
    "        print(f\"\\nğŸŒ {source_lang.upper()}-{target_lang.upper()}:\")\n",
    "        print(f\"   ğŸ’¾ ì €ì¥: {csv_filename}\")\n",
    "        print(f\"   ğŸ“„ ë²ˆì—­ ìŒ: {len(pairs)}ê°œ\")\n",
    "        print(f\"   ğŸ“Š í‰ê·  ìœ ì‚¬ë„: {df['similarity_score'].mean():.3f}\")\n",
    "\n",
    "# í†µí•© íŒŒì¼ ì €ì¥\n",
    "# if total_pairs > 0:\n",
    "    # combined_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    # combined_filename = f\"pdf_tm_all_languages_llm_enhanced_{combined_timestamp}.csv\"\n",
    "    # combined_csv_path = os.path.join(PROCESSED_DIR, combined_filename)\n",
    "    \n",
    "    # combined_df = pd.DataFrame(all_pairs_combined)\n",
    "    # combined_df.to_csv(combined_csv_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # print(f\"\\nğŸ“¦ í†µí•© íŒŒì¼: {combined_filename}\")\n",
    "    # print(f\"   ğŸ“Š ì „ì²´ ë²ˆì—­ ìŒ: {len(all_pairs_combined)}ê°œ\")\n",
    "    # print(f\"   ğŸŒ ì§€ì› ì–¸ì–´: {combined_df['target_lang'].unique().tolist()}\")\n",
    "\n",
    "print(f\"\\nâœ¨ CSV ì €ì¥ ì™„ë£Œ!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mywork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
