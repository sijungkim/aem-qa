# src/processors/llm_pdf_tm_generator.py

import re
import json
from typing import List, Dict
from pathlib import Path
import ollama
from src.config import LLM_MODEL
import datetime

class LLMPDFTMGenerator:
    """LLMì„ í™œìš©í•œ PDF ë²ˆì—­ ë©”ëª¨ë¦¬ ìƒì„±ê¸° (N:1 ë§¤í•‘ ë°©ì‹)"""

    def __init__(self, model_name: str = LLM_MODEL, fast_mode: bool = False, turbo_mode: bool = False):
        self.model_name = model_name
        self.fast_mode = fast_mode
        self.turbo_mode = turbo_mode

    def generate_meaningful_segments(self, raw_text: str, source_lang: str = 'en', target_lang: str = 'ko') -> List[Dict]:
        """PDF ì›ë¬¸ì„ LLMì„ í†µí•´ ë²ˆì—­ ë©”ëª¨ë¦¬ì— ì í•©í•œ ì˜ë¯¸ìˆëŠ” ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì¬êµ¬ì„± (ìœ„ì¹˜ ì •ë³´ í¬í•¨)"""
        chunks = self._split_into_chunks(raw_text)
        
        all_segments = []
        for i, chunk in enumerate(chunks):
            if not self.turbo_mode:
                print(f"  - ì²­í¬ {i+1}/{len(chunks)} ì²˜ë¦¬ ì¤‘...")
            
            structured_segments = self._restructure_with_llm(chunk, source_lang, target_lang)
            quality_segments = self._filter_quality_segments(structured_segments)
            
            # ìœ„ì¹˜ ì •ë³´ ì¶”ê°€
            positioned_segments = self._add_position_info(quality_segments, chunk, i)
            all_segments.extend(positioned_segments)
        
        # ì „ì²´ í…ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ìœ„ì¹˜ ì •ë³´ ì •ê·œí™”
        return self._normalize_position_info(all_segments, raw_text)

    def _split_into_chunks(self, text: str) -> List[str]:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ LLM ì²˜ë¦¬ ê°€ëŠ¥í•œ í¬ê¸°ë¡œ ë¶„í• """
        if self.turbo_mode:
            max_chunk_size = 15000
        elif self.fast_mode:
            max_chunk_size = 8000
        else:
            max_chunk_size = 3000
        
        if self.turbo_mode:
            chunks = []
            while len(text) > max_chunk_size:
                split_pos = text.rfind('\n', 0, max_chunk_size)
                if split_pos == -1:
                    split_pos = max_chunk_size
                chunks.append(text[:split_pos])
                text = text[split_pos:].lstrip()
            chunks.append(text)
            return chunks

        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            if len(current_chunk + paragraph) > max_chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = paragraph
            else:
                current_chunk += "\n\n" + paragraph if current_chunk else paragraph
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

    def _restructure_with_llm(self, text_chunk: str, source_lang: str, target_lang: str) -> List[Dict]:
        """LLMì„ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­ ë©”ëª¨ë¦¬ìš© ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì¬êµ¬ì„±"""
        if self.turbo_mode:
            prompt = self._get_turbo_prompt(text_chunk, source_lang, target_lang)
        else:
            prompt = self._get_detailed_prompt(text_chunk, source_lang, target_lang)
        
        llm_options = self._get_llm_options()
        
        try:
            response = ollama.chat(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                options=llm_options
            )
            
            response_text = response['message']['content']
            if not self.turbo_mode:
                print(f"  - LLM ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_text[:200]}...")
            
            segments = self._parse_json_response(response_text, text_chunk)
            return segments
            
        except Exception as e:
            if not self.turbo_mode:
                print(f"  - âš ï¸ LLM ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return self._fallback_sentence_split(text_chunk)

    def _get_turbo_prompt(self, text_chunk: str, source_lang: str, target_lang: str) -> str:
        """í„°ë³´ ëª¨ë“œìš© ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸"""
        if source_lang == 'ja':
            return f"""æ—¥æœ¬èªã®PDFãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã§ãã‚‹ã ã‘å¤šãã®å®Œå…¨ãªæ–‡ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚çŸ­ã„æ–‡ã‚‚å«ã‚ã¦ãã ã•ã„ã€‚

å¿…ãšJSONå½¢å¼ã§å›ç­”:
{{"segments": [{{"text": "æ–‡1", "category": "other", "confidence": 0.8}}, {{"text": "æ–‡2", "category": "other", "confidence": 0.8}}]}}

ãƒ†ã‚­ã‚¹ãƒˆ: {text_chunk}"""
        elif source_lang == 'ko':
            return f"""í•œêµ­ì–´ PDF í…ìŠ¤íŠ¸ì—ì„œ ê°€ëŠ¥í•œ ë§ì€ ì™„ì „í•œ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ì„¸ìš”. ì§§ì€ ë¬¸ì¥ë„ í¬í•¨í•˜ì„¸ìš”.

ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{"segments": [{{"text": "ë¬¸ì¥1", "category": "other", "confidence": 0.8}}, {{"text": "ë¬¸ì¥2", "category": "other", "confidence": 0.8}}]}}

í…ìŠ¤íŠ¸: {text_chunk}"""
        else:
            return f"""Extract as many complete sentences as possible from this PDF text. Include shorter sentences too.

Respond ONLY in JSON format:
{{"segments": [{{"text": "sentence1", "category": "other", "confidence": 0.8}}, {{"text": "sentence2", "category": "other", "confidence": 0.8}}]}}

Text: {text_chunk}"""

    def _get_detailed_prompt(self, text_chunk: str, source_lang: str, target_lang: str) -> str:
        """ìƒì„¸ ëª¨ë“œìš© í”„ë¡¬í”„íŠ¸"""
        return f"""ë‹¤ìŒ {source_lang} PDF í…ìŠ¤íŠ¸ë¥¼ {source_lang}-{target_lang} ë²ˆì—­ ë©”ëª¨ë¦¬(TM)ìš©ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”.

ëª©ì : {target_lang} ë²ˆì—­ì„ ìœ„í•œ ê³ í’ˆì§ˆ ì›ë¬¸ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±

ê·œì¹™:
1. ì™„ì „í•˜ê³  ë…ë¦½ì ì¸ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ê¸°
2. ì œí’ˆëª…, ê¸°ìˆ ìš©ì–´ëŠ” ë¬¸ë§¥ê³¼ í•¨ê»˜ ë³´ì¡´  
3. ë²ˆì—­í•˜ê¸° ì í•©í•œ ê¸¸ì´ (10-100ë‹¨ì–´)
4. {target_lang} ë²ˆì—­ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì™„ì „í•œ ë¬¸ì¥

ì…ë ¥ í…ìŠ¤íŠ¸:
```
{text_chunk}
```

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
```json
{{
  "segments": [
    {{"text": "ì™„ì „í•œ ë¬¸ì¥1", "category": "product_description", "confidence": 0.95}},
    {{"text": "ì™„ì „í•œ ë¬¸ì¥2", "category": "technical_spec", "confidence": 0.90}}
  ]
}}
```"""

    def _get_llm_options(self) -> dict:
        """ëª¨ë“œë³„ LLM ì˜µì…˜ ë°˜í™˜"""
        if self.turbo_mode:
            return {
                "temperature": 0.0,
                "top_p": 0.8,
                "num_predict": 1500,
                "repeat_penalty": 1.0,
                "top_k": 10
            }
        elif self.fast_mode:
            return {
                "temperature": 0.0,
                "top_p": 0.9,
                "num_predict": 2000,
                "repeat_penalty": 1.1
            }
        else:
            return {
                "temperature": 0.1,
                "top_p": 0.9,
                "num_predict": 4096,
                "repeat_penalty": 1.1
            }

    def _parse_json_response(self, response_text: str, text_chunk: str) -> List[Dict]:
        """LLM ì‘ë‹µì—ì„œ JSON íŒŒì‹±"""
        json_str = self._extract_json_from_response(response_text)
        
        if not json_str:
            if not self.turbo_mode:
                print(f"  - âš ï¸ JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, í´ë°± ì‚¬ìš©. ì‘ë‹µ: {response_text[:100]}...")
            return self._fallback_sentence_split(text_chunk)
            
        try:
            parsed = json.loads(json_str)
            segments = parsed.get('segments', [])
            if not isinstance(segments, list):
                print(f"  - âš ï¸ JSON 'segments' í‚¤ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜: {type(segments)}")
                return self._fallback_sentence_split(text_chunk)

            if not self.turbo_mode:
                print(f"  - âœ… JSON íŒŒì‹± ì„±ê³µ: {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
            return segments
        except json.JSONDecodeError as e:
            if not self.turbo_mode:
                print(f"  - âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            print(f"  - ë¬¸ì œ JSON: {json_str[:200]}...")
            return self._fallback_sentence_split(text_chunk)

    def _extract_json_from_response(self, response_text: str) -> str:
        """ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ"""
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
        if json_match:
            return json_match.group(1)
        
        json_match = re.search(r'```\s*(\{.*?\})\s*```', response_text, re.DOTALL)
        if json_match:
            return json_match.group(1)
        
        if response_text.strip().startswith('{') and response_text.strip().endswith('}'):
            return response_text.strip()
            
        brace_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if brace_match:
            return brace_match.group(0)

        return None

    def _fallback_sentence_split(self, text: str) -> List[Dict]:
        """LLM ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ë¬¸ì¥ ë¶„í• """
        sentences = re.split(r'(?<=[.!?])\s+', text)
        return [
            {
                "text": sent.strip(),
                "category": "other",
                "confidence": 0.5
            }
            for sent in sentences 
            if 10 < len(sent.strip()) < 800
        ]

    def _filter_quality_segments(self, segments: List[Dict]) -> List[Dict]:
        """í’ˆì§ˆ ê¸°ì¤€ì— ë”°ë¼ ì„¸ê·¸ë¨¼íŠ¸ í•„í„°ë§"""
        quality_segments = []
        
        for segment in segments:
            text = segment.get('text', '')
            if not text:
                continue

            confidence = segment.get('confidence', 0)
            
            if self._is_high_quality_segment(text, confidence):
                quality_segments.append(segment)
        
        return quality_segments

    def _is_high_quality_segment(self, text: str, confidence: float) -> bool:
        """ì„¸ê·¸ë¨¼íŠ¸ í’ˆì§ˆ íŒì •"""
        text_stripped = text.strip()
        
        if self.turbo_mode or self.fast_mode:
            return len(text_stripped) > 8 and len(text_stripped.split()) > 1
        
        if not (10 <= len(text_stripped) <= 800):
            return False
        
        if confidence < 0.6:
            return False
        
        word_count = len(text_stripped.split())
        if word_count < 2:
            return False
        
        if re.match(r'^[\d\s\-\.]+$', text_stripped):
            return False
        
        return True

    def _add_position_info(self, segments: List[Dict], chunk_text: str, chunk_index: int) -> List[Dict]:
        """ì„¸ê·¸ë¨¼íŠ¸ì— ìœ„ì¹˜ ì •ë³´ ì¶”ê°€"""
        paragraphs = [p.strip() for p in chunk_text.split('\n\n') if p.strip()]
        
        for segment in segments:
            text = segment['text']
            
            paragraph_idx, sentence_idx = self._find_text_position(text, paragraphs)
            structure_type = self._detect_structure_type(text)
            
            segment.update({
                'chunk_index': chunk_index,
                'paragraph_index': paragraph_idx,
                'sentence_index': sentence_idx,
                'structure_type': structure_type,
                'original_chunk': chunk_text[:100] + "..." if len(chunk_text) > 100 else chunk_text
            })
        
        return segments

    def _normalize_text_for_search(self, text: str) -> str:
        """ê²€ìƒ‰ì„ ìœ„í•´ í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        return re.sub(r'[\s\W_]+', '', text).lower()

    def _find_text_position(self, target_text: str, paragraphs: List[str]) -> tuple:
        """í…ìŠ¤íŠ¸ì˜ ë¬¸ë‹¨ ë° ë¬¸ì¥ ìœ„ì¹˜ ì°¾ê¸°"""
        target_clean = self._normalize_text_for_search(target_text)
        if not target_clean:
            return -1, -1

        for para_idx, paragraph in enumerate(paragraphs):
            if target_clean in self._normalize_text_for_search(paragraph):
                sentences = re.split(r'(?<=[.!?])\s+', paragraph)
                for sent_idx, sentence in enumerate(sentences):
                    if target_clean in self._normalize_text_for_search(sentence):
                        return para_idx, sent_idx
                return para_idx, 0
        
        return -1, -1

    def _detect_structure_type(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì˜ êµ¬ì¡° íƒ€ì… ê°ì§€"""
        text_stripped = text.strip()
        
        if re.match(r'^\d+[\.\)]\s', text_stripped):
            return "numbered_list"
        if re.match(r'^[â€¢Â·\-\*]\s', text_stripped):
            return "bullet_list"
        if (text_stripped.isupper() or text_stripped.istitle()) and len(text_stripped.split()) < 10 and not text_stripped.endswith(('.', '!', '?')):
            return "heading"
        if '\t' in text_stripped or '|' in text_stripped:
            return "table"
        if ':' in text_stripped and len(text_stripped.split(':')) == 2:
            return "specification"
        
        return "paragraph"

    def _normalize_position_info(self, all_segments: List[Dict], full_text: str) -> List[Dict]:
        """ì „ì²´ í…ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ìœ„ì¹˜ ì •ë³´ ì •ê·œí™”"""
        full_paragraphs = [p.strip() for p in full_text.split('\n\n') if p.strip()]
        
        for segment in all_segments:
            text = segment['text']
            global_para_idx, global_sent_idx = self._find_text_position(text, full_paragraphs)
            
            segment.update({
                'global_paragraph_index': global_para_idx,
                'global_sentence_index': global_sent_idx
            })
        
        return all_segments

    def create_japanese_based_translation_mapping(self, source_segments: List[Dict], 
                                                target_segments: List[Dict], 
                                                source_pdf_path: str,
                                                target_pdf_path: str,
                                                source_lang: str = 'en',
                                                target_lang: str = 'ja') -> List[Dict]:
        """ì¼ë³¸ì–´ ê¸°ì¤€ N:1 ë²ˆì—­ ë§¤í•‘ ìƒì„±"""
        print(f"  - ğŸŒ {target_lang} ê¸°ì¤€ ë²ˆì—­ ë§¤í•‘ ìƒì„± ì¤‘...")
        print(f"  - {target_lang} ì„¸ê·¸ë¨¼íŠ¸: {len(target_segments)}ê°œ")
        print(f"  - {source_lang} ì„¸ê·¸ë¨¼íŠ¸: {len(source_segments)}ê°œ")
        
        translation_mappings = []
        
        for i, ja_segment in enumerate(target_segments):
            if not self.turbo_mode:
                print(f"  - ë§¤í•‘ {i+1}/{len(target_segments)}: {ja_segment['text'][:30]}...")
            
            related_en_segments = self._find_related_source_segments(
                ja_segment, source_segments, source_lang, target_lang
            )
            
            if related_en_segments:
                mapping = self._create_translation_mapping_entry(
                    ja_segment, related_en_segments, 
                    source_pdf_path, target_pdf_path,
                    source_lang, target_lang
                )
                translation_mappings.append(mapping)
        
        print(f"  - âœ… {len(translation_mappings)}ê°œ ë²ˆì—­ ë§¤í•‘ ìƒì„± ì™„ë£Œ")
        return translation_mappings

    def _find_related_source_segments(self, target_segment: Dict, source_segments: List[Dict], 
                                    source_lang: str, target_lang: str) -> List[Dict]:
        """íƒ€ê²Ÿ ì–¸ì–´ ë¬¸ì¥ê³¼ ê´€ë ¨ëœ ì†ŒìŠ¤ ì–¸ì–´ ë¬¸ì¥ë“¤ì„ LLMìœ¼ë¡œ ì°¾ê¸°"""
        prioritized_sources = self._sort_by_proximity(target_segment, source_segments)
        
        max_candidates = 30
        chunks = [prioritized_sources[i:i+max_candidates] for i in range(0, len(prioritized_sources), max_candidates)]
        
        all_related = []
        for chunk in chunks:
            related = self._llm_find_related_segments_with_proximity(
                target_segment, chunk, source_lang, target_lang
            )
            all_related.extend(related)
            
            if len([r for r in all_related if r['final_score'] > 0.7]) >= 5:
                break
        
        all_related.sort(key=lambda x: x['final_score'], reverse=True)
        return all_related[:5]

    def _sort_by_proximity(self, target_segment: Dict, source_segments: List[Dict]) -> List[Dict]:
        """ì¸ì ‘ì„± ê¸°ì¤€ìœ¼ë¡œ ì†ŒìŠ¤ ì„¸ê·¸ë¨¼íŠ¸ ì •ë ¬"""
        target_para = target_segment.get('global_paragraph_index', -1)
        target_sent = target_segment.get('global_sentence_index', -1)
        
        if target_para == -1:
            return source_segments

        def proximity_score(source_seg):
            source_para = source_seg.get('global_paragraph_index', -1)
            source_sent = source_seg.get('global_sentence_index', -1)
            if source_para == -1:
                return 0

            para_distance = abs(target_para - source_para)
            if para_distance == 0:
                return 100 - abs(target_sent - source_sent)
            else:
                return 50 - para_distance
        
        return sorted(source_segments, key=proximity_score, reverse=True)

    def _llm_find_related_segments_with_proximity(self, target_segment: Dict, source_segments: List[Dict], 
                                                  source_lang: str, target_lang: str) -> List[Dict]:
        """LLMì„ ì‚¬ìš©í•´ ê´€ë ¨ ì†ŒìŠ¤ ë¬¸ì¥ ì°¾ê¸° (ì¸ì ‘ì„± ì •ë³´ í¬í•¨)"""
        target_text = target_segment['text']
        target_position = f"ë¬¸ë‹¨ {target_segment.get('global_paragraph_index', '?')}, ë¬¸ì¥ {target_segment.get('global_sentence_index', '?')}"
        
        numbered_sources = []
        for i, seg in enumerate(source_segments):
            position_info = f"(ë¬¸ë‹¨ {seg.get('global_paragraph_index', '?')}, ë¬¸ì¥ {seg.get('global_sentence_index', '?')})"
            numbered_sources.append(f"{i+1}. {seg['text']} {position_info}")
        sources_text = "\n".join(numbered_sources)
        
        prompt = f"""ë‹¤ìŒ {target_lang} ë¬¸ì¥ì˜ ë²ˆì—­ ì›ë¬¸ì´ ë  ìˆ˜ ìˆëŠ” {source_lang} ë¬¸ì¥ë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”.

{target_lang} ë¬¸ì¥: "{target_text}"
ìœ„ì¹˜: {target_position}

{source_lang} ë¬¸ì¥ í›„ë³´ë“¤:
{sources_text}

í‰ê°€ ê¸°ì¤€:
1. ì˜ë¯¸ì  ê´€ë ¨ì„± (0.0-0.8): ë²ˆì—­ ê´€ê³„ì˜ ê°•ë„
   - ì§ì ‘ ë²ˆì—­: 0.7-0.8
   - ë¶€ë¶„ ë²ˆì—­: 0.5-0.7
   - ë¬¸ë§¥ìƒ ê´€ë ¨: 0.3-0.5
2. ì¸ì ‘ì„± ë³´ë„ˆìŠ¤ëŠ” ìë™ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.

ì˜ë¯¸ ì ìˆ˜ê°€ 0.5 ì´ìƒì¸ ë¬¸ì¥ë“¤ë§Œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
{{"matches": [{{"number": 1, "semantic_score": 0.75, "type": "direct"}}, {{"number": 3, "semantic_score": 0.6, "type": "partial"}}]}}"""

        try:
            response = ollama.chat(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                options={"temperature": 0.0, "num_predict": 1024}
            )
            
            response_text = response['message']['content']
            matches = self._parse_relevance_response(response_text)
            
            related_segments = []
            for match in matches:
                idx = match.get('number')
                if idx is None or not (0 < idx <= len(source_segments)):
                    continue

                source_seg = source_segments[idx - 1]
                semantic_score = match.get('semantic_score', 0.0)
                
                proximity_bonus = self._calculate_proximity_bonus(target_segment, source_seg)
                final_score = semantic_score + proximity_bonus
                
                if final_score >= 0.6:
                    related_segments.append({
                        'text': source_seg['text'],
                        'semantic_score': semantic_score,
                        'proximity_bonus': proximity_bonus,
                        'final_score': round(final_score, 3),
                        'relevance': round(final_score, 3),
                        'type': match.get('type', 'related'),
                        'position_info': {
                            'paragraph': source_seg.get('global_paragraph_index'),
                            'sentence': source_seg.get('global_sentence_index'),
                            'structure': source_seg.get('structure_type')
                        }
                    })
            
            return related_segments
            
        except Exception as e:
            if not self.turbo_mode:
                print(f"  - âš ï¸ ê´€ë ¨ ë¬¸ì¥ ì°¾ê¸° ì˜¤ë¥˜: {e}")
            return []

    def _calculate_proximity_bonus(self, target_segment: Dict, source_segment: Dict) -> float:
        """ì¸ì ‘ì„± ê¸°ë°˜ ë³´ë„ˆìŠ¤ ì ìˆ˜ ê³„ì‚°"""
        bonus = 0.0
        target_para = target_segment.get('global_paragraph_index', -1)
        source_para = source_segment.get('global_paragraph_index', -1)
        target_sent = target_segment.get('global_sentence_index', -1)
        source_sent = source_segment.get('global_sentence_index', -1)
        
        if target_para == -1 or source_para == -1:
            return 0.0
        
        if target_para == source_para:
            bonus += 0.2
            if abs(target_sent - source_sent) <= 1:
                bonus += 0.05
        elif abs(target_para - source_para) == 1:
            bonus += 0.1
        
        target_structure = target_segment.get('structure_type', '')
        source_structure = source_segment.get('structure_type', '')
        if target_structure == source_structure and target_structure not in ['paragraph', 'unknown']:
            bonus += 0.1
        
        return round(bonus, 3)

    def _parse_relevance_response(self, response_text: str) -> List[Dict]:
        """LLMì˜ ê´€ë ¨ë„ ì‘ë‹µ íŒŒì‹±"""
        try:
            json_str = self._extract_json_from_response(response_text)
            if json_str:
                parsed = json.loads(json_str)
                matches = parsed.get('matches', [])
                if not isinstance(matches, list): return []
                
                for match in matches:
                    if 'semantic_score' not in match and 'relevance' in match:
                        match['semantic_score'] = match['relevance']
                return matches
        except (json.JSONDecodeError, TypeError):
            pass
        return []

    def _create_translation_mapping_entry(self, target_segment: Dict, related_source_segments: List[Dict],
                                        source_pdf_path: str, target_pdf_path: str,
                                        source_lang: str, target_lang: str) -> Dict:
        """ë²ˆì—­ ë§¤í•‘ ì—”íŠ¸ë¦¬ ìƒì„±"""
        total_score = sum(seg.get('final_score', 0) for seg in related_source_segments)
        avg_score = total_score / len(related_source_segments) if related_source_segments else 0
        
        proximity_summary = self._summarize_proximity_info(related_source_segments)
        
        try:
            s_rel_path = str(Path(source_pdf_path).relative_to(Path(source_pdf_path).parents[2]))
            t_rel_path = str(Path(target_pdf_path).relative_to(Path(target_pdf_path).parents[2]))
        except IndexError:
            s_rel_path = Path(source_pdf_path).name
            t_rel_path = Path(target_pdf_path).name

        return {
            'target_text': target_segment['text'],
            'target_language': target_lang,
            'target_category': target_segment.get('category', 'other'),
            'target_confidence': target_segment.get('confidence', 0.5),
            'target_position': {
                'paragraph': target_segment.get('global_paragraph_index'),
                'sentence': target_segment.get('global_sentence_index'),
                'structure': target_segment.get('structure_type')
            },
            
            'source_segments': related_source_segments,
            'source_language': source_lang,
            'source_count': len(related_source_segments),
            
            'mapping_method': 'llm_japanese_based_with_proximity',
            'average_final_score': round(avg_score, 3),
            'proximity_summary': proximity_summary,
            'mapping_quality_score': round((avg_score + target_segment.get('confidence', 0.5)) / 2, 3),
            
            'source_pdf_file': Path(source_pdf_path).name,
            'target_pdf_file': Path(target_pdf_path).name,
            'source_pdf_path': s_rel_path,
            'target_pdf_path': t_rel_path,
            
            'created_date': datetime.datetime.now().isoformat()[:19],
            'processing_notes': f"Japanese-based N:1 mapping with proximity analysis from {source_lang}-{target_lang} PDF pair"
        }

    def _summarize_proximity_info(self, related_segments: List[Dict]) -> Dict:
        """ì¸ì ‘ì„± ì •ë³´ ìš”ì•½"""
        if not related_segments:
            return {}
        
        same_paragraph = sum(1 for seg in related_segments if seg.get('proximity_bonus', 0) >= 0.2)
        adjacent_paragraph = sum(1 for seg in related_segments if 0.05 <= seg.get('proximity_bonus', 0) < 0.2)
        same_structure = sum(1 for seg in related_segments if seg.get('proximity_bonus', 0) % 0.1 == 0.05)
        
        return {
            'same_paragraph_count': same_paragraph,
            'adjacent_paragraph_count': adjacent_paragraph,
            'same_structure_count': same_structure,
            'avg_proximity_bonus': round(sum(seg.get('proximity_bonus', 0) for seg in related_segments) / len(related_segments), 3)
        }

    def create_translation_pairs_with_context(self, source_segments: List[Dict], 
                                            target_segments: List[Dict], 
                                            source_pdf_path: str,
                                            target_pdf_path: str,
                                            source_lang: str = 'en',
                                            target_lang: str = 'ja') -> List[Dict]:
        """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
        return self.create_japanese_based_translation_mapping(
            source_segments, target_segments, source_pdf_path, target_pdf_path, source_lang, target_lang
        )
