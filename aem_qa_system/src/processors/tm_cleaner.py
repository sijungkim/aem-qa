# src/processors/tm_cleaner.py

import re
import os
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from pymongo import MongoClient
from collections import Counter
from src.config import MONGO_CONNECTION_STRING, DB_NAME

class TMCleaner:
    """ë²ˆì—­ ë©”ëª¨ë¦¬ ì •ì œ í´ë˜ìŠ¤ - ë©”íƒ€ë°ì´í„° ì™„ì „ ë³´ì¡´"""
    
    def __init__(self):
        self.client = MongoClient(MONGO_CONNECTION_STRING)
        self.db = self.client[DB_NAME]
        
        # ì •ì œ ê·œì¹™ ì •ì˜
        self.noise_patterns = [
            r'<[^>]+>',                    # HTML íƒœê·¸
            r'jcr:[a-zA-Z_]+',             # JCR ì†ì„±
            r'sling:[a-zA-Z_]+',           # Sling ì†ì„±
            r'cq:[a-zA-Z_]+',              # CQ ì†ì„±
            r'dam:[a-zA-Z_]+',             # DAM ì†ì„±
            r'^\s*$',                      # ë¹ˆ ë¬¸ìì—´
            r'^[\s\n\t\r]+$',              # ê³µë°±ë§Œ
            r'^[^\w\sê°€-í£ã-ã‚Ÿã‚¡-ãƒ¾ä¸€-é¾¯]+$',  # íŠ¹ìˆ˜ë¬¸ìë§Œ (ë‹¤êµ­ì–´ ì§€ì›)
        ]
        
        # ì¶”ì¶œí•  í…ìŠ¤íŠ¸ í‚¤ë“¤ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
        self.valuable_keys = [
            'text', 'jcr:title', 'title', 'alt', 'linkText', 
            'label', 'placeholder', 'value', 'description'
        ]
        
        print("âœ… TM Cleaner ì´ˆê¸°í™” ì™„ë£Œ")
    
    def clean_translation_memory(self, lang_suffix: str) -> Dict:
        """ë²ˆì—­ ë©”ëª¨ë¦¬ ì •ì œ ë©”ì¸ í•¨ìˆ˜"""
        print(f"ğŸ§¹ [{lang_suffix}] ë²ˆì—­ ë©”ëª¨ë¦¬ ì •ì œ ì‹œì‘...")
        
        # ì»¬ë ‰ì…˜ ì´ë¦„ ì„¤ì •
        raw_tm_collection_name = f"translation_memory_{lang_suffix}"
        clean_tm_collection_name = f"clean_translation_memory_{lang_suffix}"
        stats_collection_name = f"tm_cleaning_stats_{lang_suffix}"
        
        # ì»¬ë ‰ì…˜ ì°¸ì¡°
        raw_tm_collection = self.db[raw_tm_collection_name]
        clean_tm_collection = self.db[clean_tm_collection_name]
        stats_collection = self.db[stats_collection_name]
        
        # ê¸°ì¡´ ì •ì œëœ TM ì‚­ì œ (ìƒˆë¡œ ì‹œì‘)
        clean_tm_collection.delete_many({})
        print(f"   - ê¸°ì¡´ ì •ì œ TM ì‚­ì œ ì™„ë£Œ")
        
        # Raw TM ë°ì´í„° ë¡œë“œ
        print(f"   - Raw TM ë°ì´í„° ë¡œë”© ì¤‘...")
        raw_tm_docs = list(raw_tm_collection.find({}))
        print(f"   - ì´ {len(raw_tm_docs)}ê°œ Raw TM ë¡œë“œ ì™„ë£Œ")
        
        if not raw_tm_docs:
            print("âš ï¸ Raw TM ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {"status": "no_data"}
        
        # ì •ì œ ì²˜ë¦¬
        cleaned_docs = []
        stats = {
            "input_count": len(raw_tm_docs),
            "output_count": 0,
            "removed_count": 0,
            "removal_reasons": Counter(),
            "quality_distribution": Counter(),
            "text_types": Counter()
        }
        
        print(f"   - í…ìŠ¤íŠ¸ ì •ì œ ì¤‘...")
        for i, raw_doc in enumerate(raw_tm_docs):
            if i % 1000 == 0:
                print(f"     ì§„í–‰ë¥ : {i}/{len(raw_tm_docs)} ({i/len(raw_tm_docs)*100:.1f}%)")
            
            # í…ìŠ¤íŠ¸ ì •ì œ ì‹œë„
            clean_result = self._clean_text_pair(raw_doc)
            
            if clean_result["is_valid"]:
                # ë©”íƒ€ë°ì´í„° ë³´ì¡´í•˜ë©´ì„œ ì •ì œëœ ë¬¸ì„œ ìƒì„±
                clean_doc = self._preserve_metadata(raw_doc, clean_result)
                cleaned_docs.append(clean_doc)
                
                stats["output_count"] += 1
                stats["quality_distribution"][clean_result["quality_tier"]] += 1
                stats["text_types"][clean_result["text_type"]] += 1
            else:
                stats["removed_count"] += 1
                for reason in clean_result["removal_reasons"]:
                    stats["removal_reasons"][reason] += 1
        
        print(f"   - ì •ì œ ì™„ë£Œ: {stats['output_count']}/{stats['input_count']} ë³´ì¡´")
        
        # ì •ì œëœ TM ì €ì¥
        if cleaned_docs:
            print(f"   - MongoDBì— ì •ì œëœ TM ì €ì¥ ì¤‘...")
            clean_tm_collection.insert_many(cleaned_docs)
            print(f"   - âœ… {len(cleaned_docs)}ê°œ ì •ì œ TM ì €ì¥ ì™„ë£Œ")
        
        # í†µê³„ ì €ì¥
        stats_doc = {
            "language_pair": lang_suffix,
            "cleaning_date": datetime.utcnow(),
            "input_count": stats["input_count"],
            "output_count": stats["output_count"], 
            "removed_count": stats["removed_count"],
            "removal_reasons": dict(stats["removal_reasons"]),
            "quality_distribution": dict(stats["quality_distribution"]),
            "text_types": dict(stats["text_types"]),
            "cleaning_efficiency": stats["output_count"] / stats["input_count"] if stats["input_count"] > 0 else 0
        }
        
        stats_collection.delete_many({})  # ê¸°ì¡´ í†µê³„ ì‚­ì œ
        stats_collection.insert_one(stats_doc)
        print(f"   - âœ… ì •ì œ í†µê³„ ì €ì¥ ì™„ë£Œ")
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        self._print_cleaning_summary(stats_doc)
        
        return stats_doc
    
    def _clean_text_pair(self, raw_doc: Dict) -> Dict:
        """ë‹¨ì¼ TM ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ ìŒì„ ì •ì œ"""
        source_text = raw_doc.get("source_text", "")
        target_text = raw_doc.get("target_text", "")
        
        # í…ìŠ¤íŠ¸ ì •ì œ
        clean_source = self._clean_single_text(source_text)
        clean_target = self._clean_single_text(target_text)
        
        # ìœ íš¨ì„± ê²€ì‚¬
        validation_result = self._validate_cleaned_text_pair(
            clean_source, clean_target, source_text, target_text
        )
        
        if not validation_result["is_valid"]:
            return validation_result
        
        # í’ˆì§ˆ í‰ê°€
        quality_score = self._calculate_quality_score(clean_source, clean_target)
        quality_tier = self._get_quality_tier(quality_score)
        text_type = self._classify_text_type(clean_source)
        
        return {
            "is_valid": True,
            "clean_source": clean_source,
            "clean_target": clean_target,
            "quality_score": quality_score,
            "quality_tier": quality_tier,
            "text_type": text_type,
            "metadata": {
                "quality_score": quality_score,
                "text_type": text_type,
                "word_count_source": len(clean_source.split()),
                "word_count_target": len(clean_target.split()),
                "cleaning_method": "html_and_noise_removal",
                "cleaned_at": datetime.utcnow()
            }
        }
    
    def _clean_single_text(self, text: str) -> str:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì •ì œ"""
        if not isinstance(text, str):
            return ""
        
        cleaned = text
        
        # HTML íƒœê·¸ ì œê±°
        cleaned = re.sub(r'<[^>]+>', '', cleaned)
        
        # ë…¸ì´ì¦ˆ íŒ¨í„´ ì œê±°
        for pattern in self.noise_patterns[1:]:  # HTML íƒœê·¸ëŠ” ì´ë¯¸ ì œê±°í–ˆìœ¼ë¯€ë¡œ ì œì™¸
            cleaned = re.sub(pattern, '', cleaned)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        cleaned = re.sub(r'\s+', ' ', cleaned)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        cleaned = cleaned.strip()
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”© (ê¸°ë³¸ì ì¸ ê²ƒë“¤)
        html_entities = {
            '&amp;': '&', '&lt;': '<', '&gt;': '>', 
            '&quot;': '"', '&#39;': "'", '&nbsp;': ' '
        }
        for entity, char in html_entities.items():
            cleaned = cleaned.replace(entity, char)
        
        return cleaned
    
    def _validate_cleaned_text_pair(self, clean_source: str, clean_target: str, 
                                   original_source: str, original_target: str) -> Dict:
        """ì •ì œëœ í…ìŠ¤íŠ¸ ìŒì˜ ìœ íš¨ì„± ê²€ì‚¬"""
        removal_reasons = []
        
        # ë¹ˆ í…ìŠ¤íŠ¸ ì²´í¬
        if not clean_source.strip():
            removal_reasons.append("empty_source_text")
        
        if not clean_target.strip():
            removal_reasons.append("empty_target_text")
        
        # ìµœì†Œ ê¸¸ì´ ì²´í¬
        if len(clean_source.strip()) < 2:
            removal_reasons.append("source_too_short")
        
        if len(clean_target.strip()) < 2:
            removal_reasons.append("target_too_short")
        
        # ì˜ë¯¸ìˆëŠ” ë¬¸ì ì²´í¬ (ì•ŒíŒŒë²³, í•œê¸€, ì¼ë³¸ì–´, ì¤‘êµ­ì–´)
        meaningful_pattern = r'[a-zA-Zê°€-í£ã-ã‚Ÿã‚¡-ãƒ¾ä¸€-é¾¯]'
        if not re.search(meaningful_pattern, clean_source):
            removal_reasons.append("source_no_meaningful_chars")
        
        if not re.search(meaningful_pattern, clean_target):
            removal_reasons.append("target_no_meaningful_chars")
        
        # ìˆ«ìë‚˜ íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” ê²½ìš°
        if re.match(r'^[\d\s\-_.,;:!?()[\]{}]+$', clean_source.strip()):
            removal_reasons.append("source_only_numbers_symbols")
        
        if re.match(r'^[\d\s\-_.,;:!?()[\]{}]+$', clean_target.strip()):
            removal_reasons.append("target_only_numbers_symbols")
        
        # JCR/Sling ì†ì„± ì²´í¬
        if any(keyword in original_source.lower() for keyword in ['jcr:', 'sling:', 'cq:', 'dam:']):
            removal_reasons.append("jcr_sling_properties")
        
        return {
            "is_valid": len(removal_reasons) == 0,
            "removal_reasons": removal_reasons
        }
    
    def _calculate_quality_score(self, source: str, target: str) -> float:
        """í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)"""
        score = 0.0
        
        # ê¸¸ì´ ì ìˆ˜ (ì ì ˆí•œ ê¸¸ì´ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        source_len = len(source.split())
        target_len = len(target.split())
        avg_len = (source_len + target_len) / 2
        
        if 2 <= avg_len <= 50:
            score += 0.3
        elif avg_len > 50:
            score += 0.1
        
        # ë¬¸ì¥ ì™„ì„±ë„ (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë“±)
        if any(source.strip().endswith(punct) for punct in '.!?ã€‚ï¼ï¼Ÿ'):
            score += 0.2
        
        # ëŒ€ì†Œë¬¸ì ì ì ˆì„± (ì²« ê¸€ì ëŒ€ë¬¸ì ë“±)
        if source and source[0].isupper():
            score += 0.1
        
        # íŠ¹ìˆ˜ë¬¸ì ë¹„ìœ¨ (ë„ˆë¬´ ë§ìœ¼ë©´ ê°ì )
        special_char_ratio = len(re.findall(r'[^\w\sê°€-í£ã-ã‚Ÿã‚¡-ãƒ¾ä¸€-é¾¯]', source)) / len(source) if source else 0
        if special_char_ratio < 0.3:
            score += 0.2
        
        # ë²ˆì—­ ìŒ ê¸¸ì´ ë¹„ìœ¨ (ë„ˆë¬´ ì°¨ì´ë‚˜ë©´ ê°ì )
        if source_len > 0 and target_len > 0:
            ratio = min(source_len, target_len) / max(source_len, target_len)
            if ratio > 0.3:
                score += 0.2
        
        return min(score, 1.0)
    
    def _get_quality_tier(self, score: float) -> str:
        """í’ˆì§ˆ ì ìˆ˜ë¥¼ í‹°ì–´ë¡œ ë³€í™˜"""
        if score >= 0.8:
            return "high"
        elif score >= 0.5:
            return "medium"
        elif score >= 0.2:
            return "low"
        else:
            return "very_low"
    
    def _classify_text_type(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ìœ í˜• ë¶„ë¥˜"""
        if not text:
            return "empty"
        
        text_lower = text.lower()
        
        # ì œëª© í˜•íƒœ (ì§§ê³  ì²« ê¸€ì ëŒ€ë¬¸ì)
        if len(text.split()) <= 5 and text[0].isupper() and not text.endswith('.'):
            return "title"
        
        # ë¬¸ì¥ í˜•íƒœ (ë§ˆì¹¨í‘œë¡œ ëë‚¨)
        if text.endswith('.') or text.endswith('ã€‚'):
            return "sentence"
        
        # ë§í¬ í…ìŠ¤íŠ¸ (íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨)
        if any(keyword in text_lower for keyword in ['learn more', 'read more', 'click here', 'ìì„¸íˆ', 'ë”ë³´ê¸°']):
            return "link_text"
        
        # ë¼ë²¨ í˜•íƒœ (ì½œë¡ ìœ¼ë¡œ ëë‚¨)
        if text.endswith(':') or text.endswith('ï¼š'):
            return "label"
        
        # ì¼ë°˜ í…ìŠ¤íŠ¸
        return "plain_text"
    
    def _preserve_metadata(self, raw_doc: Dict, clean_result: Dict) -> Dict:
        """ì›ë³¸ ë©”íƒ€ë°ì´í„°ë¥¼ 100% ë³´ì¡´í•˜ë©´ì„œ ì •ì œëœ ë°ì´í„° ì¶”ê°€"""
        # ì›ë³¸ ë¬¸ì„œ ì „ì²´ ë³µì‚¬
        clean_doc = raw_doc.copy()
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ ë°±ì—…
        clean_doc["original_source_text"] = raw_doc.get("source_text", "")
        clean_doc["original_target_text"] = raw_doc.get("target_text", "")
        
        # ì •ì œëœ í…ìŠ¤íŠ¸ë¡œ êµì²´
        clean_doc["source_text"] = clean_result["clean_source"]
        clean_doc["target_text"] = clean_result["clean_target"]
        
        # ì •ì œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        clean_doc.update(clean_result["metadata"])
        
        return clean_doc
    
    def _print_cleaning_summary(self, stats: Dict):
        """ì •ì œ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\nğŸ“Š [{stats['language_pair']}] TM ì •ì œ ê²°ê³¼ ìš”ì•½:")
        print(f"   ğŸ”¢ ì…ë ¥: {stats['input_count']:,}ê°œ")
        print(f"   âœ… ë³´ì¡´: {stats['output_count']:,}ê°œ ({stats['cleaning_efficiency']:.1%})")
        print(f"   ğŸ—‘ï¸ ì œê±°: {stats['removed_count']:,}ê°œ")
        
        print(f"\nğŸ“ˆ ì œê±° ì´ìœ :")
        for reason, count in sorted(stats['removal_reasons'].items(), key=lambda x: x[1], reverse=True):
            print(f"   - {reason}: {count:,}ê°œ")
        
        print(f"\nğŸ† í’ˆì§ˆ ë¶„í¬:")
        for tier, count in sorted(stats['quality_distribution'].items(), key=lambda x: x[1], reverse=True):
            print(f"   - {tier}: {count:,}ê°œ")
        
        print(f"\nğŸ“ í…ìŠ¤íŠ¸ ìœ í˜•:")
        for text_type, count in sorted(stats['text_types'].items(), key=lambda x: x[1], reverse=True):
            print(f"   - {text_type}: {count:,}ê°œ")

# í¸ì˜ í•¨ìˆ˜ë“¤
def clean_all_language_pairs():
    """ëª¨ë“  ì–¸ì–´ ìŒì˜ TM ì •ì œ"""
    from src.config import SUPPORTED_LANGUAGE_PAIRS
    
    cleaner = TMCleaner()
    results = {}
    
    for source_lang, target_lang in SUPPORTED_LANGUAGE_PAIRS:
        lang_suffix = f"{source_lang}_{target_lang}"
        print(f"\nğŸŒ {source_lang.upper()}-{target_lang.upper()} TM ì •ì œ ì‹œì‘...")
        
        try:
            result = cleaner.clean_translation_memory(lang_suffix)
            results[lang_suffix] = result
        except Exception as e:
            print(f"âŒ {lang_suffix} ì •ì œ ì‹¤íŒ¨: {str(e)}")
            results[lang_suffix] = {"status": "error", "error": str(e)}
    
    return results

def get_cleaning_stats(lang_suffix: str) -> Optional[Dict]:
    """íŠ¹ì • ì–¸ì–´ ìŒì˜ ì •ì œ í†µê³„ ì¡°íšŒ"""
    client = MongoClient(MONGO_CONNECTION_STRING)
    db = client[DB_NAME]
    stats_collection = db[f"tm_cleaning_stats_{lang_suffix}"]
    
    return stats_collection.find_one({}, sort=[("cleaning_date", -1)])