# src/processors/semantic_segmenter.py

import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import AgglomerativeClustering
import spacy
from typing import List, Dict, Tuple
from datetime import datetime
import re

class SemanticSegmenter:
    """ì˜ë¯¸ ì„ë² ë”© ê¸°ë°˜ ì§€ëŠ¥í˜• í…ìŠ¤íŠ¸ ë¶„í• """
    
    def __init__(self):
        # ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ (ê¸°ì¡´ ChromaDBì™€ ë™ì¼)
        self.embedding_model = SentenceTransformer('intfloat/multilingual-e5-large')
        
        # ë¬¸ì¥ ë¶„í•  ëª¨ë¸ë“¤
        self.nlp_models = {}
        self._load_spacy_models()
        
        # ë¶„í•  ì„¤ì • (ì ì‘ì  ë¶„í•  ìµœì í™”)
        self.config = {
            'min_segment_length': 15,      # ìµœì†Œ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ë” ìœ ì—°í•˜ê²Œ)
            'max_segment_length': 150,     # ìµœëŒ€ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´
            'optimal_length': 70,          # ëª©í‘œ ê¸¸ì´ (ì•½ê°„ ì¤„ì„)
            'similarity_threshold': 0.72,  # ì˜ë¯¸ ìœ ì‚¬ë„ ì„ê³„ê°’ (ì•½ê°„ ë‚®ì¶¤)
            'min_sentence_length': 8,      # ìµœì†Œ ë¬¸ì¥ ê¸¸ì´ (ë” ìœ ì—°í•˜ê²Œ)
            
            # ì ì‘ì  ë¶„í•  ê¸°ì¤€ë“¤
            'very_long_threshold': 300,    # ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ ê¸°ì¤€
            'long_threshold': 200,         # ê¸´ í…ìŠ¤íŠ¸ ê¸°ì¤€  
            'medium_threshold': 120,       # ì¤‘ê°„ ê¸¸ì´ ê¸°ì¤€
            'max_sentences_threshold': 3,  # ìµœëŒ€ ë¬¸ì¥ ìˆ˜ ê¸°ì¤€
            'complexity_threshold': 0.7,   # ë³µì¡ë„ ì„ê³„ê°’
            'length_ratio_threshold': 0.6  # ê¸¸ì´ ë¹„ìœ¨ ì„ê³„ê°’
        }
        
        print("âœ… Semantic Segmenter ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   - ì„ë² ë”© ëª¨ë¸: {self.embedding_model}")
        print(f"   - ì„¤ì •: {self.config}")
    
    def _load_spacy_models(self):
        """spaCy ëª¨ë¸ ë¡œë“œ"""
        models = {
            'en': 'en_core_web_sm',
            'ko': 'ko_core_news_sm',  # pip install spacy-koNLPy
            'ja': 'ja_core_news_sm'
        }
        
        for lang, model_name in models.items():
            try:
                self.nlp_models[lang] = spacy.load(model_name)
                print(f"   âœ… {lang}: {model_name} ë¡œë“œ ì™„ë£Œ")
            except OSError:
                print(f"   âš ï¸ {lang}: {model_name} ëª¨ë¸ ì—†ìŒ - ê¸°ë³¸ ë¶„í•  ì‚¬ìš©")
                self.nlp_models[lang] = None
    
    def segment_text_pair(self, source_text: str, target_text: str, 
                         source_lang: str, target_lang: str,
                         original_metadata: Dict) -> List[Dict]:
        """ë©”ì¸ í•¨ìˆ˜: í…ìŠ¤íŠ¸ ìŒì„ ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ ë¶„í• """
        
        # ë¶„í•  í•„ìš”ì„± ê²€ì‚¬
        if not self._needs_segmentation(source_text, target_text):
            return [{
                **original_metadata,
                'source_text': source_text,
                'target_text': target_text,
                'segment_index': 0,
                'total_segments': 1,
                'segment_order': 0,
                'original_component_order': original_metadata.get('component_order', 0),
                'segment_type': 'no_split_needed',
                'is_segmented': False,
                'segmentation_reason': 'below_threshold',
                'segmentation_conditions_checked': {
                    'max_length': max(len(source_text), len(target_text)),
                    'avg_length': (len(source_text) + len(target_text)) / 2,
                    'source_sentences': self._count_sentences(source_text, 'source'),
                    'target_sentences': self._count_sentences(target_text, 'target'),
                    'complexity_score': self._calculate_text_complexity(source_text, target_text),
                    'length_imbalance': self._has_length_imbalance(source_text, target_text)
                }
            }]
        
        print(f"ğŸ” ì ì‘ì  ë¶„í•  ì‹œì‘: {len(source_text)}ì vs {len(target_text)}ì")
        print(f"   ëª©í‘œ: ì˜ë¯¸ ì¼ê´€ì„± ìœ ì§€í•˜ë©° {self.config['optimal_length']}ì ë‚´ì™¸ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±")
        
        try:
            # 1. ì´ˆê¸° ë¬¸ì¥ ë¶„í• 
            source_sentences = self._split_into_sentences(source_text, source_lang)
            target_sentences = self._split_into_sentences(target_text, target_lang)
            
            print(f"   - ë¬¸ì¥ ë¶„í• : Source {len(source_sentences)}ê°œ, Target {len(target_sentences)}ê°œ")
            
            # 2. ì„ë² ë”© ê³„ì‚°
            source_embeddings = self._compute_embeddings(source_sentences)
            target_embeddings = self._compute_embeddings(target_sentences)
            
            # 3. ì˜ë¯¸ ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
            source_segments = self._create_semantic_segments(source_sentences, source_embeddings)
            target_segments = self._create_semantic_segments(target_sentences, target_embeddings)
            
            print(f"   - ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±: Source {len(source_segments)}ê°œ, Target {len(target_segments)}ê°œ")
            
            # 4. ë²ˆì—­ ìŒ ì •ë ¬
            aligned_pairs = self._align_translation_pairs(
                source_segments, target_segments, 
                source_embeddings, target_embeddings
            )
            
            print(f"   - ì •ë ¬ ì™„ë£Œ: {len(aligned_pairs)}ê°œ ìŒ")
            
            # 5. ë©”íƒ€ë°ì´í„° ë³´ì¡´í•˜ì—¬ ìµœì¢… ê²°ê³¼ ìƒì„±
            segmented_records = self._create_segmented_records(
                aligned_pairs, original_metadata, source_text, target_text
            )
            
            return segmented_records
            
        except Exception as e:
            print(f"âŒ ë¶„í•  ì‹¤íŒ¨: {str(e)} - ì›ë³¸ ë°˜í™˜")
            return [{
                **original_metadata,
                'source_text': source_text,
                'target_text': target_text,
                'segment_index': 0,
                'total_segments': 1,
                'segment_order': 0,
                'original_component_order': original_metadata.get('component_order', 0),
                'segment_type': 'segmentation_failed',
                'is_segmented': False,
                'segmentation_error': str(e)
            }]
    
    def _needs_segmentation(self, source_text: str, target_text: str) -> bool:
        """ì ì‘ì  ë¶„í•  í•„ìš”ì„± íŒë‹¨"""
        max_length = max(len(source_text), len(target_text))
        avg_length = (len(source_text) + len(target_text)) / 2
        
        # ë¬¸ì¥ ìˆ˜ ê³„ì‚° (ì–¸ì–´ë³„ íŒ¨í„´)
        source_sentences = self._count_sentences(source_text, 'source')
        target_sentences = self._count_sentences(target_text, 'target')
        max_sentences = max(source_sentences, target_sentences)
        
        # ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°
        complexity_score = self._calculate_text_complexity(source_text, target_text)
        
        # ì ì‘ì  ë¶„í•  ì¡°ê±´ (ì—¬ëŸ¬ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ ë¶„í• )
        conditions = {
            'very_long': max_length > 300,                    # ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸
            'long_text': max_length > 200,                    # ê¸´ í…ìŠ¤íŠ¸
            'multiple_sentences': max_sentences > 3,          # ë‹¤ì¤‘ ë¬¸ì¥
            'medium_multi': max_length > 120 and max_sentences > 2,  # ì¤‘ê°„ ê¸¸ì´ + ë³µìˆ˜ ë¬¸ì¥
            'complex_text': complexity_score > 0.7,           # ë³µì¡í•œ í…ìŠ¤íŠ¸
            'length_imbalance': self._has_length_imbalance(source_text, target_text)  # ê¸¸ì´ ë¶ˆê· í˜•
        }
        
        # ë¶„í•  ì‚¬ìœ  ì €ì¥ (ë””ë²„ê¹…ìš©)
        triggered_conditions = [k for k, v in conditions.items() if v]
        
        needs_split = len(triggered_conditions) > 0
        
        if needs_split:
            print(f"   ğŸ” ë¶„í•  ì¡°ê±´ ì¶©ì¡±: {triggered_conditions}")
            print(f"      ê¸¸ì´: {len(source_text)}â†”{len(target_text)}, ë¬¸ì¥: {source_sentences}â†”{target_sentences}")
        
        return needs_split
    
    def _count_sentences(self, text: str, text_type: str) -> int:
        """ì–¸ì–´ íŒ¨í„´ì„ ê³ ë ¤í•œ ë¬¸ì¥ ìˆ˜ ê³„ì‚°"""
        if not text.strip():
            return 0
        
        # ë‹¤êµ­ì–´ ë¬¸ì¥ ì¢…ë£Œ íŒ¨í„´
        sentence_patterns = [
            r'[.!?]+\s+',     # ì˜ì–´: ë§ˆì¹¨í‘œ+ê³µë°±
            r'[ã€‚ï¼ï¼Ÿ]+\s*',   # ì¼ë³¸ì–´: ì¼ë³¸ì–´ ë§ˆì¹¨í‘œ
            r'[.!?ã€‚]+\s*'    # í•œêµ­ì–´: í˜¼í•© íŒ¨í„´
        ]
        
        max_sentence_count = 0
        for pattern in sentence_patterns:
            sentences = re.split(pattern, text.strip())
            sentences = [s.strip() for s in sentences if s.strip()]
            max_sentence_count = max(max_sentence_count, len(sentences))
        
        return max_sentence_count
    
    def _calculate_text_complexity(self, source_text: str, target_text: str) -> float:
        """í…ìŠ¤íŠ¸ ë³µì¡ë„ ì ìˆ˜ ê³„ì‚° (0.0-1.0)"""
        complexity_score = 0.0
        
        # 1. íŠ¹ìˆ˜ë¬¸ì ë°€ë„
        for text in [source_text, target_text]:
            special_chars = len(re.findall(r'[^\w\sê°€-í£ã-ã‚Ÿã‚¡-ãƒ¾ä¸€-é¾¯]', text))
            if len(text) > 0:
                special_ratio = special_chars / len(text)
                complexity_score += special_ratio * 0.3
        
        # 2. ê¸´ ë‹¨ì–´ ë¹„ìœ¨
        for text in [source_text, target_text]:
            words = text.split()
            if words:
                long_words = [w for w in words if len(w) > 8]
                long_word_ratio = len(long_words) / len(words)
                complexity_score += long_word_ratio * 0.2
        
        # 3. ì—°ì†ëœ ëŒ€ë¬¸ì (ì•½ì–´/ê¸°ìˆ ìš©ì–´)
        for text in [source_text, target_text]:
            caps_sequences = re.findall(r'[A-Z]{2,}', text)
            if len(text) > 0:
                caps_ratio = len(''.join(caps_sequences)) / len(text)
                complexity_score += caps_ratio * 0.3
        
        # 4. ê´„í˜¸, ì½œë¡  ë“± êµ¬ì¡°ì  ìš”ì†Œ
        structural_elements = 0
        for text in [source_text, target_text]:
            structural_elements += len(re.findall(r'[():;,\-â€“â€”]', text))
        
        if len(source_text) + len(target_text) > 0:
            structural_ratio = structural_elements / (len(source_text) + len(target_text))
            complexity_score += structural_ratio * 0.2
        
        return min(complexity_score / 2, 1.0)  # 2ë¡œ ë‚˜ëˆ„ì–´ ì •ê·œí™”
    
    def _has_length_imbalance(self, source_text: str, target_text: str) -> bool:
        """ë²ˆì—­ ìŒ ê¸¸ì´ ë¶ˆê· í˜• ê²€ì‚¬"""
        if not source_text or not target_text:
            return False
        
        ratio = min(len(source_text), len(target_text)) / max(len(source_text), len(target_text))
        
        # ê¸¸ì´ ë¹„ìœ¨ì´ ë„ˆë¬´ ì°¨ì´ë‚˜ê³ , ë‘˜ ì¤‘ í•˜ë‚˜ê°€ ì¶©ë¶„íˆ ê¸¸ë©´ ë¶„í•  ê³ ë ¤
        return ratio < 0.6 and max(len(source_text), len(target_text)) > 150
    
    def _split_into_sentences(self, text: str, language: str) -> List[str]:
        """ì–¸ì–´ë³„ ë¬¸ì¥ ë¶„í• """
        nlp = self.nlp_models.get(language)
        
        if nlp:
            # spaCy ê¸°ë°˜ ì§€ëŠ¥í˜• ë¶„í• 
            doc = nlp(text)
            sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]
        else:
            # í´ë°±: ê·œì¹™ ê¸°ë°˜ ë¶„í• 
            sentences = self._rule_based_split(text, language)
        
        # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ í•„í„°ë§
        sentences = [s for s in sentences if len(s) >= self.config['min_sentence_length']]
        
        return sentences
    
    def _rule_based_split(self, text: str, language: str) -> List[str]:
        """ê·œì¹™ ê¸°ë°˜ ë¬¸ì¥ ë¶„í•  (í´ë°±)"""
        patterns = {
            'en': r'[.!?]+\s+(?=[A-Z])',
            'ko': r'[.!?ã€‚]+\s*',
            'ja': r'[ã€‚ï¼ï¼Ÿ]+\s*'
        }
        
        pattern = patterns.get(language, r'[.!?]+\s+')
        sentences = re.split(pattern, text.strip())
        return [s.strip() for s in sentences if s.strip()]
    
    def _compute_embeddings(self, sentences: List[str]) -> np.ndarray:
        """ë¬¸ì¥ ì„ë² ë”© ê³„ì‚°"""
        if not sentences:
            return np.array([])
        
        embeddings = self.embedding_model.encode(
            sentences, 
            show_progress_bar=False,
            convert_to_tensor=False,
            normalize_embeddings=True  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœì í™”
        )
        
        return embeddings
    
    def _create_semantic_segments(self, sentences: List[str], embeddings: np.ndarray) -> List[Dict]:
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±"""
        if len(sentences) <= 1:
            return [{'text': ' '.join(sentences), 'sentences': sentences, 'embedding': embeddings[0] if len(embeddings) > 0 else None}]
        
        # ë™ì  í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ ìµœì  ë¶„í• ì  ì°¾ê¸°
        segments = self._find_optimal_segments(sentences, embeddings)
        
        return segments
    
    def _find_optimal_segments(self, sentences: List[str], embeddings: np.ndarray) -> List[Dict]:
        """ë™ì  í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ ìµœì  ì„¸ê·¸ë¨¼íŠ¸ ì°¾ê¸°"""
        n = len(sentences)
        if n == 0:
            return []
        
        # DP í…Œì´ë¸”: dp[i] = 0ë¶€í„° iê¹Œì§€ì˜ ìµœì  ë¶„í•  ì ìˆ˜
        dp = [float('-inf')] * (n + 1)
        dp[0] = 0
        parent = [-1] * (n + 1)  # ì—­ì¶”ì ìš©
        
        for i in range(1, n + 1):
            for j in range(i):
                # jë¶€í„° i-1ê¹Œì§€ë¥¼ í•˜ë‚˜ì˜ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë§Œë“¤ ë•Œì˜ ì ìˆ˜
                segment_sentences = sentences[j:i]
                segment_text = ' '.join(segment_sentences)
                
                # ì„¸ê·¸ë¨¼íŠ¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                quality_score = self._calculate_segment_quality(
                    segment_text, segment_sentences, embeddings[j:i]
                )
                
                if dp[j] + quality_score > dp[i]:
                    dp[i] = dp[j] + quality_score
                    parent[i] = j
        
        # ì—­ì¶”ì ìœ¼ë¡œ ìµœì  ë¶„í•  ë³µì›
        segments = []
        i = n
        while i > 0:
            j = parent[i]
            segment_sentences = sentences[j:i]
            segment_text = ' '.join(segment_sentences)
            segment_embedding = np.mean(embeddings[j:i], axis=0)
            
            segments.append({
                'text': segment_text,
                'sentences': segment_sentences,
                'embedding': segment_embedding,
                'start_idx': j,
                'end_idx': i-1
            })
            i = j
        
        segments.reverse()  # ìˆœì„œ ë³µì›
        return segments
    
    def _calculate_segment_quality(self, text: str, sentences: List[str], embeddings: np.ndarray) -> float:
        """ì„¸ê·¸ë¨¼íŠ¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if len(embeddings) == 0:
            return float('-inf')
        
        score = 0.0
        
        # 1. ê¸¸ì´ ì ìˆ˜ (ê°€ìš°ì‹œì•ˆ ë¶„í¬)
        length = len(text)
        optimal_length = self.config['optimal_length']
        length_score = np.exp(-((length - optimal_length) ** 2) / (2 * (optimal_length/3) ** 2))
        score += length_score * 100
        
        # 2. ê¸¸ì´ ì œì•½ (í•˜ë“œ ì œì•½)
        if length < self.config['min_segment_length'] or length > self.config['max_segment_length']:
            return float('-inf')
        
        # 3. ì˜ë¯¸ì  ì‘ì§‘ë„ ì ìˆ˜
        if len(embeddings) > 1:
            # ë¬¸ì¥ ê°„ í‰ê·  ìœ ì‚¬ë„
            similarities = []
            for i in range(len(embeddings)):
                for j in range(i+1, len(embeddings)):
                    sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]
                    similarities.append(sim)
            
            if similarities:
                coherence_score = np.mean(similarities)
                score += coherence_score * 50
        
        # 4. ë¬¸ì¥ ìˆ˜ ê· í˜• ì ìˆ˜ (ë„ˆë¬´ ë§ê±°ë‚˜ ì ìœ¼ë©´ ê°ì )
        sentence_count = len(sentences)
        if 1 <= sentence_count <= 3:
            score += 20
        elif sentence_count > 5:
            score -= 10
        
        return score
    
    def _align_translation_pairs(self, source_segments: List[Dict], target_segments: List[Dict],
                                source_embeddings: np.ndarray, target_embeddings: np.ndarray) -> List[Dict]:
        """ë²ˆì—­ ìŒ ì •ë ¬ (Cross-lingual Alignment)"""
        
        # ì„¸ê·¸ë¨¼íŠ¸ ì„ë² ë”© ê³„ì‚°
        source_seg_embeddings = np.array([seg['embedding'] for seg in source_segments])
        target_seg_embeddings = np.array([seg['embedding'] for seg in target_segments])
        
        # í¬ë¡œìŠ¤ ì–¸ì–´ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        similarity_matrix = cosine_similarity(source_seg_embeddings, target_seg_embeddings)
        
        # í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜ ë˜ëŠ” ê·¸ë¦¬ë”” ë§¤ì¹­ìœ¼ë¡œ ìµœì  ì •ë ¬
        aligned_pairs = self._greedy_alignment(
            source_segments, target_segments, similarity_matrix
        )
        
        return aligned_pairs
    
    def _greedy_alignment(self, source_segments: List[Dict], target_segments: List[Dict], 
                         similarity_matrix: np.ndarray) -> List[Dict]:
        """ê·¸ë¦¬ë”” ë°©ì‹ìœ¼ë¡œ ë²ˆì—­ ìŒ ì •ë ¬"""
        aligned_pairs = []
        used_target_indices = set()
        
        for i, source_seg in enumerate(source_segments):
            # ê°€ì¥ ìœ ì‚¬í•œ íƒ€ê²Ÿ ì„¸ê·¸ë¨¼íŠ¸ ì°¾ê¸°
            best_target_idx = -1
            best_similarity = -1
            
            for j, target_seg in enumerate(target_segments):
                if j not in used_target_indices:
                    similarity = similarity_matrix[i][j]
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_target_idx = j
            
            if best_target_idx >= 0:
                aligned_pairs.append({
                    'source_segment': source_segments[i],
                    'target_segment': target_segments[best_target_idx],
                    'alignment_confidence': best_similarity,
                    'source_index': i,
                    'target_index': best_target_idx
                })
                used_target_indices.add(best_target_idx)
            else:
                # ë§¤ì¹­ë˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
                aligned_pairs.append({
                    'source_segment': source_segments[i],
                    'target_segment': None,
                    'alignment_confidence': 0.0,
                    'source_index': i,
                    'target_index': -1
                })
        
        # ë§¤ì¹­ë˜ì§€ ì•Šì€ íƒ€ê²Ÿ ì„¸ê·¸ë¨¼íŠ¸ë“¤ë„ ì¶”ê°€
        for j, target_seg in enumerate(target_segments):
            if j not in used_target_indices:
                aligned_pairs.append({
                    'source_segment': None,
                    'target_segment': target_segments[j],
                    'alignment_confidence': 0.0,
                    'source_index': -1,
                    'target_index': j
                })
        
        return aligned_pairs
    
    def _create_segmented_records(self, aligned_pairs: List[Dict], 
                                 original_metadata: Dict,
                                 original_source: str, original_target: str) -> List[Dict]:
        """ë©”íƒ€ë°ì´í„° 100% ë³´ì¡´í•˜ì—¬ ì„¸ê·¸ë¨¼íŠ¸ ë ˆì½”ë“œ ìƒì„±"""
        segmented_records = []
        total_segments = len(aligned_pairs)
        
        for i, pair in enumerate(aligned_pairs):
            # ì›ë³¸ ë©”íƒ€ë°ì´í„° 100% ë³µì‚¬
            segment_record = original_metadata.copy()
            
            # í…ìŠ¤íŠ¸ ì •ë³´ ì—…ë°ì´íŠ¸
            source_text = pair['source_segment']['text'] if pair['source_segment'] else ""
            target_text = pair['target_segment']['text'] if pair['target_segment'] else ""
            
            # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´ ì¶”ê°€
            segment_record.update({
                # === í•µì‹¬ í…ìŠ¤íŠ¸ ë°ì´í„° ===
                'source_text': source_text,
                'target_text': target_text,
                
                # === ì›ë³¸ ë³´ì¡´ ===
                'original_source_text': original_source,
                'original_target_text': original_target,
                
                # === ì„¸ê·¸ë¨¼íŠ¸ ë©”íƒ€ë°ì´í„° ===
                'segment_index': i,
                'total_segments': total_segments,
                'segment_order': i,  # ëª…ì‹œì  ìˆœì„œ í•„ë“œ
                'original_component_order': original_metadata.get('component_order', 0),  # ì›ë³¸ ì»´í¬ë„ŒíŠ¸ ìˆœì„œ
                'segment_type': 'semantic_embedding_split',
                'is_segmented': total_segments > 1,
                
                # === ì„¸ê·¸ë¨¼íŠ¸ í’ˆì§ˆ ì •ë³´ ===
                'alignment_confidence': pair['alignment_confidence'],
                'source_sentence_count': len(pair['source_segment']['sentences']) if pair['source_segment'] else 0,
                'target_sentence_count': len(pair['target_segment']['sentences']) if pair['target_segment'] else 0,
                'source_segment_length': len(source_text),
                'target_segment_length': len(target_text),
                
                # === ì„ë² ë”© ì •ë³´ ===
                'has_source_embedding': pair['source_segment'] is not None,
                'has_target_embedding': pair['target_segment'] is not None,
                'cross_lingual_similarity': pair['alignment_confidence'],
                
                # === ë¶„í•  ë©”íƒ€ë°ì´í„° ===
                'segmentation_method': 'semantic_embedding_dp',
                'embedding_model': 'intfloat/multilingual-e5-large',
                'segmentation_config': self.config,
                'segmentation_timestamp': datetime.utcnow(),
                
                # === í’ˆì§ˆ ë©”íŠ¸ë¦­ ===
                'length_optimization_score': self._calculate_length_score(source_text, target_text),
                'semantic_coherence_score': self._calculate_coherence_score(pair),
                'translation_alignment_score': pair['alignment_confidence']
            })
            
            segmented_records.append(segment_record)
        
        return segmented_records
    
    def _calculate_length_score(self, source_text: str, target_text: str) -> float:
        """ê¸¸ì´ ìµœì í™” ì ìˆ˜ ê³„ì‚°"""
        avg_length = (len(source_text) + len(target_text)) / 2
        optimal_length = self.config['optimal_length']
        
        # ê°€ìš°ì‹œì•ˆ ì ìˆ˜ (0-1)
        return np.exp(-((avg_length - optimal_length) ** 2) / (2 * (optimal_length/3) ** 2))
    
    def _calculate_coherence_score(self, pair: Dict) -> float:
        """ì˜ë¯¸ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        # ì†ŒìŠ¤ì™€ íƒ€ê²Ÿ ëª¨ë‘ ìˆëŠ” ê²½ìš°ë§Œ ê³„ì‚°
        if not (pair['source_segment'] and pair['target_segment']):
            return 0.0
        
        # ì„¸ê·¸ë¨¼íŠ¸ ë‚´ ë¬¸ì¥ ìˆ˜ê°€ ì ì ˆí•œì§€ í™•ì¸
        source_sentences = len(pair['source_segment']['sentences'])
        target_sentences = len(pair['target_segment']['sentences'])
        
        # ë¬¸ì¥ ìˆ˜ ê· í˜• ì ìˆ˜
        if source_sentences == target_sentences and 1 <= source_sentences <= 3:
            return 1.0
        elif abs(source_sentences - target_sentences) <= 1:
            return 0.8
        else:
            return 0.5