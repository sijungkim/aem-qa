# src/processors/translation_pair_generator.py

import numpy as np
from pathlib import Path
from typing import List, Dict
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from langdetect import detect
import re

from src.config import RAG_EMBEDDING_MODEL


class TranslationPairGenerator:
    """ì„ë² ë”© ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²ˆì—­ ìŒì„ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, similarity_threshold: float = 0.6, allow_multiple_matches: bool = False):
        self.similarity_threshold = similarity_threshold
        self.allow_multiple_matches = allow_multiple_matches
        self.embedding_model = None

    def load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        print("ğŸ¤– ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
        self.embedding_model = SentenceTransformer(RAG_EMBEDDING_MODEL)
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

    def generate_pairs(
        self,
        source_segments: List[str],
        target_segments: List[str],
        target_pdf_path: str,
        source_lang: str = 'en',
        target_lang: str = 'ja'
    ) -> List[Dict]:
        """ì†ŒìŠ¤ì™€ íƒ€ê²Ÿ ì–¸ì–´ ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ë²ˆì—­ ìŒì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.embedding_model:
            self.load_embedding_model()

        if not source_segments or not target_segments:
            return []

        # ì–¸ì–´ë³„ ì „ì²˜ë¦¬
        source_segments = self._preprocess_segments(source_segments, source_lang)
        target_segments = self._preprocess_segments(target_segments, target_lang)

        # ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ ì •ê·œí™”
        source_segments = self._normalize_segments(source_segments)
        target_segments = self._normalize_segments(target_segments)

        # ì„ë² ë”© ê³„ì‚°
        source_embeddings = self._compute_embeddings(source_segments)
        target_embeddings = self._compute_embeddings(target_segments)

        # ìœ ì‚¬ë„ ë§¤ì¹­
        return self._match_by_similarity(
            source_segments, target_segments,
            source_embeddings, target_embeddings,
            target_pdf_path, source_lang, target_lang
        )

    def _compute_embeddings(self, segments: List[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì˜ ì„ë² ë”©ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        return self.embedding_model.encode(segments, show_progress_bar=False)

    def _match_by_similarity(
        self,
        source_segments: List[str],
        target_segments: List[str],
        source_embeddings: np.ndarray,
        target_embeddings: np.ndarray,
        target_pdf_path: str,
        source_lang: str,
        target_lang: str
    ) -> List[Dict]:
        """ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ë²ˆì—­ ìŒì„ ë§¤ì¹­í•©ë‹ˆë‹¤."""
        pairs = []
        target_filename = Path(target_pdf_path).name
        used_target_indices = set()

        for i, source_segment in enumerate(source_segments):
            similarities = cosine_similarity([source_embeddings[i]], target_embeddings)[0]
            best_idx = int(np.argmax(similarities))
            best_similarity = float(similarities[best_idx])

            if best_similarity >= self.similarity_threshold and (
                self.allow_multiple_matches or best_idx not in used_target_indices
            ):
                pairs.append({
                    'source_text': source_segment.strip(),
                    'target_text': target_segments[best_idx].strip(),
                    'similarity_score': round(best_similarity, 4),
                    'source_file': target_filename,
                    'source_lang': source_lang,
                    'target_lang': target_lang,
                    'pair_type': 'pdf_extracted'
                })

                used_target_indices.add(best_idx)

        return pairs

    def _normalize_segments(self, segments: List[str], min_len: int = 5, max_len: int = 512) -> List[str]:
        """ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ ì •ê·œí™”"""
        return [seg for seg in segments if min_len <= len(seg) <= max_len]

    def _preprocess_segments(self, segments: List[str], lang: str) -> List[str]:
        """ì–¸ì–´ë³„ ì„¸ê·¸ë¨¼íŠ¸ ì „ì²˜ë¦¬"""
        if lang == 'ja':
            return self._split_japanese_sentences(" ".join(segments))
        return segments

    # def _split_japanese_sentences(self, text: str) -> List[str]:
    #     """ì¼ë³¸ì–´ ë¬¸ì¥ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
    #     raw_segments = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
    #     return [seg.strip() for seg in raw_segments if seg.strip()]
    
    def _split_japanese_sentences(self, text: str) -> List[str]:
        """ì¼ë³¸ì–´ ë¬¸ì¥ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•  (LLM ê¸°ë°˜)"""
        try:
            from src.processors.pdf_text_extractor import llama3_generate
        except ImportError:
            print("âš ï¸ llama3_generate í•¨ìˆ˜ ë¡œë“œ ì‹¤íŒ¨. ê¸°ë³¸ ë¶„í•  ë°©ì‹ ì‚¬ìš©.")
            return re.split(r'[ã€‚ï¼ï¼Ÿ]', text)

        prompt = f"""ä»¥ä¸‹ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’æ„å‘³ã®ã‚ã‚‹å˜ä½ã§åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¯1ã€œ2æ–‡ç¨‹åº¦ã§ã€æ”¹è¡Œã§åŒºåˆ‡ã£ã¦ãã ã•ã„ã€‚

    {text}
    """
        response = llama3_generate(prompt)
        segments = [line.strip("-â€¢â— ") for line in response.split("\n") if line.strip()]

        return [s for s in segments if len(s) > 5]

