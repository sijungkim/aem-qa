# src/processors/hybrid_pdf_pair_extractor.py (Xì¶• ì¢Œí‘œ ì¶”ê°€ ë²„ì „)

import fitz  # PyMuPDF
import pdfplumber
import pandas as pd
import torch
from sentence_transformers import SentenceTransformer, util
from typing import List, Tuple, Dict

class HybridPDFPairExtractor:
    """
    ë ˆì´ì•„ì›ƒ(X,Y ì¢Œí‘œ)ê³¼ ì˜ë¯¸(ì„ë² ë”©)ë¥¼ í•¨ê»˜ ë¶„ì„í•˜ì—¬ PDF ë²ˆì—­ìŒì„ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤.
    """

    def __init__(self, model_name: str = 'intfloat/multilingual-e5-large'):
        """ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        print("ğŸ¤– ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        self.model = SentenceTransformer(model_name)
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

    def _get_table_areas(self, doc: fitz.Document) -> List[List[Dict]]:
        """PyMuPDFë¥¼ ì‚¬ìš©í•´ í˜ì´ì§€ë³„ í…Œì´ë¸”ì˜ ìœ„ì¹˜(bounding box)ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
        areas_by_page = []
        for page in doc:
            page_areas = []
            tables = page.find_tables()
            for table in tables:
                page_areas.append({'top': table.bbox[1], 'bottom': table.bbox[3]})
            areas_by_page.append(page_areas)
        return areas_by_page

    def _extract_text_blocks(self, doc: fitz.Document, table_areas_by_page: List[List[Dict]]) -> List[List[Tuple]]:
        """PyMuPDFë¥¼ ì‚¬ìš©í•´ í‘œ(table) ì˜ì—­ì„ ì œì™¸í•œ í…ìŠ¤íŠ¸ ë¸”ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        blocks_by_page = []
        for i, page in enumerate(doc):
            page_blocks = []
            table_areas = table_areas_by_page[i]
            blocks = page.get_text("blocks")
            for block in blocks:
                x0, y0, x1, y1, text, _, _ = block
                text = " ".join(text.strip().split())
                if not text:
                    continue

                is_in_table = False
                for table_area in table_areas:
                    block_center_y = (y0 + y1) / 2
                    if table_area['top'] <= block_center_y <= table_area['bottom']:
                        is_in_table = True
                        break
                
                if not is_in_table:
                    page_blocks.append(block)
            blocks_by_page.append(page_blocks)
        return blocks_by_page

    def generate_pairs(self, source_pdf_path: str, target_pdf_path: str, 
                       y_tolerance: int = 10, x_tolerance: int = 50, 
                       similarity_threshold: float = 0.65) -> pd.DataFrame:
        """
        ë‘ PDF íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ ë²ˆì—­ìŒì„ ìƒì„±í•˜ê³  ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        x_tolerance: Xì¶•(ìˆ˜í‰) ìœ„ì¹˜ í—ˆìš© ì˜¤ì°¨ (í”½ì…€)
        """
        all_pairs = []
        source_doc = fitz.open(source_pdf_path)
        target_doc = fitz.open(target_pdf_path)

        source_table_areas = self._get_table_areas(source_doc)
        target_table_areas = self._get_table_areas(target_doc)

        source_blocks = self._extract_text_blocks(source_doc, source_table_areas)
        target_blocks = self._extract_text_blocks(target_doc, target_table_areas)

        with pdfplumber.open(source_pdf_path) as source_pdf, pdfplumber.open(target_pdf_path) as target_pdf:
            num_pages = min(len(source_pdf.pages), len(target_pdf.pages))
            
            for i in range(num_pages):
                print(f"\r   - í˜ì´ì§€ ì²˜ë¦¬ ì¤‘: {i + 1}/{num_pages}", end="")
                
                # í‘œ ë§¤ì¹­
                source_tables = source_pdf.pages[i].extract_tables()
                target_tables = target_pdf.pages[i].extract_tables()
                if source_tables and len(source_tables) == len(target_tables):
                    for tbl_idx, source_table in enumerate(source_tables):
                        target_table = target_tables[tbl_idx]
                        if len(source_table) == len(target_table):
                            for row_idx, source_row in enumerate(source_table):
                                if len(source_row) == len(target_table[row_idx]):
                                    for col_idx, source_cell in enumerate(source_row):
                                        target_cell = target_table[row_idx][col_idx]
                                        if source_cell and target_cell:
                                            all_pairs.append({
                                                'source_text': " ".join(str(source_cell).split()),
                                                'target_text': " ".join(str(target_cell).split()),
                                                'similarity_score': 1.0,
                                                'page': i + 1, 'type': 'table'
                                            })
                
                # í…ìŠ¤íŠ¸ ë¸”ë¡ ë§¤ì¹­
                page_source_blocks = source_blocks[i]
                page_target_blocks = target_blocks[i]

                if not page_source_blocks or not page_target_blocks:
                    continue

                source_texts = [block[4] for block in page_source_blocks]
                target_texts = [block[4] for block in page_target_blocks]
                source_embeddings = self.model.encode(source_texts, convert_to_tensor=True)
                target_embeddings = self.model.encode(target_texts, convert_to_tensor=True)
                
                for s_idx, source_block in enumerate(page_source_blocks):
                    source_x = source_block[0]
                    source_y = (source_block[1] + source_block[3]) / 2
                    
                    best_match_score = 0
                    best_match_target_text = None

                    for t_idx, target_block in enumerate(page_target_blocks):
                        target_x = target_block[0]
                        target_y = (target_block[1] + target_block[3]) / 2
                        
                        # â–¼â–¼â–¼â–¼â–¼ í•µì‹¬ ë³€ê²½ì‚¬í•­ â–¼â–¼â–¼â–¼â–¼
                        # Yì¶•ê³¼ Xì¶• ìœ„ì¹˜ë¥¼ ëª¨ë‘ í™•ì¸
                        if (abs(source_y - target_y) < y_tolerance and 
                            abs(source_x - target_x) < x_tolerance):
                        # â–²â–²â–²â–²â–² í•µì‹¬ ë³€ê²½ì‚¬í•­ â–²â–²â–²â–²â–²
                            similarity = util.cos_sim(source_embeddings[s_idx], target_embeddings[t_idx]).item()
                            if similarity > best_match_score:
                                best_match_score = similarity
                                best_match_target_text = target_block[4]

                    if best_match_score > similarity_threshold:
                        all_pairs.append({
                            'source_text': source_block[4],
                            'target_text': best_match_target_text,
                            'similarity_score': round(best_match_score, 4),
                            'page': i + 1, 'type': 'text'
                        })
        
        source_doc.close()
        target_doc.close()
        print("\n   - âœ… ë§¤ì¹­ ì™„ë£Œ.")

        if not all_pairs:
            return pd.DataFrame()

        df = pd.DataFrame(all_pairs)
        df.drop_duplicates(subset=['source_text', 'target_text'], inplace=True)
        return df