# src/processors/ultimate_tm_builder.py

from pymongo import MongoClient
from datetime import datetime
from typing import List, Dict, Tuple
import re
from .semantic_segmenter import SemanticSegmenter
from ..config import MONGO_CONNECTION_STRING, DB_NAME

class HTMLDetector:
    """HTML ê²€ì¶œ ë° ë¶„ì„"""
    
    def __init__(self):
        self.html_patterns = [
            r'<[^>]+>',           # HTML íƒœê·¸
            r'&[a-zA-Z]+;',       # HTML ì—”í‹°í‹°
            r'&#+\d+;',           # ìˆ«ì ì—”í‹°í‹°
        ]
    
    def contains_html(self, text: str) -> bool:
        """HTML í¬í•¨ ì—¬ë¶€ ì •í™• ê²€ì¶œ"""
        if not isinstance(text, str):
            return False
            
        for pattern in self.html_patterns:
            if re.search(pattern, text):
                return True
        return False
    
    def detect_html_in_component(self, component_content: Dict) -> Dict:
        """ì»´í¬ë„ŒíŠ¸ ì „ì²´ì—ì„œ HTML ê²€ì¶œ"""
        html_fields = []
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ í•„ë“œ ê²€ì‚¬
        for key, value in component_content.items():
            if isinstance(value, str) and self.contains_html(value):
                html_fields.append({
                    "field": key,
                    "value": value,
                    "detected_tags": self._extract_tags(value)
                })
        
        return {
            "has_html": len(html_fields) > 0,
            "html_fields": html_fields,
            "total_html_fields": len(html_fields)
        }
    
    def _extract_tags(self, text: str) -> List[str]:
        """HTML íƒœê·¸ ì¶”ì¶œ"""
        tags = re.findall(r'<([^/>]+)[^>]*>', text)
        return list(set(tags))

class MetadataPreserver:
    """ë©”íƒ€ë°ì´í„° 100% ë³´ì¡´ ì²˜ë¦¬"""
    
    def preserve_all_metadata(self, original_record: Dict) -> Dict:
        """ì›ë³¸ ë ˆì½”ë“œì˜ ëª¨ë“  ë©”íƒ€ë°ì´í„° ë³´ì¡´"""
        
        # í•µì‹¬: ì›ë³¸ ë°ì´í„° 100% ë³µì‚¬
        preserved = original_record.copy()
        
        # í˜ì´ì§€ ë³µêµ¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ë©”íƒ€ë°ì´í„° í™•ì¸
        required_fields = [
            'page_path', 'component_path', 'component_type',
            'version_name', 'version_number', 'component_order',
            'parent_component_path', 'snapshot_timestamp',
            'snapshot_hash', 'component_hash', 'original_filepath'
        ]
        
        for field in required_fields:
            if field not in preserved:
                print(f"âš ï¸ Missing required field: {field}")
        
        return preserved

class UltimateTMBuilder:
    """HTML ë¶„ë¦¬ + ì˜ë¯¸ ë¶„í•  í†µí•© TM ë¹Œë”"""
    
    def __init__(self):
        self.client = MongoClient(MONGO_CONNECTION_STRING)
        self.db = self.client[DB_NAME]
        self.html_detector = HTMLDetector()
        self.metadata_preserver = MetadataPreserver()
        self.semantic_segmenter = SemanticSegmenter()
        
        print("ğŸš€ Ultimate TM Builder ì´ˆê¸°í™” ì™„ë£Œ")
        print("   - HTML ë¶„ë¦¬ ê¸°ëŠ¥ âœ…")
        print("   - ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  ê¸°ëŠ¥ âœ…")
        print("   - ë©”íƒ€ë°ì´í„° 100% ë³´ì¡´ âœ…")
    
    def build_ultimate_tm(self, source_version: str, target_version: str, lang_suffix: str):
        """ìµœê³ ê¸‰ TM êµ¬ì¶•: HTML ë¶„ë¦¬ + ì˜ë¯¸ ë¶„í• """
        
        print(f"ğŸ¯ Ultimate TM êµ¬ì¶• ì‹œì‘: {lang_suffix}")
        print(f"   Source: {source_version}, Target: {target_version}")
        
        # 1. ì›ë³¸ TM ë°ì´í„° ë¡œë“œ
        raw_tm_data = self._load_raw_tm_data(source_version, target_version)
        print(f"   - ì›ë³¸ TM: {len(raw_tm_data)}ê°œ ë ˆì½”ë“œ")
        
        # ê²°ê³¼ ì €ì¥ìš©
        clean_segments = []
        html_components = []
        processing_stats = {
            'total_input': len(raw_tm_data),
            'html_separated': 0,
            'clean_processed': 0,
            'segments_created': 0,
            'segmentation_applied': 0
        }
        
        # 2. ê° ë ˆì½”ë“œ ì²˜ë¦¬
        for i, tm_record in enumerate(raw_tm_data):
            if i % 100 == 0:
                print(f"   - ì§„í–‰ë¥ : {i}/{len(raw_tm_data)} ({i/len(raw_tm_data)*100:.1f}%)")
            
            try:
                if self._contains_html(tm_record):
                    # HTML ì»´í¬ë„ŒíŠ¸ â†’ Archive ì €ì¥
                    html_archive_record = self._prepare_html_archive(tm_record)
                    html_components.append(html_archive_record)
                    processing_stats['html_separated'] += 1
                    
                else:
                    # Clean í…ìŠ¤íŠ¸ â†’ ì˜ë¯¸ ë¶„í•  ì²˜ë¦¬
                    segments = self._process_with_semantic_segmentation(tm_record, lang_suffix)
                    clean_segments.extend(segments)
                    
                    processing_stats['clean_processed'] += 1
                    processing_stats['segments_created'] += len(segments)
                    if len(segments) > 1:
                        processing_stats['segmentation_applied'] += 1
                        
            except Exception as e:
                print(f"   âš ï¸ ë ˆì½”ë“œ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                continue
        
        print(f"   - ì²˜ë¦¬ ì™„ë£Œ: Clean {len(clean_segments)}ê°œ, HTML {len(html_components)}ê°œ")
        
        # 3. ê²°ê³¼ ì €ì¥
        self._save_ultimate_results(clean_segments, html_components, lang_suffix, processing_stats)
        
        return processing_stats
    
    def _load_raw_tm_data(self, source_version: str, target_version: str) -> List[Dict]:
        """ì›ë³¸ TM ë°ì´í„° ë¡œë“œ"""
        # ê¸°ì¡´ TM ì»¬ë ‰ì…˜ì—ì„œ ë°ì´í„° ë¡œë“œ
        # ì´ ë¶€ë¶„ì€ ê¸°ì¡´ TM êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”
        tm_collection = self.db["translation_memory_en_ko"]  # ì˜ˆì‹œ
        
        query = {
            '$or': [
                {'version_name': source_version},
                {'version_name': target_version}
            ]
        }
        
        return list(tm_collection.find(query))
    
    def _contains_html(self, tm_record: Dict) -> bool:
        """TM ë ˆì½”ë“œì— HTML í¬í•¨ ì—¬ë¶€ ê²€ì‚¬"""
        source_text = tm_record.get("source_text", "")
        target_text = tm_record.get("target_text", "")
        
        return (self.html_detector.contains_html(source_text) or 
                self.html_detector.contains_html(target_text))
    
    def _prepare_html_archive(self, tm_record: Dict) -> Dict:
        """HTML Archive ë ˆì½”ë“œ ì¤€ë¹„"""
        # ë©”íƒ€ë°ì´í„° 100% ë³´ì¡´
        html_record = self.metadata_preserver.preserve_all_metadata(tm_record)
        
        # HTML ë¶„ì„ ì •ë³´ ì¶”ê°€
        source_text = tm_record.get("source_text", "")
        target_text = tm_record.get("target_text", "")
        
        html_analysis = {
            "detected_tags": self._extract_all_tags(source_text, target_text),
            "html_complexity": self._assess_html_complexity(source_text, target_text),
            "tag_count": len(self._extract_all_tags(source_text, target_text)),
            "has_inline_styles": self._has_inline_styles(source_text, target_text),
            "has_scripts": self._has_scripts(source_text, target_text)
        }
        
        # HTML ê´€ë ¨ ì •ë³´ ì¶”ê°€
        html_record.update({
            "source_component_content": {"text": source_text},  # ì‹¤ì œë¡œëŠ” ì „ì²´ ì»´í¬ë„ŒíŠ¸ ë‚´ìš©
            "target_component_content": {"text": target_text},  # ì‹¤ì œë¡œëŠ” ì „ì²´ ì»´í¬ë„ŒíŠ¸ ë‚´ìš©
            "html_detection": html_analysis,
            "extracted_source_text": self._clean_html(source_text),
            "extracted_target_text": self._clean_html(target_text),
            "separation_reason": "html_tags_detected",
            "is_html_component": True,
            "processed_at": datetime.utcnow()
        })
        
        return html_record
    
    def _process_with_semantic_segmentation(self, tm_record: Dict, lang_suffix: str) -> List[Dict]:
        """ì˜ë¯¸ ë¶„í• ì„ ì ìš©í•œ Clean í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        
        source_text = tm_record.get("source_text", "")
        target_text = tm_record.get("target_text", "")
        
        # ì–¸ì–´ ì¶”ë¡ 
        source_lang, target_lang = self._infer_languages(lang_suffix)
        
        # ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  ì‹¤í–‰
        segments = self.semantic_segmenter.segment_text_pair(
            source_text=source_text,
            target_text=target_text,
            source_lang=source_lang,
            target_lang=target_lang,
            original_metadata=tm_record
        )
        
        return segments
    
    def _infer_languages(self, lang_suffix: str) -> Tuple[str, str]:
        """ì–¸ì–´ ìŒ ì¶”ë¡ """
        lang_map = {
            'en_ko': ('en', 'ko'),
            'en_ja': ('en', 'ja'),
            # í•„ìš”ì‹œ ì¶”ê°€
        }
        return lang_map.get(lang_suffix, ('en', 'ko'))
    
    def _save_ultimate_results(self, clean_segments: List[Dict], html_components: List[Dict], 
                              lang_suffix: str, stats: Dict):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        
        # MongoDB ì €ì¥
        if clean_segments:
            clean_collection_name = f"clean_translation_memory_{lang_suffix}"
            clean_collection = self.db[clean_collection_name]
            clean_collection.delete_many({})  # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            clean_collection.insert_many(clean_segments)
            print(f"   âœ… Clean TM ì €ì¥: {len(clean_segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
        
        if html_components:
            html_collection_name = f"html_component_archive_{lang_suffix}"
            html_collection = self.db[html_collection_name]
            html_collection.delete_many({})  # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            html_collection.insert_many(html_components)
            print(f"   âœ… HTML Archive ì €ì¥: {len(html_components)}ê°œ ì»´í¬ë„ŒíŠ¸")
        
        # í†µê³„ ì €ì¥
        stats_record = {
            'language_pair': lang_suffix,
            'processing_date': datetime.utcnow(),
            'method': 'html_separation_semantic_segmentation',
            'embedding_model': 'intfloat/multilingual-e5-large',
            **stats,
            'segmentation_ratio': stats['segmentation_applied'] / stats['clean_processed'] if stats['clean_processed'] > 0 else 0,
            'average_segments_per_record': stats['segments_created'] / stats['clean_processed'] if stats['clean_processed'] > 0 else 0
        }
        
        stats_collection_name = f"tm_separation_stats_{lang_suffix}"
        stats_collection = self.db[stats_collection_name]
        stats_collection.delete_many({})
        stats_collection.insert_one(stats_record)
        print(f"   âœ… í†µê³„ ì €ì¥ ì™„ë£Œ")
    
    def _extract_all_tags(self, source_text: str, target_text: str) -> List[str]:
        """ëª¨ë“  HTML íƒœê·¸ ì¶”ì¶œ"""
        all_text = source_text + " " + target_text
        tags = re.findall(r'<([^/>]+)[^>]*>', all_text)
        return list(set(tags))
    
    def _assess_html_complexity(self, source_text: str, target_text: str) -> str:
        """HTML ë³µì¡ë„ í‰ê°€"""
        all_text = source_text + " " + target_text
        tag_count = len(re.findall(r'<[^>]+>', all_text))
        
        if tag_count == 0:
            return "none"
        elif tag_count <= 3:
            return "simple"
        elif tag_count <= 8:
            return "medium"
        else:
            return "complex"
    
    def _has_inline_styles(self, source_text: str, target_text: str) -> bool:
        """ì¸ë¼ì¸ ìŠ¤íƒ€ì¼ í¬í•¨ ì—¬ë¶€"""
        all_text = source_text + " " + target_text
        return 'style=' in all_text
    
    def _has_scripts(self, source_text: str, target_text: str) -> bool:
        """ìŠ¤í¬ë¦½íŠ¸ í¬í•¨ ì—¬ë¶€"""
        all_text = source_text + " " + target_text
        return '<script' in all_text.lower()
    
    def _clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        cleaned = re.sub(r'<[^>]+>', '', text)
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        html_entities = {
            '&amp;': '&', '&lt;': '<', '&gt;': '>', 
            '&quot;': '"', '&#39;': "'", '&nbsp;': ' '
        }
        for entity, char in html_entities.items():
            cleaned = cleaned.replace(entity, char)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        cleaned = re.sub(r'\s+', ' ', cleaned)
        
        return cleaned.strip()